{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: MSFT 1-Minute GRU Forecast (Anti-Linear v3)\n",
        "\n",
        "This notebook implements the six requested anti-linear fixes plus volume/order-flow integration:\n",
        "1. Market-calendar sessionization (no hard 390 assumption) + high-fill session drop\n",
        "2. Imputation policy update: drop windows only if target horizon touches imputed bars; add `imputedFracWindow` feature\n",
        "3. Lookback sweep (`96/160/256`) before final fold runs\n",
        "4. Probabilistic decoder (`mu` + `sigma`) with Gaussian NLL\n",
        "5. Relaxed clipping + variance-matching loss\n",
        "6. Aggressive teacher-forcing decay to near-zero by epoch 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional dependency bootstrap\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required = {\n",
        "    'alpaca': 'alpaca-py',\n",
        "    'numpy': 'numpy',\n",
        "    'pandas': 'pandas',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'pandas_market_calendars': 'pandas-market-calendars',\n",
        "}\n",
        "\n",
        "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
        "if missing:\n",
        "    print('Installing missing packages:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
        "else:\n",
        "    print('All required third-party packages are already installed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_market_calendars as mcal\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from alpaca.data.enums import DataFeed\n",
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests import StockBarsRequest\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('CUDA:', torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Defaults are tuned for free-plan IEX while reducing linear-collapse behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "SYMBOL = 'MSFT'\n",
        "LOOKBACK_DAYS = 60\n",
        "\n",
        "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
        "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
        "\n",
        "BASE_FEATURE_COLS = [\n",
        "    'rOpen',\n",
        "    'rHigh',\n",
        "    'rLow',\n",
        "    'rClose',\n",
        "    'logVolChange',\n",
        "    'logTradeCountChange',\n",
        "    'vwapDelta',\n",
        "    'rangeFrac',\n",
        "    'orderFlowProxy',\n",
        "    'tickPressure',\n",
        "]\n",
        "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
        "\n",
        "HORIZON = 15\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "\n",
        "LOOKBACK_CANDIDATES = [96, 160, 256]\n",
        "DEFAULT_LOOKBACK = 160\n",
        "ENABLE_LOOKBACK_SWEEP = True\n",
        "\n",
        "# Model capacity / regularization\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.0\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Training schedules\n",
        "SWEEP_MAX_EPOCHS = 12\n",
        "SWEEP_PATIENCE = 4\n",
        "FINAL_MAX_EPOCHS = 35\n",
        "FINAL_PATIENCE = 8\n",
        "TF_START = 1.0\n",
        "TF_END = 0.05\n",
        "TF_RAMP_EPOCHS = 10\n",
        "\n",
        "# Probabilistic + auxiliary objectives\n",
        "HUBER_DELTA = 1.0\n",
        "NLL_WEIGHTS = np.array([1.0, 1.0, 1.0, 2.0, 0.4, 0.4, 0.3, 0.5, 0.6, 0.6], dtype=np.float32)\n",
        "DIR_LOSS_WEIGHT = 0.20\n",
        "VAR_MATCH_WEIGHT = 0.15\n",
        "\n",
        "# Clipping / sampling\n",
        "APPLY_CLIPPING = True\n",
        "CLIP_NON_OHLC = False\n",
        "CLIP_QUANTILES = (0.0005, 0.9995)\n",
        "INFER_SAMPLE_TEMPERATURE = 1.0\n",
        "\n",
        "# Diagnostics\n",
        "DIRECTION_EPS = 0.02\n",
        "STD_RATIO_TARGET_MIN = 0.60\n",
        "\n",
        "# Session and feed\n",
        "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
        "SESSION_TZ = 'America/New_York'\n",
        "REQUEST_CHUNK_DAYS = 5\n",
        "MAX_REQUESTS_PER_MINUTE = 120\n",
        "MAX_RETRIES = 5\n",
        "MAX_SESSION_FILL_RATIO = 0.08\n",
        "\n",
        "print({\n",
        "    'symbol': SYMBOL,\n",
        "    'lookback_days': LOOKBACK_DAYS,\n",
        "    'lookback_candidates': LOOKBACK_CANDIDATES,\n",
        "    'horizon': HORIZON,\n",
        "    'feed': ALPACA_FEED,\n",
        "    'max_session_fill_ratio': MAX_SESSION_FILL_RATIO,\n",
        "    'clip_quantiles': CLIP_QUANTILES,\n",
        "    'tf_ramp_epochs': TF_RAMP_EPOCHS,\n",
        "    'device': str(DEVICE),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data pull + calendar-aware sessionization\n",
        "class RequestPacer:\n",
        "    def __init__(self, max_calls_per_minute: int):\n",
        "        if max_calls_per_minute <= 0:\n",
        "            raise ValueError('max_calls_per_minute must be > 0')\n",
        "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
        "        self.last_call_ts = 0.0\n",
        "\n",
        "    def wait(self) -> None:\n",
        "        now = time.monotonic()\n",
        "        elapsed = now - self.last_call_ts\n",
        "        if elapsed < self.min_interval:\n",
        "            time.sleep(self.min_interval - elapsed)\n",
        "        self.last_call_ts = time.monotonic()\n",
        "\n",
        "\n",
        "def _require_alpaca_credentials() -> tuple[str, str]:\n",
        "    api_key = os.getenv('ALPACA_API_KEY')\n",
        "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
        "    if not api_key or not secret_key:\n",
        "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
        "    return api_key, secret_key\n",
        "\n",
        "\n",
        "def _resolve_feed(feed_name: str) -> DataFeed:\n",
        "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
        "    k = feed_name.strip().lower()\n",
        "    if k not in mapping:\n",
        "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
        "    return mapping[k]\n",
        "\n",
        "\n",
        "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
        "    api_key, secret_key = _require_alpaca_credentials()\n",
        "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
        "\n",
        "    feed = _resolve_feed(ALPACA_FEED)\n",
        "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
        "\n",
        "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
        "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
        "        end_ts = end_ts - timedelta(minutes=20)\n",
        "    start_ts = end_ts - timedelta(days=lookback_days)\n",
        "\n",
        "    parts = []\n",
        "    cursor = start_ts\n",
        "    calls = 0\n",
        "\n",
        "    while cursor < end_ts:\n",
        "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
        "        chunk = None\n",
        "\n",
        "        for attempt in range(1, MAX_RETRIES + 1):\n",
        "            pacer.wait()\n",
        "            calls += 1\n",
        "            try:\n",
        "                req = StockBarsRequest(\n",
        "                    symbol_or_symbols=[symbol],\n",
        "                    timeframe=TimeFrame.Minute,\n",
        "                    start=cursor,\n",
        "                    end=chunk_end,\n",
        "                    feed=feed,\n",
        "                    limit=10000,\n",
        "                )\n",
        "                chunk = client.get_stock_bars(req).df\n",
        "                break\n",
        "            except Exception as exc:\n",
        "                msg = str(exc).lower()\n",
        "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
        "                    backoff = min(2 ** attempt, 30)\n",
        "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
        "                    time.sleep(backoff)\n",
        "                    continue\n",
        "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
        "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
        "                raise\n",
        "\n",
        "        if chunk is not None and not chunk.empty:\n",
        "            d = chunk.reset_index().rename(\n",
        "                columns={\n",
        "                    'timestamp': 'Datetime',\n",
        "                    'open': 'Open',\n",
        "                    'high': 'High',\n",
        "                    'low': 'Low',\n",
        "                    'close': 'Close',\n",
        "                    'volume': 'Volume',\n",
        "                    'trade_count': 'TradeCount',\n",
        "                    'vwap': 'VWAP',\n",
        "                }\n",
        "            )\n",
        "            if 'Volume' not in d.columns:\n",
        "                d['Volume'] = 0.0\n",
        "            if 'TradeCount' not in d.columns:\n",
        "                d['TradeCount'] = 0.0\n",
        "            if 'VWAP' not in d.columns:\n",
        "                d['VWAP'] = d['Close']\n",
        "\n",
        "            need = ['Datetime'] + RAW_COLS\n",
        "            missing = [c for c in need if c not in d.columns]\n",
        "            if missing:\n",
        "                raise RuntimeError(f'Alpaca response missing columns: {missing}')\n",
        "\n",
        "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
        "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
        "            parts.append(d)\n",
        "\n",
        "        cursor = chunk_end\n",
        "\n",
        "    if not parts:\n",
        "        raise RuntimeError('No bars returned from Alpaca.')\n",
        "\n",
        "    out = pd.concat(parts, axis=0).sort_index()\n",
        "    out = out[~out.index.duplicated(keep='last')]\n",
        "    return out.astype(np.float32), calls\n",
        "\n",
        "\n",
        "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
        "    if df_utc.empty:\n",
        "        raise RuntimeError('Input bars are empty.')\n",
        "\n",
        "    idx = pd.DatetimeIndex(df_utc.index)\n",
        "    if idx.tz is None:\n",
        "        idx = idx.tz_localize('UTC')\n",
        "    else:\n",
        "        idx = idx.tz_convert('UTC')\n",
        "    df_utc = df_utc.copy()\n",
        "    df_utc.index = idx\n",
        "\n",
        "    cal = mcal.get_calendar('XNYS')\n",
        "    sched = cal.schedule(\n",
        "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
        "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
        "    )\n",
        "\n",
        "    pieces = []\n",
        "    kept = 0\n",
        "    dropped_high_fill = 0\n",
        "    dropped_empty = 0\n",
        "    fill_ratios = []\n",
        "    expected_lengths = []\n",
        "\n",
        "    for _, row in sched.iterrows():\n",
        "        open_ts = pd.Timestamp(row['market_open'])\n",
        "        close_ts = pd.Timestamp(row['market_close'])\n",
        "        if open_ts.tzinfo is None:\n",
        "            open_ts = open_ts.tz_localize('UTC')\n",
        "        else:\n",
        "            open_ts = open_ts.tz_convert('UTC')\n",
        "        if close_ts.tzinfo is None:\n",
        "            close_ts = close_ts.tz_localize('UTC')\n",
        "        else:\n",
        "            close_ts = close_ts.tz_convert('UTC')\n",
        "\n",
        "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
        "        if len(exp_idx) == 0:\n",
        "            continue\n",
        "\n",
        "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)][RAW_COLS].copy()\n",
        "        day = day.reindex(exp_idx)\n",
        "\n",
        "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
        "        fill_ratio = float(imputed.mean())\n",
        "\n",
        "        if fill_ratio >= 1.0:\n",
        "            dropped_empty += 1\n",
        "            continue\n",
        "\n",
        "        if fill_ratio > MAX_SESSION_FILL_RATIO:\n",
        "            dropped_high_fill += 1\n",
        "            continue\n",
        "\n",
        "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
        "        if day['VWAP'].isna().all():\n",
        "            day['VWAP'] = day['Close']\n",
        "        else:\n",
        "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
        "\n",
        "        day['Volume'] = day['Volume'].fillna(0.0)\n",
        "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
        "        day['is_imputed'] = imputed.astype(np.int8)\n",
        "\n",
        "        if day[RAW_COLS].isna().any().any():\n",
        "            raise RuntimeError('NaNs remain after per-session fill.')\n",
        "\n",
        "        pieces.append(day)\n",
        "        kept += 1\n",
        "        fill_ratios.append(fill_ratio)\n",
        "        expected_lengths.append(len(exp_idx))\n",
        "\n",
        "    if not pieces:\n",
        "        raise RuntimeError('No sessions kept after calendar filtering and fill-ratio threshold.')\n",
        "\n",
        "    out = pd.concat(pieces, axis=0).sort_index()\n",
        "\n",
        "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
        "\n",
        "    meta = {\n",
        "        'calendar_sessions_total': int(len(sched)),\n",
        "        'kept_sessions': int(kept),\n",
        "        'dropped_empty_sessions': int(dropped_empty),\n",
        "        'dropped_high_fill_sessions': int(dropped_high_fill),\n",
        "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
        "        'max_fill_ratio_kept': float(np.max(fill_ratios)) if fill_ratios else float('nan'),\n",
        "        'avg_session_minutes_kept': float(np.mean(expected_lengths)) if expected_lengths else float('nan'),\n",
        "    }\n",
        "    return out.astype(np.float32), meta\n",
        "\n",
        "\n",
        "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
        "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
        "\n",
        "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
        "print(f'Sessionized rows kept: {len(price_df):,}')\n",
        "print(f'Alpaca API calls: {api_calls} (<= {MAX_REQUESTS_PER_MINUTE}/min target)')\n",
        "print('Session meta:', session_meta)\n",
        "\n",
        "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
        "if len(price_df) < min_needed:\n",
        "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')\n",
        "\n",
        "display(price_df.head(3))\n",
        "display(price_df.tail(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering (volume/order-flow included)\n",
        "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
        "    out = np.asarray(ohlc, dtype=np.float32).copy()\n",
        "    o = out[:, 0]\n",
        "    h = out[:, 1]\n",
        "    l = out[:, 2]\n",
        "    c = out[:, 3]\n",
        "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
        "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
        "    return out\n",
        "\n",
        "\n",
        "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
        "    seq = []\n",
        "    prev_close = float(last_close)\n",
        "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
        "        o = prev_close * np.exp(float(rO))\n",
        "        h = prev_close * np.exp(float(rH))\n",
        "        l = prev_close * np.exp(float(rL))\n",
        "        c = prev_close * np.exp(float(rC))\n",
        "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
        "        seq.append(cand)\n",
        "        prev_close = float(cand[3])\n",
        "    return np.asarray(seq, dtype=np.float32)\n",
        "\n",
        "\n",
        "def one_step_returns_to_prices_batch(return_ohlc: np.ndarray, prev_close: np.ndarray) -> np.ndarray:\n",
        "    r = np.asarray(return_ohlc, dtype=np.float32)\n",
        "    p = np.asarray(prev_close, dtype=np.float32)\n",
        "    out = np.stack([\n",
        "        p * np.exp(r[:, 0]),\n",
        "        p * np.exp(r[:, 1]),\n",
        "        p * np.exp(r[:, 2]),\n",
        "        p * np.exp(r[:, 3]),\n",
        "    ], axis=1).astype(np.float32)\n",
        "    return enforce_candle_validity(out)\n",
        "\n",
        "\n",
        "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    eps = 1e-9\n",
        "\n",
        "    prev_close = df['Close'].shift(1)\n",
        "    prev_vol = df['Volume'].shift(1)\n",
        "    prev_tc = df['TradeCount'].shift(1)\n",
        "\n",
        "    row_imputed = (df['is_imputed'].astype(bool) | df['is_imputed'].shift(1).fillna(0).astype(bool))\n",
        "\n",
        "    valid = prev_close.notna() & prev_vol.notna() & prev_tc.notna()\n",
        "    c = df.loc[valid]\n",
        "    base = prev_close.loc[valid]\n",
        "\n",
        "    out = pd.DataFrame(index=c.index, dtype=np.float32)\n",
        "\n",
        "    out['rOpen'] = np.log(c['Open'] / (base + eps))\n",
        "    out['rHigh'] = np.log(c['High'] / (base + eps))\n",
        "    out['rLow'] = np.log(c['Low'] / (base + eps))\n",
        "    out['rClose'] = np.log(c['Close'] / (base + eps))\n",
        "\n",
        "    out['logVolChange'] = np.log((c['Volume'] + 1.0) / (prev_vol.loc[valid] + 1.0))\n",
        "    out['logTradeCountChange'] = np.log((c['TradeCount'] + 1.0) / (prev_tc.loc[valid] + 1.0))\n",
        "    out['vwapDelta'] = np.log((c['VWAP'] + eps) / (c['Close'] + eps))\n",
        "    out['rangeFrac'] = (c['High'] - c['Low']) / (base + eps)\n",
        "\n",
        "    signed_body = (c['Close'] - c['Open']) / ((c['High'] - c['Low']) + eps)\n",
        "    out['orderFlowProxy'] = signed_body * np.log1p(c['Volume'])\n",
        "    out['tickPressure'] = np.sign(c['Close'] - c['Open']) * np.log1p(c['TradeCount'])\n",
        "\n",
        "    out['row_imputed'] = row_imputed.loc[valid].astype(np.int8).to_numpy()\n",
        "    out['prev_close'] = base.astype(np.float32).to_numpy()\n",
        "\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "\n",
        "feat_df = build_feature_frame(price_df)\n",
        "print('Feature rows:', len(feat_df))\n",
        "print('Rows marked imputed-sensitive:', int(feat_df['row_imputed'].sum()))\n",
        "\n",
        "# Transform round-trip check\n",
        "rt_n = min(2000, len(price_df))\n",
        "sub = price_df.iloc[:rt_n].copy()\n",
        "sub_feat = build_feature_frame(sub)\n",
        "ret = sub_feat[['rOpen', 'rHigh', 'rLow', 'rClose']].to_numpy(np.float32)\n",
        "recon = returns_to_prices_seq(ret, float(sub['Close'].iloc[0]))\n",
        "actual = sub[OHLC_COLS].iloc[1:].to_numpy(np.float32)\n",
        "\n",
        "rt_max = float(np.max(np.abs(recon - actual)))\n",
        "rt_mean = float(np.mean(np.abs(recon - actual)))\n",
        "print({'roundtrip_max_abs_error': rt_max, 'roundtrip_mean_abs_error': rt_mean})\n",
        "assert rt_max < 2e-3, 'Round-trip transform error too large.'\n",
        "\n",
        "display(feat_df[BASE_FEATURE_COLS + ['row_imputed', 'prev_close']].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowing/splits + lookback sweep helpers\n",
        "def fit_standardizer(train_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "    mean = train_values.mean(axis=0)\n",
        "    std = train_values.std(axis=0)\n",
        "    std = np.where(std < 1e-8, 1.0, std)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "\n",
        "def apply_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return ((values - mean) / std).astype(np.float32)\n",
        "\n",
        "\n",
        "def undo_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (values * std + mean).astype(np.float32)\n",
        "\n",
        "\n",
        "def split_points(n_rows: int) -> tuple[int, int]:\n",
        "    tr = int(n_rows * TRAIN_RATIO)\n",
        "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
        "    return tr, va\n",
        "\n",
        "\n",
        "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
        "    n = len(price_df_full)\n",
        "    span = int(round(n * 0.85))\n",
        "    shift = max(1, n - span)\n",
        "\n",
        "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
        "\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for name, a, b in cands:\n",
        "        key = (a, b)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        if b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1200:\n",
        "            continue\n",
        "        out.append((name, a, b))\n",
        "        seen.add(key)\n",
        "\n",
        "    if not out:\n",
        "        raise RuntimeError('Unable to create walk-forward slices.')\n",
        "    return out\n",
        "\n",
        "\n",
        "def make_multistep_windows(\n",
        "    base_scaled: np.ndarray,\n",
        "    base_raw: np.ndarray,\n",
        "    row_imputed: np.ndarray,\n",
        "    starts_prev_close: np.ndarray,\n",
        "    window: int,\n",
        "    horizon: int,\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    X = []\n",
        "    y_s = []\n",
        "    y_r = []\n",
        "    starts = []\n",
        "    prev_close = []\n",
        "    dropped_target_imputed = 0\n",
        "\n",
        "    n = len(base_scaled)\n",
        "    for i in range(window, n - horizon + 1):\n",
        "        # Item 2: drop only if TARGET horizon touches imputed rows.\n",
        "        if row_imputed[i : i + horizon].any():\n",
        "            dropped_target_imputed += 1\n",
        "            continue\n",
        "\n",
        "        xb = base_scaled[i - window : i]\n",
        "\n",
        "        # Item 2: add input feature `imputedFracWindow` (fraction in input window).\n",
        "        imp_frac = float(row_imputed[i - window : i].mean())\n",
        "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
        "        xb_aug = np.concatenate([xb, imp_col], axis=1).astype(np.float32)\n",
        "\n",
        "        X.append(xb_aug)\n",
        "        y_s.append(base_scaled[i : i + horizon])\n",
        "        y_r.append(base_raw[i : i + horizon])\n",
        "        starts.append(i)\n",
        "        prev_close.append(starts_prev_close[i])\n",
        "\n",
        "    return (\n",
        "        np.asarray(X, dtype=np.float32),\n",
        "        np.asarray(y_s, dtype=np.float32),\n",
        "        np.asarray(y_r, dtype=np.float32),\n",
        "        np.asarray(starts, dtype=np.int64),\n",
        "        np.asarray(prev_close, dtype=np.float32),\n",
        "        dropped_target_imputed,\n",
        "    )\n",
        "\n",
        "\n",
        "class MultiStepDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y_s: np.ndarray, y_r: np.ndarray):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y_s = torch.from_numpy(y_s).float()\n",
        "        self.y_r = torch.from_numpy(y_r).float()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.X[idx], self.y_s[idx], self.y_r[idx]\n",
        "\n",
        "\n",
        "slices = build_walkforward_slices(price_df)\n",
        "print('Walk-forward slices:', slices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model + losses (probabilistic seq2seq)\n",
        "class Seq2SeqProbGRU(nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int, hidden_size: int, num_layers: int, dropout: float, horizon: int):\n",
        "        super().__init__()\n",
        "        self.horizon = horizon\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.encoder = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        self.decoder_cell = nn.GRUCell(output_size, hidden_size)\n",
        "        self.mu_head = nn.Linear(hidden_size, output_size)\n",
        "        self.log_sigma_head = nn.Linear(hidden_size, output_size)\n",
        "        self.direction_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        y_teacher: torch.Tensor | None = None,\n",
        "        teacher_forcing_ratio: float = 0.0,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        _, h = self.encoder(x)\n",
        "        h_dec = h[-1]\n",
        "\n",
        "        dec_input = x[:, -1, : self.output_size]\n",
        "\n",
        "        mu_seq = []\n",
        "        log_sigma_seq = []\n",
        "        dir_seq = []\n",
        "\n",
        "        for t in range(self.horizon):\n",
        "            h_dec = self.decoder_cell(dec_input, h_dec)\n",
        "\n",
        "            mu = self.mu_head(h_dec)\n",
        "            log_sigma = torch.clamp(self.log_sigma_head(h_dec), min=-5.0, max=2.0)\n",
        "            dir_logit = self.direction_head(h_dec).squeeze(-1)\n",
        "\n",
        "            mu_seq.append(mu.unsqueeze(1))\n",
        "            log_sigma_seq.append(log_sigma.unsqueeze(1))\n",
        "            dir_seq.append(dir_logit.unsqueeze(1))\n",
        "\n",
        "            if y_teacher is not None:\n",
        "                sample_like = mu + 0.30 * torch.exp(log_sigma) * torch.randn_like(mu)\n",
        "\n",
        "                if teacher_forcing_ratio >= 1.0:\n",
        "                    dec_input = y_teacher[:, t, :]\n",
        "                elif teacher_forcing_ratio <= 0.0:\n",
        "                    dec_input = sample_like\n",
        "                else:\n",
        "                    m = (torch.rand(x.size(0), device=x.device) < teacher_forcing_ratio).unsqueeze(1)\n",
        "                    dec_input = torch.where(m, y_teacher[:, t, :], sample_like)\n",
        "            else:\n",
        "                dec_input = mu\n",
        "\n",
        "        return (\n",
        "            torch.cat(mu_seq, dim=1),\n",
        "            torch.cat(log_sigma_seq, dim=1),\n",
        "            torch.cat(dir_seq, dim=1),\n",
        "        )\n",
        "\n",
        "\n",
        "class WeightedGaussianNLL(nn.Module):\n",
        "    def __init__(self, weights: np.ndarray):\n",
        "        super().__init__()\n",
        "        w = torch.as_tensor(weights, dtype=torch.float32).view(1, 1, -1)\n",
        "        self.register_buffer('weights', w)\n",
        "\n",
        "    def forward(self, mu: torch.Tensor, log_sigma: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        z = (target - mu) / sigma\n",
        "        nll = 0.5 * (z * z + 2.0 * log_sigma)\n",
        "        return (nll * self.weights).mean()\n",
        "\n",
        "\n",
        "def tf_ratio_for_epoch(epoch: int) -> float:\n",
        "    # Item 6: aggressive decay to near-zero by epoch 10.\n",
        "    if epoch <= TF_RAMP_EPOCHS:\n",
        "        if TF_RAMP_EPOCHS <= 1:\n",
        "            return float(TF_END)\n",
        "        p = (epoch - 1) / float(TF_RAMP_EPOCHS - 1)\n",
        "        return float(TF_START + (TF_END - TF_START) * p)\n",
        "    return float(TF_END)\n",
        "\n",
        "\n",
        "def run_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    nll_loss_fn: nn.Module,\n",
        "    dir_loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer | None = None,\n",
        "    tf_ratio: float = 0.0,\n",
        ") -> dict:\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    total = 0.0\n",
        "    nll_tot = 0.0\n",
        "    dir_tot = 0.0\n",
        "    var_tot = 0.0\n",
        "    n_items = 0\n",
        "\n",
        "    for xb, yb_s, yb_r in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb_s = yb_s.to(DEVICE)\n",
        "        yb_r = yb_r.to(DEVICE)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            mu, log_sigma, dir_logits = model(\n",
        "                xb,\n",
        "                y_teacher=yb_s if is_train else None,\n",
        "                teacher_forcing_ratio=tf_ratio if is_train else 0.0,\n",
        "            )\n",
        "\n",
        "            nll = nll_loss_fn(mu, log_sigma, yb_s)\n",
        "\n",
        "            dir_target = (yb_r[:, :, 3] > 0.0).float()\n",
        "            dir_loss = dir_loss_fn(dir_logits, dir_target)\n",
        "\n",
        "            # Item 5: variance matching penalty to fight collapse.\n",
        "            pred_std = torch.std(mu[:, :, 3], unbiased=False)\n",
        "            true_std = torch.std(yb_s[:, :, 3], unbiased=False)\n",
        "            var_pen = (pred_std - true_std) ** 2\n",
        "\n",
        "            loss = nll + DIR_LOSS_WEIGHT * dir_loss + VAR_MATCH_WEIGHT * var_pen\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total += loss.item() * bs\n",
        "        nll_tot += nll.item() * bs\n",
        "        dir_tot += dir_loss.item() * bs\n",
        "        var_tot += var_pen.item() * bs\n",
        "        n_items += bs\n",
        "\n",
        "    return {\n",
        "        'total': total / max(n_items, 1),\n",
        "        'nll': nll_tot / max(n_items, 1),\n",
        "        'dir': dir_tot / max(n_items, 1),\n",
        "        'var': var_tot / max(n_items, 1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    max_epochs: int,\n",
        "    patience: int,\n",
        ") -> pd.DataFrame:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-5)\n",
        "\n",
        "    nll_loss_fn = WeightedGaussianNLL(NLL_WEIGHTS).to(DEVICE)\n",
        "    dir_loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    wait = 0\n",
        "    rows = []\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        tf = tf_ratio_for_epoch(epoch)\n",
        "\n",
        "        tr = run_epoch(model, train_loader, nll_loss_fn, dir_loss_fn, optimizer=optimizer, tf_ratio=tf)\n",
        "        va = run_epoch(model, val_loader, nll_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "\n",
        "        scheduler.step(va['total'])\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        rows.append({\n",
        "            'epoch': epoch,\n",
        "            'tf_ratio': tf,\n",
        "            'lr': lr,\n",
        "            'train_total': tr['total'],\n",
        "            'val_total': va['total'],\n",
        "            'train_nll': tr['nll'],\n",
        "            'val_nll': va['nll'],\n",
        "            'train_dir': tr['dir'],\n",
        "            'val_dir': va['dir'],\n",
        "            'train_var': tr['var'],\n",
        "            'val_var': va['var'],\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
        "            f\"train={tr['total']:.5f} (nll={tr['nll']:.5f}, dir={tr['dir']:.5f}, var={tr['var']:.5f}) | \"\n",
        "            f\"val={va['total']:.5f} (nll={va['nll']:.5f}, dir={va['dir']:.5f}, var={va['var']:.5f}) | lr={lr:.6g}\"\n",
        "        )\n",
        "\n",
        "        if va['total'] < best_val:\n",
        "            best_val = va['total']\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}.')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics and fold execution\n",
        "def rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b)) ** 2)))\n",
        "\n",
        "\n",
        "def directional_accuracy_eps(actual_close: np.ndarray, pred_close: np.ndarray, prev_close: np.ndarray, eps: float) -> float:\n",
        "    am = np.asarray(actual_close) - np.asarray(prev_close)\n",
        "    pm = np.asarray(pred_close) - np.asarray(prev_close)\n",
        "    mask = np.abs(am) > eps\n",
        "    if not np.any(mask):\n",
        "        return float('nan')\n",
        "    return float(np.mean(np.sign(am[mask]) == np.sign(pm[mask])))\n",
        "\n",
        "\n",
        "def evaluate_metrics(actual_ohlc: np.ndarray, pred_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
        "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
        "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
        "    prev_close = np.asarray(prev_close, dtype=np.float32)\n",
        "\n",
        "    ac = actual_ohlc[:, 3]\n",
        "    pc = pred_ohlc[:, 3]\n",
        "\n",
        "    return {\n",
        "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
        "        'close_rmse': rmse(ac, pc),\n",
        "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
        "        'ohlc_rmse': rmse(actual_ohlc.reshape(-1), pred_ohlc.reshape(-1)),\n",
        "        'directional_accuracy_eps': directional_accuracy_eps(ac, pc, prev_close, DIRECTION_EPS),\n",
        "        'mean_signed_bias': float(np.mean(pc - ac)),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_baselines(actual_ohlc: np.ndarray, prev_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
        "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
        "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
        "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
        "    return {'persistence': persistence, 'flat_close_rw': flat_rw}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_multistep_mean(model: nn.Module, X: np.ndarray, batch_size: int = 512) -> tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    mus = []\n",
        "    logs = []\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = torch.from_numpy(X[i : i + batch_size]).float().to(DEVICE)\n",
        "        mu, log_sigma, _ = model(xb, y_teacher=None, teacher_forcing_ratio=0.0)\n",
        "        mus.append(mu.cpu().numpy())\n",
        "        logs.append(log_sigma.cpu().numpy())\n",
        "    return np.concatenate(mus, axis=0).astype(np.float32), np.concatenate(logs, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def recursive_forecast_features(\n",
        "    model: nn.Module,\n",
        "    seed_window: np.ndarray,\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        "    sample: bool,\n",
        "    temperature: float,\n",
        ") -> np.ndarray:\n",
        "    model.eval()\n",
        "    w = seed_window.copy().astype(np.float32)\n",
        "    out = []\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        xb = torch.from_numpy(w).float().unsqueeze(0).to(DEVICE)\n",
        "        mu_s, log_s, _ = model(xb, y_teacher=None, teacher_forcing_ratio=0.0)\n",
        "\n",
        "        mu_s = mu_s[:, 0, :].cpu().numpy()[0]\n",
        "        log_s = log_s[:, 0, :].cpu().numpy()[0]\n",
        "\n",
        "        mu_raw = undo_standardizer(mu_s.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "        sigma_raw = (np.exp(log_s) * scale_std).astype(np.float32)\n",
        "\n",
        "        if sample:\n",
        "            step_raw = mu_raw + np.random.normal(loc=0.0, scale=1.0, size=mu_raw.shape).astype(np.float32) * sigma_raw * float(temperature)\n",
        "        else:\n",
        "            step_raw = mu_raw\n",
        "\n",
        "        # Item 5: relaxed clipping, mainly OHLC.\n",
        "        if APPLY_CLIPPING:\n",
        "            step_raw[:4] = np.clip(step_raw[:4], clip_low[:4], clip_high[:4])\n",
        "            if CLIP_NON_OHLC:\n",
        "                step_raw[4:] = np.clip(step_raw[4:], clip_low[4:], clip_high[4:])\n",
        "\n",
        "        step_scaled = apply_standardizer(step_raw.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "\n",
        "        # Input-only feature: rolling imputed fraction proxy (keep continuity).\n",
        "        next_imp_frac = float(np.mean(w[:, -1]))\n",
        "        next_in = np.concatenate([step_scaled, np.array([next_imp_frac], dtype=np.float32)]).astype(np.float32)\n",
        "\n",
        "        out.append(step_raw.astype(np.float32))\n",
        "        w = np.vstack([w[1:], next_in]).astype(np.float32)\n",
        "\n",
        "    return np.asarray(out, dtype=np.float32)\n",
        "\n",
        "\n",
        "def evaluate_recursive_by_horizon(\n",
        "    model: nn.Module,\n",
        "    base_scaled: np.ndarray,\n",
        "    row_imputed: np.ndarray,\n",
        "    price_values: np.ndarray,\n",
        "    prev_close_values: np.ndarray,\n",
        "    anchors: list[int],\n",
        "    window: int,\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        ") -> pd.DataFrame:\n",
        "    m_err = [[] for _ in range(horizon)]\n",
        "    p_err = [[] for _ in range(horizon)]\n",
        "    f_err = [[] for _ in range(horizon)]\n",
        "\n",
        "    for i in anchors:\n",
        "        imp_frac = float(row_imputed[i - window : i].mean())\n",
        "        seed = np.concatenate([base_scaled[i - window : i], np.full((window, 1), imp_frac, dtype=np.float32)], axis=1)\n",
        "\n",
        "        last_close = float(prev_close_values[i])\n",
        "\n",
        "        pred_feat = recursive_forecast_features(\n",
        "            model=model,\n",
        "            seed_window=seed,\n",
        "            horizon=horizon,\n",
        "            scale_mean=scale_mean,\n",
        "            scale_std=scale_std,\n",
        "            clip_low=clip_low,\n",
        "            clip_high=clip_high,\n",
        "            sample=False,\n",
        "            temperature=1.0,\n",
        "        )\n",
        "\n",
        "        pred_price = returns_to_prices_seq(pred_feat[:, :4], last_close=last_close)\n",
        "        actual = price_values[i + 1 : i + 1 + horizon]\n",
        "\n",
        "        persist_seq = np.repeat(price_values[i : i + 1], horizon, axis=0)\n",
        "        c0 = prev_close_values[i]\n",
        "        flat_seq = np.repeat(np.array([[c0, c0, c0, c0]], dtype=np.float32), horizon, axis=0)\n",
        "\n",
        "        for h in range(horizon):\n",
        "            m_err[h].append(abs(float(pred_price[h, 3] - actual[h, 3])))\n",
        "            p_err[h].append(abs(float(persist_seq[h, 3] - actual[h, 3])))\n",
        "            f_err[h].append(abs(float(flat_seq[h, 3] - actual[h, 3])))\n",
        "\n",
        "    rows = []\n",
        "    for h in range(horizon):\n",
        "        rows.append({\n",
        "            'horizon': h + 1,\n",
        "            'model_close_mae': float(np.mean(m_err[h])) if m_err[h] else np.nan,\n",
        "            'persistence_close_mae': float(np.mean(p_err[h])) if p_err[h] else np.nan,\n",
        "            'flat_close_mae': float(np.mean(f_err[h])) if f_err[h] else np.nan,\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def overfit_sanity_check(X_train: np.ndarray, y_train_s: np.ndarray, y_train_r: np.ndarray, output_dim: int) -> dict:\n",
        "    n = min(1000, len(X_train))\n",
        "    if n < 300:\n",
        "        return {'ran': False, 'reason': 'too_few_samples'}\n",
        "\n",
        "    ds = MultiStepDataset(X_train[:n], y_train_s[:n], y_train_r[:n])\n",
        "    dl = DataLoader(ds, batch_size=128, shuffle=True, drop_last=False)\n",
        "\n",
        "    model = Seq2SeqProbGRU(\n",
        "        input_size=X_train.shape[-1],\n",
        "        output_size=output_dim,\n",
        "        hidden_size=128,\n",
        "        num_layers=1,\n",
        "        dropout=0.0,\n",
        "        horizon=HORIZON,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    nll_loss_fn = WeightedGaussianNLL(NLL_WEIGHTS).to(DEVICE)\n",
        "    dir_loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.0)\n",
        "\n",
        "    init_stats = run_epoch(model, dl, nll_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "    for _ in range(15):\n",
        "        _ = run_epoch(model, dl, nll_loss_fn, dir_loss_fn, optimizer=opt, tf_ratio=1.0)\n",
        "    fin_stats = run_epoch(model, dl, nll_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "\n",
        "    passed = bool(fin_stats['total'] < init_stats['total'] * 0.65)\n",
        "    return {'ran': True, 'initial_total': float(init_stats['total']), 'final_total': float(fin_stats['total']), 'passed': passed}\n",
        "\n",
        "\n",
        "def run_fold(\n",
        "    fold_name: str,\n",
        "    price_fold: pd.DataFrame,\n",
        "    window: int,\n",
        "    max_epochs: int,\n",
        "    patience: int,\n",
        "    run_sanity: bool = False,\n",
        "    quick_mode: bool = False,\n",
        ") -> dict:\n",
        "    feat_fold = build_feature_frame(price_fold)\n",
        "\n",
        "    base_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
        "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
        "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
        "\n",
        "    price_vals = price_fold[OHLC_COLS].to_numpy(np.float32)\n",
        "\n",
        "    tr_end, va_end = split_points(len(base_raw))\n",
        "    if tr_end <= window or va_end <= tr_end:\n",
        "        raise RuntimeError(f'{fold_name}: invalid split points for rows={len(base_raw)}, window={window}')\n",
        "\n",
        "    mean, std = fit_standardizer(base_raw[:tr_end])\n",
        "    base_scaled = apply_standardizer(base_raw, mean, std)\n",
        "\n",
        "    ql, qh = CLIP_QUANTILES\n",
        "    clip_low = np.quantile(base_raw[:tr_end], ql, axis=0).astype(np.float32)\n",
        "    clip_high = np.quantile(base_raw[:tr_end], qh, axis=0).astype(np.float32)\n",
        "\n",
        "    X_all, y_all_s, y_all_r, starts, prev_close_starts, dropped_target_imputed = make_multistep_windows(\n",
        "        base_scaled=base_scaled,\n",
        "        base_raw=base_raw,\n",
        "        row_imputed=row_imputed,\n",
        "        starts_prev_close=prev_close,\n",
        "        window=window,\n",
        "        horizon=HORIZON,\n",
        "    )\n",
        "\n",
        "    if len(X_all) == 0:\n",
        "        raise RuntimeError(f'{fold_name}: no windows available after target-imputation filtering.')\n",
        "\n",
        "    end_idx = starts + HORIZON - 1\n",
        "    tr_m = end_idx < tr_end\n",
        "    va_m = (end_idx >= tr_end) & (end_idx < va_end)\n",
        "    te_m = end_idx >= va_end\n",
        "\n",
        "    X_train, y_train_s, y_train_r = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m]\n",
        "    X_val, y_val_s, y_val_r = X_all[va_m], y_all_s[va_m], y_all_r[va_m]\n",
        "    X_test, y_test_s, y_test_r = X_all[te_m], y_all_s[te_m], y_all_r[te_m]\n",
        "    test_starts = starts[te_m]\n",
        "    test_prev_close = prev_close_starts[te_m]\n",
        "\n",
        "    if min(len(X_train), len(X_val), len(X_test)) == 0:\n",
        "        raise RuntimeError(f'{fold_name}: empty train/val/test split after windowing.')\n",
        "\n",
        "    sanity = overfit_sanity_check(X_train, y_train_s, y_train_r, output_dim=len(BASE_FEATURE_COLS)) if run_sanity else {'ran': False}\n",
        "\n",
        "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = Seq2SeqProbGRU(\n",
        "        input_size=X_train.shape[-1],\n",
        "        output_size=len(BASE_FEATURE_COLS),\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "        horizon=HORIZON,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    hist = train_model(model, train_loader, val_loader, max_epochs=max_epochs, patience=patience)\n",
        "\n",
        "    mu_test_s, log_test_s = predict_multistep_mean(model, X_test, batch_size=512)\n",
        "    mu_test_raw = undo_standardizer(mu_test_s.reshape(-1, len(BASE_FEATURE_COLS)), mean, std).reshape(mu_test_s.shape)\n",
        "\n",
        "    pred_step1_ret = mu_test_raw[:, 0, :4]\n",
        "    actual_step1_ret = y_test_r[:, 0, :4]\n",
        "\n",
        "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
        "    pred_ohlc_1 = one_step_returns_to_prices_batch(pred_step1_ret, test_prev_close)\n",
        "\n",
        "    prev_ohlc = price_vals[test_starts]\n",
        "\n",
        "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
        "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
        "\n",
        "    pred_close_std = float(np.std(pred_step1_ret[:, 3]))\n",
        "    actual_close_std = float(np.std(actual_step1_ret[:, 3]))\n",
        "    std_ratio = float(pred_close_std / max(actual_close_std, 1e-12))\n",
        "\n",
        "    if quick_mode:\n",
        "        return {\n",
        "            'fold': fold_name,\n",
        "            'window': int(window),\n",
        "            'history_df': hist,\n",
        "            'sanity': sanity,\n",
        "            'model_metrics': model_metrics,\n",
        "            'baseline_metrics': baseline_metrics,\n",
        "            'pred_actual_std_ratio': std_ratio,\n",
        "            'samples': {\n",
        "                'train': int(len(X_train)),\n",
        "                'val': int(len(X_val)),\n",
        "                'test': int(len(X_test)),\n",
        "                'dropped_target_imputed': int(dropped_target_imputed),\n",
        "            },\n",
        "        }\n",
        "\n",
        "    anchors = [int(i) for i in test_starts if i >= window and (i + HORIZON) < len(price_vals)]\n",
        "    if not anchors:\n",
        "        raise RuntimeError(f'{fold_name}: no valid anchors for recursive evaluation.')\n",
        "\n",
        "    horizon_df = evaluate_recursive_by_horizon(\n",
        "        model=model,\n",
        "        base_scaled=base_scaled,\n",
        "        row_imputed=row_imputed,\n",
        "        price_values=price_vals,\n",
        "        prev_close_values=prev_close,\n",
        "        anchors=anchors,\n",
        "        window=window,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=mean,\n",
        "        scale_std=std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "    )\n",
        "\n",
        "    last_anchor = anchors[-1]\n",
        "    imp_frac = float(row_imputed[last_anchor - window : last_anchor].mean())\n",
        "    seed = np.concatenate([\n",
        "        base_scaled[last_anchor - window : last_anchor],\n",
        "        np.full((window, 1), imp_frac, dtype=np.float32),\n",
        "    ], axis=1)\n",
        "\n",
        "    last_close = float(prev_close[last_anchor])\n",
        "\n",
        "    pred_feat_det = recursive_forecast_features(\n",
        "        model=model,\n",
        "        seed_window=seed,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=mean,\n",
        "        scale_std=std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "        sample=False,\n",
        "        temperature=1.0,\n",
        "    )\n",
        "    pred_price_det = returns_to_prices_seq(pred_feat_det[:, :4], last_close=last_close)\n",
        "\n",
        "    pred_feat_sample = recursive_forecast_features(\n",
        "        model=model,\n",
        "        seed_window=seed,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=mean,\n",
        "        scale_std=std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "        sample=True,\n",
        "        temperature=INFER_SAMPLE_TEMPERATURE,\n",
        "    )\n",
        "    pred_price_sample = returns_to_prices_seq(pred_feat_sample[:, :4], last_close=last_close)\n",
        "\n",
        "    actual_price_path = price_vals[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
        "    future_idx = price_fold.index[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
        "\n",
        "    pred_future_df_det = pd.DataFrame(pred_price_det, index=future_idx, columns=OHLC_COLS)\n",
        "    pred_future_df_sample = pd.DataFrame(pred_price_sample, index=future_idx, columns=OHLC_COLS)\n",
        "    actual_future_df = pd.DataFrame(actual_price_path, index=future_idx, columns=OHLC_COLS)\n",
        "\n",
        "    known_pos = last_anchor\n",
        "    context_df = price_fold.iloc[max(0, known_pos - 169) : known_pos + 1][OHLC_COLS].copy()\n",
        "\n",
        "    step15 = horizon_df[horizon_df['horizon'] == HORIZON].iloc[0]\n",
        "\n",
        "    return {\n",
        "        'fold': fold_name,\n",
        "        'window': int(window),\n",
        "        'history_df': hist,\n",
        "        'sanity': sanity,\n",
        "        'model_metrics': model_metrics,\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'horizon_df': horizon_df,\n",
        "        'pred_actual_std_ratio': std_ratio,\n",
        "        'samples': {\n",
        "            'train': int(len(X_train)),\n",
        "            'val': int(len(X_val)),\n",
        "            'test': int(len(X_test)),\n",
        "            'dropped_target_imputed': int(dropped_target_imputed),\n",
        "            'anchors': int(len(anchors)),\n",
        "        },\n",
        "        'step15': {\n",
        "            'model_close_mae': float(step15['model_close_mae']),\n",
        "            'persistence_close_mae': float(step15['persistence_close_mae']),\n",
        "            'flat_close_mae': float(step15['flat_close_mae']),\n",
        "        },\n",
        "        'context_df': context_df,\n",
        "        'actual_future_df': actual_future_df,\n",
        "        'pred_future_df_det': pred_future_df_det,\n",
        "        'pred_future_df_sample': pred_future_df_sample,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run lookback sweep then full walk-forward\n",
        "fold_results = []\n",
        "\n",
        "primary_slice = slices[0]\n",
        "sweep_rows = []\n",
        "selected_window = DEFAULT_LOOKBACK\n",
        "\n",
        "if ENABLE_LOOKBACK_SWEEP:\n",
        "    print('\\n=== Lookback sweep (quick mode on first fold) ===')\n",
        "    _, a0, b0 = primary_slice\n",
        "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
        "\n",
        "    for w in LOOKBACK_CANDIDATES:\n",
        "        print(f'\\n-- Sweep candidate lookback={w} --')\n",
        "        r = run_fold(\n",
        "            fold_name=f'sweep_w{w}',\n",
        "            price_fold=fold_price0,\n",
        "            window=w,\n",
        "            max_epochs=SWEEP_MAX_EPOCHS,\n",
        "            patience=SWEEP_PATIENCE,\n",
        "            run_sanity=False,\n",
        "            quick_mode=True,\n",
        "        )\n",
        "\n",
        "        m = r['model_metrics']\n",
        "        p = r['baseline_metrics']['persistence']\n",
        "        std_ratio = r['pred_actual_std_ratio']\n",
        "\n",
        "        # Higher is better: beat persistence and increase variance ratio.\n",
        "        score = (p['close_mae'] - m['close_mae']) + 0.15 * std_ratio\n",
        "\n",
        "        sweep_rows.append({\n",
        "            'lookback': w,\n",
        "            'model_close_mae': m['close_mae'],\n",
        "            'persistence_close_mae': p['close_mae'],\n",
        "            'std_ratio': std_ratio,\n",
        "            'score': score,\n",
        "            'dropped_target_imputed': r['samples']['dropped_target_imputed'],\n",
        "        })\n",
        "\n",
        "    sweep_df = pd.DataFrame(sweep_rows).sort_values('score', ascending=False).reset_index(drop=True)\n",
        "    display(sweep_df)\n",
        "\n",
        "    selected_window = int(sweep_df.iloc[0]['lookback'])\n",
        "    print(f'Selected lookback from sweep: {selected_window}')\n",
        "else:\n",
        "    sweep_df = pd.DataFrame()\n",
        "    selected_window = DEFAULT_LOOKBACK\n",
        "\n",
        "print('\\n=== Full walk-forward with selected lookback ===')\n",
        "for i, (name, a, b) in enumerate(slices, start=1):\n",
        "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
        "    fold_price = price_df.iloc[a:b].copy()\n",
        "\n",
        "    res = run_fold(\n",
        "        fold_name=name,\n",
        "        price_fold=fold_price,\n",
        "        window=selected_window,\n",
        "        max_epochs=FINAL_MAX_EPOCHS,\n",
        "        patience=FINAL_PATIENCE,\n",
        "        run_sanity=(i == 1),\n",
        "        quick_mode=False,\n",
        "    )\n",
        "    fold_results.append(res)\n",
        "\n",
        "    print('Samples:', res['samples'])\n",
        "    print('Sanity:', res['sanity'])\n",
        "    print('One-step model:', res['model_metrics'])\n",
        "    print('One-step persistence:', res['baseline_metrics']['persistence'])\n",
        "    print('One-step flat:', res['baseline_metrics']['flat_close_rw'])\n",
        "    print('Recursive step-15:', res['step15'])\n",
        "    print('Std ratio (pred/actual close return):', res['pred_actual_std_ratio'])\n",
        "\n",
        "if not fold_results:\n",
        "    raise RuntimeError('No fold results produced.')\n",
        "\n",
        "summary_rows = []\n",
        "for r in fold_results:\n",
        "    m = r['model_metrics']\n",
        "    p = r['baseline_metrics']['persistence']\n",
        "    f = r['baseline_metrics']['flat_close_rw']\n",
        "    s15 = r['step15']\n",
        "\n",
        "    summary_rows.append({\n",
        "        'fold': r['fold'],\n",
        "        'window': r['window'],\n",
        "        'model_close_mae': m['close_mae'],\n",
        "        'model_directional_acc_eps': m['directional_accuracy_eps'],\n",
        "        'model_bias': m['mean_signed_bias'],\n",
        "        'persist_close_mae': p['close_mae'],\n",
        "        'flat_close_mae': f['close_mae'],\n",
        "        'step15_model_mae': s15['model_close_mae'],\n",
        "        'step15_persist_mae': s15['persistence_close_mae'],\n",
        "        'pred_actual_std_ratio': r['pred_actual_std_ratio'],\n",
        "        'dropped_target_imputed': r['samples']['dropped_target_imputed'],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "display(summary_df)\n",
        "\n",
        "metric_cols = [c for c in summary_df.columns if c not in {'fold'}]\n",
        "agg_df = pd.DataFrame({'mean': summary_df[metric_cols].mean(), 'std': summary_df[metric_cols].std(ddof=0)})\n",
        "display(agg_df)\n",
        "\n",
        "horizon_all = pd.concat([r['horizon_df'].assign(fold=r['fold']) for r in fold_results], ignore_index=True)\n",
        "horizon_stats = horizon_all.groupby('horizon', as_index=False)[['model_close_mae', 'persistence_close_mae', 'flat_close_mae']].mean()\n",
        "display(horizon_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Acceptance checks + diagnostics\n",
        "latest = fold_results[-1]\n",
        "\n",
        "mean_model_mae = float(summary_df['model_close_mae'].mean())\n",
        "mean_persist_mae = float(summary_df['persist_close_mae'].mean())\n",
        "mean_dir = float(summary_df['model_directional_acc_eps'].mean())\n",
        "mean_bias = float(summary_df['model_bias'].mean())\n",
        "mean_step15_model = float(summary_df['step15_model_mae'].mean())\n",
        "mean_step15_persist = float(summary_df['step15_persist_mae'].mean())\n",
        "mean_std_ratio = float(summary_df['pred_actual_std_ratio'].mean())\n",
        "\n",
        "pred_close_path = latest['pred_future_df_sample']['Close'].to_numpy()\n",
        "is_monotonic = bool(np.all(np.diff(pred_close_path) >= 0) or np.all(np.diff(pred_close_path) <= 0))\n",
        "\n",
        "acceptance = {\n",
        "    'criterion_1_model_mae_20pct_better_than_persistence': mean_model_mae <= 0.8 * mean_persist_mae,\n",
        "    'criterion_2_directional_accuracy_eps_at_least_0_52': mean_dir >= 0.52,\n",
        "    'criterion_3_step15_better_than_persistence': mean_step15_model < mean_step15_persist,\n",
        "    'criterion_4_abs_bias_within_25pct_of_mae': abs(mean_bias) <= 0.25 * mean_model_mae,\n",
        "    'criterion_5_non_monotonic_latest_prediction_path': not is_monotonic,\n",
        "    'criterion_6_prediction_variance_not_collapsed': mean_std_ratio >= STD_RATIO_TARGET_MIN,\n",
        "}\n",
        "\n",
        "print('Selected lookback:', int(summary_df['window'].iloc[0]))\n",
        "print('Acceptance checks:')\n",
        "for k, v in acceptance.items():\n",
        "    print(f'  {k}: {v}')\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4.8), facecolor='white')\n",
        "\n",
        "hist = latest['history_df']\n",
        "axes[0].plot(hist['epoch'], hist['train_total'], label='Train total', color='black')\n",
        "axes[0].plot(hist['epoch'], hist['val_total'], label='Val total', color='gray')\n",
        "axes[0].plot(hist['epoch'], hist['val_nll'], label='Val NLL', color='#1f77b4', alpha=0.8)\n",
        "axes[0].plot(hist['epoch'], hist['val_dir'], label='Val Dir', color='#d62728', alpha=0.8)\n",
        "axes[0].set_title(f\"Loss Curves ({latest['fold']})\")\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(alpha=0.25)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['model_close_mae'], label='Model', color='black', linewidth=2.0)\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['persistence_close_mae'], label='Persistence', color='#E74C3C')\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['flat_close_mae'], label='Flat RW', color='#3498DB')\n",
        "axes[1].set_title('Recursive Close MAE by Horizon')\n",
        "axes[1].set_xlabel('Horizon')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].grid(alpha=0.25)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].bar(summary_df['fold'], summary_df['pred_actual_std_ratio'], color='#555555')\n",
        "axes[2].axhline(1.0, color='#2ca02c', linestyle='--', linewidth=1.2, label='ideal=1.0')\n",
        "axes[2].axhline(STD_RATIO_TARGET_MIN, color='#ff7f0e', linestyle=':', linewidth=1.2, label=f'min target={STD_RATIO_TARGET_MIN}')\n",
        "axes[2].set_title('Predicted/Actual Close Return Std Ratio')\n",
        "axes[2].set_ylabel('Std ratio')\n",
        "axes[2].grid(alpha=0.25)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final candlestick chart (history green/red, predicted white/black)\n",
        "def draw_candles(\n",
        "    ax,\n",
        "    ohlc: pd.DataFrame,\n",
        "    start_x: int,\n",
        "    up_edge: str,\n",
        "    up_face: str,\n",
        "    down_edge: str,\n",
        "    down_face: str,\n",
        "    wick_color: str,\n",
        "    width: float = 0.62,\n",
        "    lw: float = 1.0,\n",
        "    alpha: float = 1.0,\n",
        "):\n",
        "    vals = ohlc[OHLC_COLS].to_numpy()\n",
        "    for i, (o, h, l, c) in enumerate(vals):\n",
        "        x = start_x + i\n",
        "        bull = c >= o\n",
        "\n",
        "        ax.vlines(x, l, h, color=wick_color, linewidth=lw, alpha=alpha, zorder=2)\n",
        "\n",
        "        lower = min(o, c)\n",
        "        height = abs(c - o)\n",
        "        if height < 1e-8:\n",
        "            height = 1e-6\n",
        "\n",
        "        rect = Rectangle(\n",
        "            (x - width / 2, lower),\n",
        "            width,\n",
        "            height,\n",
        "            facecolor=up_face if bull else down_face,\n",
        "            edgecolor=up_edge if bull else down_edge,\n",
        "            linewidth=lw,\n",
        "            alpha=alpha,\n",
        "            zorder=3,\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "\n",
        "context_df = latest['context_df']\n",
        "actual_future_df = latest['actual_future_df']\n",
        "pred_future_df = latest['pred_future_df_sample']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(17, 8), facecolor='black')\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "draw_candles(ax, context_df, 0, '#2ECC71', '#2ECC71', '#E74C3C', '#E74C3C', '#DADADA', width=0.58, lw=1.0, alpha=0.95)\n",
        "draw_candles(ax, actual_future_df, len(context_df), '#1D6F42', '#1D6F42', '#8E2F25', '#8E2F25', '#9A9A9A', width=0.58, lw=1.0, alpha=0.70)\n",
        "draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#000000', '#000000', '#F5F5F5', width=0.50, lw=1.35, alpha=1.0)\n",
        "\n",
        "ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=0.9, alpha=0.6)\n",
        "\n",
        "idx = context_df.index.append(actual_future_df.index)\n",
        "n = len(idx)\n",
        "step = max(1, n // 10)\n",
        "ticks = list(range(0, n, step))\n",
        "if ticks[-1] != n - 1:\n",
        "    ticks.append(n - 1)\n",
        "\n",
        "labels = [idx[i].strftime('%m-%d %H:%M') for i in ticks]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(labels, rotation=26, ha='right', color='white', fontsize=9)\n",
        "\n",
        "ax.tick_params(axis='y', colors='white')\n",
        "for sp in ax.spines.values():\n",
        "    sp.set_color('#666666')\n",
        "\n",
        "ax.grid(color='#252525', linewidth=0.6, alpha=0.35)\n",
        "ax.set_title(f'MSFT 1m ({latest[\"fold\"]}) - History vs 15-step Forecast', color='white', pad=14)\n",
        "ax.set_ylabel('Price', color='white')\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='History bullish (green)'),\n",
        "    Patch(facecolor='#E74C3C', edgecolor='#E74C3C', label='History bearish (red)'),\n",
        "    Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted bullish (white)'),\n",
        "    Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted bearish (black)'),\n",
        "]\n",
        "leg = ax.legend(handles=legend_elements, facecolor='black', edgecolor='#707070', framealpha=1.0, loc='upper left')\n",
        "for t in leg.get_texts():\n",
        "    t.set_color('white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- If `pred_actual_std_ratio` remains low, the model is still volatility-collapsed.\n",
        "- The chart now displays the sampled recursive path (`temperature` controlled), not only deterministic mean path.\n",
        "- For stricter realism, next step is quote-level order-book imbalance features from NBBO/trades data.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
