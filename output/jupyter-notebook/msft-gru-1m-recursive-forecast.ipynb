{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: MSFT 1-Minute GRU Recursive Forecast (Stabilized)\n",
        "\n",
        "This notebook implements the full recovery plan to address forecast collapse (linear drift/up-only bias):\n",
        "- Alpaca `IEX` feed with free-plan-safe throttling\n",
        "- Regular-trading-hours sessionization (09:30-15:59 ET)\n",
        "- Return-space OHLC targets instead of raw absolute price targets\n",
        "- Weighted Huber loss + AdamW + LR scheduler + early stopping\n",
        "- Recursive 15-step forecast with train-quantile clipping guards\n",
        "- Baselines, walk-forward evaluation, and acceptance checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional dependency bootstrap (lightweight)\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required = {\n",
        "    'alpaca': 'alpaca-py',\n",
        "    'numpy': 'numpy',\n",
        "    'pandas': 'pandas',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'sklearn': 'scikit-learn',\n",
        "}\n",
        "\n",
        "missing = [pkg for module_name, pkg in required.items() if importlib.util.find_spec(module_name) is None]\n",
        "if missing:\n",
        "    print('Installing missing packages:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
        "else:\n",
        "    print('All required third-party packages are already installed.')\n",
        "\n",
        "print('Install PyTorch separately with the correct CUDA wheel for your RTX 3070.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: imports and reproducibility\n",
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from alpaca.data.enums import DataFeed\n",
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests import StockBarsRequest\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('CUDA:', torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Locked defaults from the fix plan:\n",
        "- `ALPACA_FEED='iex'`\n",
        "- `USE_RTH_ONLY=True` with ET sessionization\n",
        "- `TARGET_MODE='log_return_ohlc'`\n",
        "- `LOSS_TYPE='weighted_huber'`\n",
        "- `MAX_REQUESTS_PER_MINUTE=120`\n",
        "- `CLIP_TARGET_QUANTILES=(0.005, 0.995)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "SYMBOL = 'MSFT'\n",
        "LOOKBACK_DAYS = 60\n",
        "FEATURES = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "WINDOW = 500\n",
        "HORIZON = 15\n",
        "TARGET_MODE = 'log_return_ohlc'\n",
        "\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "MAX_EPOCHS = 30\n",
        "EARLY_STOPPING_PATIENCE = 6\n",
        "LOSS_TYPE = 'weighted_huber'\n",
        "HUBER_DELTA = 1.0\n",
        "LOSS_WEIGHTS = np.array([1.0, 1.0, 1.0, 2.0], dtype=np.float32)\n",
        "CLIP_TARGET_QUANTILES = (0.005, 0.995)\n",
        "\n",
        "# Alpaca + sessionization\n",
        "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
        "USE_RTH_ONLY = True\n",
        "SESSION_TZ = 'America/New_York'\n",
        "RTH_START = '09:30'\n",
        "RTH_END = '16:00'\n",
        "REQUEST_CHUNK_DAYS = 5\n",
        "MAX_REQUESTS_PER_MINUTE = 120\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "print({\n",
        "    'symbol': SYMBOL,\n",
        "    'lookback_days': LOOKBACK_DAYS,\n",
        "    'window': WINDOW,\n",
        "    'horizon': HORIZON,\n",
        "    'target_mode': TARGET_MODE,\n",
        "    'loss_type': LOSS_TYPE,\n",
        "    'alpaca_feed': ALPACA_FEED,\n",
        "    'use_rth_only': USE_RTH_ONLY,\n",
        "    'session_tz': SESSION_TZ,\n",
        "    'max_requests_per_minute': MAX_REQUESTS_PER_MINUTE,\n",
        "    'device': str(DEVICE),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data ingestion, rate-limit-safe pulling, and RTH sessionization\n",
        "class RequestPacer:\n",
        "    def __init__(self, max_calls_per_minute: int):\n",
        "        if max_calls_per_minute <= 0:\n",
        "            raise ValueError('max_calls_per_minute must be > 0')\n",
        "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
        "        self.last_call_ts = 0.0\n",
        "\n",
        "    def wait(self) -> None:\n",
        "        now = time.monotonic()\n",
        "        elapsed = now - self.last_call_ts\n",
        "        if elapsed < self.min_interval:\n",
        "            time.sleep(self.min_interval - elapsed)\n",
        "        self.last_call_ts = time.monotonic()\n",
        "\n",
        "\n",
        "def _require_alpaca_credentials() -> tuple[str, str]:\n",
        "    api_key = os.getenv('ALPACA_API_KEY')\n",
        "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
        "    if not api_key or not secret_key:\n",
        "        raise RuntimeError(\n",
        "            'Missing Alpaca credentials. Set ALPACA_API_KEY and ALPACA_SECRET_KEY before running this notebook.'\n",
        "        )\n",
        "    return api_key, secret_key\n",
        "\n",
        "\n",
        "def _resolve_feed(feed_name: str) -> DataFeed:\n",
        "    mapping = {\n",
        "        'iex': DataFeed.IEX,\n",
        "        'sip': DataFeed.SIP,\n",
        "        'delayed_sip': DataFeed.DELAYED_SIP,\n",
        "    }\n",
        "    key = feed_name.strip().lower()\n",
        "    if key not in mapping:\n",
        "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
        "    return mapping[key]\n",
        "\n",
        "\n",
        "def fetch_ohlc_1m_alpaca(\n",
        "    symbol: str,\n",
        "    lookback_days: int,\n",
        "    feed_name: str,\n",
        "    chunk_days: int,\n",
        "    max_calls_per_minute: int,\n",
        "    max_retries: int = 5,\n",
        ") -> tuple[pd.DataFrame, int]:\n",
        "    api_key, secret_key = _require_alpaca_credentials()\n",
        "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
        "\n",
        "    feed = _resolve_feed(feed_name)\n",
        "    pacer = RequestPacer(max_calls_per_minute=max_calls_per_minute)\n",
        "\n",
        "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
        "    if feed_name in {'sip', 'delayed_sip'}:\n",
        "        end_ts = end_ts - timedelta(minutes=20)\n",
        "\n",
        "    start_ts = end_ts - timedelta(days=lookback_days)\n",
        "\n",
        "    frames: list[pd.DataFrame] = []\n",
        "    cursor = start_ts\n",
        "    api_call_count = 0\n",
        "\n",
        "    while cursor < end_ts:\n",
        "        chunk_end = min(cursor + timedelta(days=chunk_days), end_ts)\n",
        "        bars = None\n",
        "\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "            pacer.wait()\n",
        "            api_call_count += 1\n",
        "            try:\n",
        "                req = StockBarsRequest(\n",
        "                    symbol_or_symbols=[symbol],\n",
        "                    timeframe=TimeFrame.Minute,\n",
        "                    start=cursor,\n",
        "                    end=chunk_end,\n",
        "                    feed=feed,\n",
        "                    limit=10_000,\n",
        "                )\n",
        "                bars = client.get_stock_bars(req).df\n",
        "                break\n",
        "            except Exception as exc:\n",
        "                msg = str(exc).lower()\n",
        "\n",
        "                if ('429' in msg or 'rate limit' in msg) and attempt < max_retries:\n",
        "                    backoff = min(2 ** attempt, 30)\n",
        "                    print(\n",
        "                        f'Rate limit response on chunk {cursor} -> {chunk_end}; '\n",
        "                        f'sleeping {backoff}s (attempt {attempt}/{max_retries}).'\n",
        "                    )\n",
        "                    time.sleep(backoff)\n",
        "                    continue\n",
        "\n",
        "                if ('subscription' in msg or 'forbidden' in msg) and feed_name != 'iex':\n",
        "                    raise RuntimeError(\n",
        "                        f'Feed {feed_name!r} is unavailable for this account. '\n",
        "                        'Set ALPACA_FEED=iex or upgrade your data subscription.'\n",
        "                    ) from exc\n",
        "\n",
        "                raise\n",
        "\n",
        "        if bars is not None and not bars.empty:\n",
        "            chunk_df = bars.reset_index()\n",
        "            chunk_df = chunk_df.rename(\n",
        "                columns={\n",
        "                    'timestamp': 'Datetime',\n",
        "                    'open': 'Open',\n",
        "                    'high': 'High',\n",
        "                    'low': 'Low',\n",
        "                    'close': 'Close',\n",
        "                }\n",
        "            )\n",
        "\n",
        "            needed = ['Datetime', 'Open', 'High', 'Low', 'Close']\n",
        "            missing = [c for c in needed if c not in chunk_df.columns]\n",
        "            if missing:\n",
        "                raise RuntimeError(f'Alpaca response missing required columns: {missing}')\n",
        "\n",
        "            chunk_df['Datetime'] = pd.to_datetime(chunk_df['Datetime'], utc=True)\n",
        "            chunk_df = chunk_df[needed].dropna().set_index('Datetime').sort_index()\n",
        "            frames.append(chunk_df)\n",
        "\n",
        "        cursor = chunk_end\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError('No bars returned from Alpaca. Verify symbol, credentials, feed, and market session.')\n",
        "\n",
        "    df = pd.concat(frames, axis=0).sort_index()\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    df = df.dropna(subset=FEATURES)\n",
        "    return df.astype(np.float32), api_call_count\n",
        "\n",
        "\n",
        "def sessionize_regular_hours(\n",
        "    df_utc: pd.DataFrame,\n",
        "    tz_name: str,\n",
        "    start_hhmm: str,\n",
        "    end_hhmm: str,\n",
        "    use_rth_only: bool = True,\n",
        ") -> tuple[pd.DataFrame, dict]:\n",
        "    if df_utc.empty:\n",
        "        raise RuntimeError('Input dataframe is empty before sessionization.')\n",
        "\n",
        "    local = df_utc.copy()\n",
        "    local.index = pd.DatetimeIndex(local.index).tz_convert(tz_name)\n",
        "\n",
        "    local = local[local.index.dayofweek < 5]\n",
        "\n",
        "    if use_rth_only:\n",
        "        local = local.between_time(start_hhmm, end_hhmm, inclusive='left')\n",
        "\n",
        "    if local.empty:\n",
        "        raise RuntimeError('No bars remain after weekday/RTH filtering.')\n",
        "\n",
        "    session_dates = pd.Index(local.index.date).unique().tolist()\n",
        "    raw_counts = {d: int((local.index.date == d).sum()) for d in session_dates}\n",
        "\n",
        "    if not raw_counts:\n",
        "        raise RuntimeError('No sessions found after filtering.')\n",
        "\n",
        "    latest_date = max(raw_counts.keys())\n",
        "    drop_latest = raw_counts[latest_date] < 390\n",
        "\n",
        "    session_frames = []\n",
        "    kept_dates = []\n",
        "    fill_counts = {}\n",
        "\n",
        "    for d in session_dates:\n",
        "        if drop_latest and d == latest_date:\n",
        "            continue\n",
        "\n",
        "        day_df = local[local.index.date == d][FEATURES].copy()\n",
        "\n",
        "        day_start = pd.Timestamp(f'{d} {start_hhmm}', tz=tz_name)\n",
        "        expected_idx = pd.date_range(day_start, periods=390, freq='1min')\n",
        "\n",
        "        reindexed = day_df.reindex(expected_idx)\n",
        "        missing_before = int(reindexed[FEATURES].isna().any(axis=1).sum())\n",
        "\n",
        "        reindexed[FEATURES] = reindexed[FEATURES].ffill().bfill()\n",
        "\n",
        "        if reindexed[FEATURES].isna().any().any():\n",
        "            raise RuntimeError(f'Session {d} still contains NaNs after fill.')\n",
        "\n",
        "        session_frames.append(reindexed)\n",
        "        kept_dates.append(d)\n",
        "        fill_counts[d] = missing_before\n",
        "\n",
        "    if not session_frames:\n",
        "        raise RuntimeError('No complete sessions remain after dropping incomplete latest session.')\n",
        "\n",
        "    out = pd.concat(session_frames, axis=0).sort_index()\n",
        "\n",
        "    grouped_sizes = out.groupby(out.index.date).size()\n",
        "    if not bool((grouped_sizes == 390).all()):\n",
        "        raise AssertionError('Integrity check failed: each kept session must have exactly 390 bars.')\n",
        "\n",
        "    if out[FEATURES].isna().any().any():\n",
        "        raise AssertionError('Integrity check failed: NaNs present after sessionization.')\n",
        "\n",
        "    out.index = out.index.tz_localize(None)\n",
        "\n",
        "    meta = {\n",
        "        'raw_sessions': len(session_dates),\n",
        "        'kept_sessions': len(kept_dates),\n",
        "        'dropped_latest_session': bool(drop_latest),\n",
        "        'latest_session_raw_count': int(raw_counts[latest_date]),\n",
        "        'avg_filled_bars_per_session': float(np.mean(list(fill_counts.values()))) if fill_counts else 0.0,\n",
        "    }\n",
        "\n",
        "    return out.astype(np.float32), meta\n",
        "\n",
        "\n",
        "raw_df_utc, api_calls = fetch_ohlc_1m_alpaca(\n",
        "    symbol=SYMBOL,\n",
        "    lookback_days=LOOKBACK_DAYS,\n",
        "    feed_name=ALPACA_FEED,\n",
        "    chunk_days=REQUEST_CHUNK_DAYS,\n",
        "    max_calls_per_minute=MAX_REQUESTS_PER_MINUTE,\n",
        "    max_retries=MAX_RETRIES,\n",
        ")\n",
        "\n",
        "price_df, session_meta = sessionize_regular_hours(\n",
        "    raw_df_utc,\n",
        "    tz_name=SESSION_TZ,\n",
        "    start_hhmm=RTH_START,\n",
        "    end_hhmm=RTH_END,\n",
        "    use_rth_only=USE_RTH_ONLY,\n",
        ")\n",
        "\n",
        "span_days = (price_df.index.max() - price_df.index.min()).total_seconds() / 86400\n",
        "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
        "print(f'Sessionized rows: {len(price_df):,}')\n",
        "print(f'Time span after sessionization: {span_days:.1f} days')\n",
        "print(f'Alpaca API calls: {api_calls} (throttled to <= {MAX_REQUESTS_PER_MINUTE}/min)')\n",
        "print('Session meta:', session_meta)\n",
        "\n",
        "min_needed = WINDOW + HORIZON + 500\n",
        "if len(price_df) < min_needed:\n",
        "    raise RuntimeError(\n",
        "        f'Not enough sessionized rows ({len(price_df)}) for robust training. Need at least {min_needed}.'\n",
        "    )\n",
        "\n",
        "display(price_df.head(3))\n",
        "display(price_df.tail(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform utilities (prices <-> return targets) with correctness checks\n",
        "def prices_to_targets(price_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    prev_close = price_df['Close'].shift(1)\n",
        "    valid = prev_close.notna()\n",
        "\n",
        "    base = prev_close[valid]\n",
        "    tgt = pd.DataFrame(index=price_df.index[valid], dtype=np.float32)\n",
        "\n",
        "    tgt['Open'] = np.log(price_df.loc[valid, 'Open'] / base)\n",
        "    tgt['High'] = np.log(price_df.loc[valid, 'High'] / base)\n",
        "    tgt['Low'] = np.log(price_df.loc[valid, 'Low'] / base)\n",
        "    tgt['Close'] = np.log(price_df.loc[valid, 'Close'] / base)\n",
        "\n",
        "    return tgt.astype(np.float32), base.astype(np.float32)\n",
        "\n",
        "\n",
        "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
        "    out = np.asarray(ohlc, dtype=np.float32).copy()\n",
        "    o = out[:, 0]\n",
        "    h = out[:, 1]\n",
        "    l = out[:, 2]\n",
        "    c = out[:, 3]\n",
        "\n",
        "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
        "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
        "    return out\n",
        "\n",
        "\n",
        "def targets_to_prices_seq(pred_targets: np.ndarray, last_close: float) -> np.ndarray:\n",
        "    seq = []\n",
        "    prev_close = float(last_close)\n",
        "\n",
        "    for rO, rH, rL, rC in np.asarray(pred_targets, dtype=np.float32):\n",
        "        o = prev_close * np.exp(float(rO))\n",
        "        h = prev_close * np.exp(float(rH))\n",
        "        l = prev_close * np.exp(float(rL))\n",
        "        c = prev_close * np.exp(float(rC))\n",
        "\n",
        "        cand = np.array([[o, h, l, c]], dtype=np.float32)\n",
        "        cand = enforce_candle_validity(cand)[0]\n",
        "        seq.append(cand)\n",
        "        prev_close = float(cand[3])\n",
        "\n",
        "    return np.asarray(seq, dtype=np.float32)\n",
        "\n",
        "\n",
        "def one_step_targets_to_prices_batch(pred_targets: np.ndarray, prev_close: np.ndarray) -> np.ndarray:\n",
        "    pred_targets = np.asarray(pred_targets, dtype=np.float32)\n",
        "    prev_close = np.asarray(prev_close, dtype=np.float32)\n",
        "\n",
        "    o = prev_close * np.exp(pred_targets[:, 0])\n",
        "    h = prev_close * np.exp(pred_targets[:, 1])\n",
        "    l = prev_close * np.exp(pred_targets[:, 2])\n",
        "    c = prev_close * np.exp(pred_targets[:, 3])\n",
        "\n",
        "    out = np.stack([o, h, l, c], axis=1).astype(np.float32)\n",
        "    return enforce_candle_validity(out)\n",
        "\n",
        "\n",
        "targets_df, prev_close_series = prices_to_targets(price_df)\n",
        "print('Price rows:', len(price_df), 'Target rows:', len(targets_df))\n",
        "\n",
        "# Round-trip test on early segment (price -> returns -> prices)\n",
        "round_n = min(1500, len(price_df))\n",
        "round_df = price_df.iloc[:round_n].copy()\n",
        "round_targets, _ = prices_to_targets(round_df)\n",
        "round_recon = targets_to_prices_seq(round_targets.to_numpy(np.float32), float(round_df['Close'].iloc[0]))\n",
        "round_actual = round_df[FEATURES].iloc[1:].to_numpy(np.float32)\n",
        "\n",
        "rt_max_abs = float(np.max(np.abs(round_recon - round_actual)))\n",
        "rt_mean_abs = float(np.mean(np.abs(round_recon - round_actual)))\n",
        "print({'roundtrip_max_abs_error': rt_max_abs, 'roundtrip_mean_abs_error': rt_mean_abs})\n",
        "assert rt_max_abs < 1e-3, 'Round-trip transform error too large.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split/window utilities and walk-forward slice definitions\n",
        "def fit_standardizer(train_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "    mean = train_values.mean(axis=0)\n",
        "    std = train_values.std(axis=0)\n",
        "    std = np.where(std < 1e-8, 1.0, std)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "\n",
        "def apply_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return ((values - mean) / std).astype(np.float32)\n",
        "\n",
        "\n",
        "def undo_standardizer(values_scaled: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (values_scaled * std + mean).astype(np.float32)\n",
        "\n",
        "\n",
        "def make_windows(values: np.ndarray, window: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    X, y, idx = [], [], []\n",
        "    for i in range(window, len(values)):\n",
        "        X.append(values[i - window : i])\n",
        "        y.append(values[i])\n",
        "        idx.append(i)\n",
        "\n",
        "    return (\n",
        "        np.asarray(X, dtype=np.float32),\n",
        "        np.asarray(y, dtype=np.float32),\n",
        "        np.asarray(idx, dtype=np.int64),\n",
        "    )\n",
        "\n",
        "\n",
        "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
        "    n = len(price_df_full)\n",
        "    span = int(round(n * 0.85))\n",
        "    shift = max(1, n - span)\n",
        "\n",
        "    s1 = (0, min(span, n))\n",
        "    s2 = (shift, min(shift + span, n))\n",
        "\n",
        "    slices = [('slice_1', s1[0], s1[1]), ('slice_2', s2[0], s2[1])]\n",
        "\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for name, a, b in slices:\n",
        "        key = (a, b)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        if b - a < (WINDOW + HORIZON + 1000):\n",
        "            continue\n",
        "        unique.append((name, a, b))\n",
        "        seen.add(key)\n",
        "\n",
        "    if not unique:\n",
        "        raise RuntimeError('Unable to construct walk-forward slices with enough rows.')\n",
        "\n",
        "    return unique\n",
        "\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "slices = build_walkforward_slices(price_df)\n",
        "print('Walk-forward slices:', slices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model, loss, and training utilities\n",
        "class GRUForecaster(nn.Module):\n",
        "    def __init__(self, input_size: int = 4, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out, _ = self.gru(x)\n",
        "        return self.head(out[:, -1, :])\n",
        "\n",
        "\n",
        "class WeightedHuberLoss(nn.Module):\n",
        "    def __init__(self, weights: np.ndarray, delta: float = 1.0):\n",
        "        super().__init__()\n",
        "        w = torch.as_tensor(weights, dtype=torch.float32)\n",
        "        self.register_buffer('weights', w)\n",
        "        self.delta = float(delta)\n",
        "\n",
        "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        err = pred - target\n",
        "        abs_err = torch.abs(err)\n",
        "        huber = torch.where(\n",
        "            abs_err <= self.delta,\n",
        "            0.5 * (err ** 2),\n",
        "            self.delta * (abs_err - 0.5 * self.delta),\n",
        "        )\n",
        "        weighted = huber * self.weights\n",
        "        return weighted.mean()\n",
        "\n",
        "\n",
        "def run_epoch(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, optimizer: torch.optim.Optimizer | None = None) -> float:\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_items = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_items += bs\n",
        "\n",
        "    return total_loss / max(total_items, 1)\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader) -> pd.DataFrame:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "    loss_fn = WeightedHuberLoss(weights=LOSS_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    wait = 0\n",
        "    rows = []\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        train_loss = run_epoch(model, train_loader, loss_fn, optimizer=optimizer)\n",
        "        val_loss = run_epoch(model, val_loader, loss_fn, optimizer=None)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        rows.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'lr': lr})\n",
        "\n",
        "        print(f'Epoch {epoch:02d} | train={train_loss:.6f} | val={val_loss:.6f} | lr={lr:.6g}')\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f'Early stopping at epoch {epoch}.')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics, baselines, recursive rollout, and fold runner\n",
        "def rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b)) ** 2)))\n",
        "\n",
        "\n",
        "def evaluate_metrics(actual_ohlc: np.ndarray, pred_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
        "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
        "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
        "    prev_close = np.asarray(prev_close, dtype=np.float32)\n",
        "\n",
        "    actual_close = actual_ohlc[:, 3]\n",
        "    pred_close = pred_ohlc[:, 3]\n",
        "\n",
        "    out = {\n",
        "        'close_mae': float(np.mean(np.abs(actual_close - pred_close))),\n",
        "        'close_rmse': rmse(actual_close, pred_close),\n",
        "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
        "        'ohlc_rmse': rmse(actual_ohlc.reshape(-1), pred_ohlc.reshape(-1)),\n",
        "        'directional_accuracy': float(np.mean(np.sign(actual_close - prev_close) == np.sign(pred_close - prev_close))),\n",
        "        'mean_signed_bias': float(np.mean(pred_close - actual_close)),\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def evaluate_baselines(actual_ohlc: np.ndarray, prev_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
        "    baseline_persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
        "\n",
        "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
        "    baseline_flat = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
        "\n",
        "    return {'persistence': baseline_persistence, 'flat_close_rw': baseline_flat}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_preds(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yhat = model(xb).cpu().numpy()\n",
        "        preds.append(yhat)\n",
        "        targets.append(yb.numpy())\n",
        "\n",
        "    return np.vstack(preds).astype(np.float32), np.vstack(targets).astype(np.float32)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def recursive_forecast_returns(\n",
        "    model: nn.Module,\n",
        "    seed_window_scaled: np.ndarray,\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "    model.eval()\n",
        "    window = seed_window_scaled.copy().astype(np.float32)\n",
        "    preds = []\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        x = torch.from_numpy(window).float().unsqueeze(0).to(DEVICE)\n",
        "        pred_scaled = model(x).cpu().numpy()[0]\n",
        "\n",
        "        pred_ret = undo_standardizer(pred_scaled.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "        pred_ret = np.clip(pred_ret, clip_low, clip_high).astype(np.float32)\n",
        "\n",
        "        pred_scaled_clipped = apply_standardizer(pred_ret.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "\n",
        "        preds.append(pred_ret)\n",
        "        window = np.vstack([window[1:], pred_scaled_clipped]).astype(np.float32)\n",
        "\n",
        "    return np.asarray(preds, dtype=np.float32)\n",
        "\n",
        "\n",
        "def evaluate_recursive_by_horizon(\n",
        "    model: nn.Module,\n",
        "    returns_scaled: np.ndarray,\n",
        "    price_ohlc: np.ndarray,\n",
        "    close_values: np.ndarray,\n",
        "    anchors: list[int],\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        ") -> pd.DataFrame:\n",
        "    model_err = [[] for _ in range(horizon)]\n",
        "    persist_err = [[] for _ in range(horizon)]\n",
        "    flat_err = [[] for _ in range(horizon)]\n",
        "\n",
        "    for anchor in anchors:\n",
        "        seed = returns_scaled[anchor - WINDOW + 1 : anchor + 1]\n",
        "        last_close = float(close_values[anchor + 1])\n",
        "\n",
        "        pred_ret = recursive_forecast_returns(\n",
        "            model=model,\n",
        "            seed_window_scaled=seed,\n",
        "            horizon=horizon,\n",
        "            scale_mean=scale_mean,\n",
        "            scale_std=scale_std,\n",
        "            clip_low=clip_low,\n",
        "            clip_high=clip_high,\n",
        "        )\n",
        "        pred_price = targets_to_prices_seq(pred_ret, last_close=last_close)\n",
        "\n",
        "        actual = price_ohlc[anchor + 2 : anchor + 2 + horizon]\n",
        "\n",
        "        persist_seq = np.repeat(price_ohlc[anchor + 1 : anchor + 2], horizon, axis=0)\n",
        "        flat_c = close_values[anchor + 1]\n",
        "        flat_seq = np.repeat(np.array([[flat_c, flat_c, flat_c, flat_c]], dtype=np.float32), horizon, axis=0)\n",
        "\n",
        "        for h in range(horizon):\n",
        "            model_err[h].append(abs(float(pred_price[h, 3] - actual[h, 3])))\n",
        "            persist_err[h].append(abs(float(persist_seq[h, 3] - actual[h, 3])))\n",
        "            flat_err[h].append(abs(float(flat_seq[h, 3] - actual[h, 3])))\n",
        "\n",
        "    rows = []\n",
        "    for h in range(horizon):\n",
        "        rows.append({\n",
        "            'horizon': h + 1,\n",
        "            'model_close_mae': float(np.mean(model_err[h])) if model_err[h] else np.nan,\n",
        "            'persistence_close_mae': float(np.mean(persist_err[h])) if persist_err[h] else np.nan,\n",
        "            'flat_close_mae': float(np.mean(flat_err[h])) if flat_err[h] else np.nan,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def split_points(n_rows: int) -> tuple[int, int]:\n",
        "    train_end = int(n_rows * TRAIN_RATIO)\n",
        "    val_end = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
        "    return train_end, val_end\n",
        "\n",
        "\n",
        "def overfit_sanity_check(X_train: np.ndarray, y_train: np.ndarray) -> dict:\n",
        "    n = min(1000, len(X_train))\n",
        "    if n < 300:\n",
        "        return {'ran': False, 'reason': 'too_few_train_samples'}\n",
        "\n",
        "    Xs = X_train[:n]\n",
        "    ys = y_train[:n]\n",
        "\n",
        "    ds = SequenceDataset(Xs, ys)\n",
        "    dl = DataLoader(ds, batch_size=128, shuffle=True, drop_last=False)\n",
        "\n",
        "    model = GRUForecaster(input_size=4, hidden_size=64, num_layers=1, dropout=0.0).to(DEVICE)\n",
        "    loss_fn = WeightedHuberLoss(weights=LOSS_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "\n",
        "    init_loss = run_epoch(model, dl, loss_fn, optimizer=None)\n",
        "    for _ in range(12):\n",
        "        _ = run_epoch(model, dl, loss_fn, optimizer=opt)\n",
        "    final_loss = run_epoch(model, dl, loss_fn, optimizer=None)\n",
        "\n",
        "    passed = bool(final_loss < init_loss * 0.6)\n",
        "    return {'ran': True, 'initial_loss': float(init_loss), 'final_loss': float(final_loss), 'passed': passed}\n",
        "\n",
        "\n",
        "def run_fold(fold_name: str, price_fold: pd.DataFrame, run_sanity: bool = False) -> dict:\n",
        "    targets_fold, prev_close_fold = prices_to_targets(price_fold)\n",
        "\n",
        "    ret_values = targets_fold[FEATURES].to_numpy(np.float32)\n",
        "    prev_close_values = prev_close_fold.to_numpy(np.float32)\n",
        "\n",
        "    price_values = price_fold[FEATURES].to_numpy(np.float32)\n",
        "    close_values = price_fold['Close'].to_numpy(np.float32)\n",
        "\n",
        "    train_end, val_end = split_points(len(ret_values))\n",
        "\n",
        "    if train_end <= WINDOW or val_end <= train_end:\n",
        "        raise RuntimeError(f'{fold_name}: invalid split points for return rows={len(ret_values)}')\n",
        "\n",
        "    scale_mean, scale_std = fit_standardizer(ret_values[:train_end])\n",
        "    ret_scaled = apply_standardizer(ret_values, scale_mean, scale_std)\n",
        "\n",
        "    q_low, q_high = CLIP_TARGET_QUANTILES\n",
        "    clip_low = np.quantile(ret_values[:train_end], q_low, axis=0).astype(np.float32)\n",
        "    clip_high = np.quantile(ret_values[:train_end], q_high, axis=0).astype(np.float32)\n",
        "\n",
        "    X_all, y_all, target_idx = make_windows(ret_scaled, WINDOW)\n",
        "\n",
        "    train_mask = target_idx < train_end\n",
        "    val_mask = (target_idx >= train_end) & (target_idx < val_end)\n",
        "    test_mask = target_idx >= val_end\n",
        "\n",
        "    X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
        "    X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
        "    X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
        "\n",
        "    if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
        "        raise RuntimeError(f'{fold_name}: one or more splits are empty after windowing.')\n",
        "\n",
        "    sanity = overfit_sanity_check(X_train, y_train) if run_sanity else {'ran': False}\n",
        "\n",
        "    train_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(SequenceDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "    test_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = GRUForecaster(input_size=4, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT).to(DEVICE)\n",
        "    history_df = train_model(model, train_loader, val_loader)\n",
        "\n",
        "    pred_scaled, y_scaled = collect_preds(model, test_loader)\n",
        "    pred_ret = undo_standardizer(pred_scaled, scale_mean, scale_std)\n",
        "    y_ret = undo_standardizer(y_scaled, scale_mean, scale_std)\n",
        "\n",
        "    test_idx = target_idx[test_mask]\n",
        "\n",
        "    prev_close_t = prev_close_values[test_idx]\n",
        "    actual_ohlc = price_values[test_idx + 1]\n",
        "\n",
        "    pred_ohlc = one_step_targets_to_prices_batch(pred_ret, prev_close_t)\n",
        "    prev_ohlc = price_values[test_idx]\n",
        "\n",
        "    model_metrics = evaluate_metrics(actual_ohlc, pred_ohlc, prev_close_t)\n",
        "    baseline_metrics = evaluate_baselines(actual_ohlc, prev_ohlc, prev_close_t)\n",
        "\n",
        "    anchors = sorted(set(int(i - 1) for i in test_idx))\n",
        "    max_anchor = len(ret_values) - HORIZON - 1\n",
        "    anchors = [a for a in anchors if a >= (WINDOW - 1) and a <= max_anchor]\n",
        "    if not anchors:\n",
        "        raise RuntimeError(f'{fold_name}: no eligible anchors for recursive evaluation.')\n",
        "\n",
        "    horizon_df = evaluate_recursive_by_horizon(\n",
        "        model=model,\n",
        "        returns_scaled=ret_scaled,\n",
        "        price_ohlc=price_values,\n",
        "        close_values=close_values,\n",
        "        anchors=anchors,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=scale_mean,\n",
        "        scale_std=scale_std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "    )\n",
        "\n",
        "    last_anchor = anchors[-1]\n",
        "    seed_scaled = ret_scaled[last_anchor - WINDOW + 1 : last_anchor + 1]\n",
        "    last_close = float(close_values[last_anchor + 1])\n",
        "\n",
        "    pred_ret_path = recursive_forecast_returns(\n",
        "        model=model,\n",
        "        seed_window_scaled=seed_scaled,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=scale_mean,\n",
        "        scale_std=scale_std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "    )\n",
        "    pred_price_path = targets_to_prices_seq(pred_ret_path, last_close=last_close)\n",
        "    actual_price_path = price_values[last_anchor + 2 : last_anchor + 2 + HORIZON]\n",
        "\n",
        "    future_index = price_fold.index[last_anchor + 2 : last_anchor + 2 + HORIZON]\n",
        "    pred_future_df = pd.DataFrame(pred_price_path, index=future_index, columns=FEATURES)\n",
        "    actual_future_df = pd.DataFrame(actual_price_path, index=future_index, columns=FEATURES)\n",
        "\n",
        "    known_pos = last_anchor + 1\n",
        "    context_bars = 170\n",
        "    c_start = max(0, known_pos - context_bars + 1)\n",
        "    context_df = price_fold.iloc[c_start : known_pos + 1][FEATURES].copy()\n",
        "\n",
        "    step15_row = horizon_df[horizon_df['horizon'] == HORIZON].iloc[0]\n",
        "\n",
        "    out = {\n",
        "        'fold': fold_name,\n",
        "        'history_df': history_df,\n",
        "        'model_metrics': model_metrics,\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'horizon_df': horizon_df,\n",
        "        'context_df': context_df,\n",
        "        'actual_future_df': actual_future_df,\n",
        "        'pred_future_df': pred_future_df,\n",
        "        'pred_ret_path': pred_ret_path,\n",
        "        'sanity': sanity,\n",
        "        'samples': {\n",
        "            'train': int(len(X_train)),\n",
        "            'val': int(len(X_val)),\n",
        "            'test': int(len(X_test)),\n",
        "            'anchors': int(len(anchors)),\n",
        "        },\n",
        "        'step15': {\n",
        "            'model_close_mae': float(step15_row['model_close_mae']),\n",
        "            'persistence_close_mae': float(step15_row['persistence_close_mae']),\n",
        "            'flat_close_mae': float(step15_row['flat_close_mae']),\n",
        "        },\n",
        "    }\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run walk-forward folds\n",
        "fold_results = []\n",
        "\n",
        "for i, (name, start_i, end_i) in enumerate(slices, start=1):\n",
        "    print(f'\\n=== Running {name} [{start_i}:{end_i}] ===')\n",
        "    fold_price = price_df.iloc[start_i:end_i].copy()\n",
        "\n",
        "    result = run_fold(name, fold_price, run_sanity=(i == 1))\n",
        "    fold_results.append(result)\n",
        "\n",
        "    print('Sample counts:', result['samples'])\n",
        "    print('Sanity check:', result['sanity'])\n",
        "    print('One-step model:', result['model_metrics'])\n",
        "    print('One-step persistence:', result['baseline_metrics']['persistence'])\n",
        "    print('One-step flat_close_rw:', result['baseline_metrics']['flat_close_rw'])\n",
        "    print('Recursive step-15:', result['step15'])\n",
        "\n",
        "if not fold_results:\n",
        "    raise RuntimeError('No fold results were produced.')\n",
        "\n",
        "summary_rows = []\n",
        "for r in fold_results:\n",
        "    m = r['model_metrics']\n",
        "    b1 = r['baseline_metrics']['persistence']\n",
        "    b2 = r['baseline_metrics']['flat_close_rw']\n",
        "    s15 = r['step15']\n",
        "\n",
        "    summary_rows.append({\n",
        "        'fold': r['fold'],\n",
        "        'model_close_mae': m['close_mae'],\n",
        "        'model_directional_acc': m['directional_accuracy'],\n",
        "        'model_mean_signed_bias': m['mean_signed_bias'],\n",
        "        'persistence_close_mae': b1['close_mae'],\n",
        "        'flat_close_mae': b2['close_mae'],\n",
        "        'recursive_step15_model_mae': s15['model_close_mae'],\n",
        "        'recursive_step15_persistence_mae': s15['persistence_close_mae'],\n",
        "        'recursive_step15_flat_mae': s15['flat_close_mae'],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "display(summary_df)\n",
        "\n",
        "metric_cols = [c for c in summary_df.columns if c != 'fold']\n",
        "agg_df = pd.DataFrame({'mean': summary_df[metric_cols].mean(), 'std': summary_df[metric_cols].std(ddof=0)})\n",
        "display(agg_df)\n",
        "\n",
        "horizon_all = pd.concat([r['horizon_df'].assign(fold=r['fold']) for r in fold_results], ignore_index=True)\n",
        "horizon_stats = horizon_all.groupby('horizon', as_index=False)[['model_close_mae', 'persistence_close_mae', 'flat_close_mae']].mean()\n",
        "display(horizon_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Acceptance checks + diagnostic plots\n",
        "latest = fold_results[-1]\n",
        "\n",
        "mean_model_mae = float(summary_df['model_close_mae'].mean())\n",
        "mean_persist_mae = float(summary_df['persistence_close_mae'].mean())\n",
        "mean_dir = float(summary_df['model_directional_acc'].mean())\n",
        "mean_bias = float(summary_df['model_mean_signed_bias'].mean())\n",
        "mean_step15_model = float(summary_df['recursive_step15_model_mae'].mean())\n",
        "mean_step15_persist = float(summary_df['recursive_step15_persistence_mae'].mean())\n",
        "\n",
        "pred_close_path = latest['pred_future_df']['Close'].to_numpy()\n",
        "is_monotonic = bool(np.all(np.diff(pred_close_path) >= 0) or np.all(np.diff(pred_close_path) <= 0))\n",
        "\n",
        "acceptance = {\n",
        "    'criterion_1_model_mae_20pct_better_than_persistence': mean_model_mae <= 0.8 * mean_persist_mae,\n",
        "    'criterion_2_directional_accuracy_at_least_0_52': mean_dir >= 0.52,\n",
        "    'criterion_3_step15_better_than_persistence': mean_step15_model < mean_step15_persist,\n",
        "    'criterion_4_abs_bias_within_25pct_of_mae': abs(mean_bias) <= 0.25 * mean_model_mae,\n",
        "    'criterion_5_non_monotonic_latest_prediction_path': not is_monotonic,\n",
        "}\n",
        "\n",
        "print('Acceptance checks:')\n",
        "for k, v in acceptance.items():\n",
        "    print(f'  {k}: {v}')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4.8), facecolor='white')\n",
        "\n",
        "hist_df = latest['history_df']\n",
        "axes[0].plot(hist_df['epoch'], hist_df['train_loss'], label='Train', color='black')\n",
        "axes[0].plot(hist_df['epoch'], hist_df['val_loss'], label='Validation', color='gray')\n",
        "axes[0].set_title(f\"Loss Curves ({latest['fold']})\")\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Weighted Huber')\n",
        "axes[0].grid(alpha=0.25)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['model_close_mae'], label='Model', color='black', linewidth=2.0)\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['persistence_close_mae'], label='Persistence', color='#E74C3C')\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['flat_close_mae'], label='Flat RW', color='#3498DB')\n",
        "axes[1].set_title('Recursive Close MAE by Horizon (mean across folds)')\n",
        "axes[1].set_xlabel('Horizon step')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].grid(alpha=0.25)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final candlestick chart: history green/red, predicted white/black\n",
        "def draw_candles(\n",
        "    ax,\n",
        "    ohlc: pd.DataFrame,\n",
        "    start_x: int,\n",
        "    up_edge: str,\n",
        "    up_face: str,\n",
        "    down_edge: str,\n",
        "    down_face: str,\n",
        "    wick_color: str,\n",
        "    width: float = 0.62,\n",
        "    lw: float = 1.0,\n",
        "    alpha: float = 1.0,\n",
        "):\n",
        "    values = ohlc[['Open', 'High', 'Low', 'Close']].to_numpy()\n",
        "\n",
        "    for i, (o, h, l, c) in enumerate(values):\n",
        "        x = start_x + i\n",
        "        bullish = c >= o\n",
        "\n",
        "        ax.vlines(x, l, h, color=wick_color, linewidth=lw, alpha=alpha, zorder=2)\n",
        "\n",
        "        lower = min(o, c)\n",
        "        height = abs(c - o)\n",
        "        if height < 1e-8:\n",
        "            height = 1e-6\n",
        "\n",
        "        face = up_face if bullish else down_face\n",
        "        edge = up_edge if bullish else down_edge\n",
        "\n",
        "        rect = Rectangle(\n",
        "            (x - width / 2, lower),\n",
        "            width,\n",
        "            height,\n",
        "            facecolor=face,\n",
        "            edgecolor=edge,\n",
        "            linewidth=lw,\n",
        "            alpha=alpha,\n",
        "            zorder=3,\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "\n",
        "context_df = latest['context_df']\n",
        "actual_future_df = latest['actual_future_df']\n",
        "pred_future_df = latest['pred_future_df']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(17, 8), facecolor='black')\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "# Historical candles: green/red\n",
        "draw_candles(\n",
        "    ax,\n",
        "    context_df,\n",
        "    start_x=0,\n",
        "    up_edge='#2ECC71',\n",
        "    up_face='#2ECC71',\n",
        "    down_edge='#E74C3C',\n",
        "    down_face='#E74C3C',\n",
        "    wick_color='#DADADA',\n",
        "    width=0.58,\n",
        "    lw=1.0,\n",
        "    alpha=0.95,\n",
        ")\n",
        "\n",
        "# Actual future candles (reference): muted green/red\n",
        "draw_candles(\n",
        "    ax,\n",
        "    actual_future_df,\n",
        "    start_x=len(context_df),\n",
        "    up_edge='#1D6F42',\n",
        "    up_face='#1D6F42',\n",
        "    down_edge='#8E2F25',\n",
        "    down_face='#8E2F25',\n",
        "    wick_color='#9A9A9A',\n",
        "    width=0.58,\n",
        "    lw=1.0,\n",
        "    alpha=0.70,\n",
        ")\n",
        "\n",
        "# Predicted candles: white (bullish), black (bearish)\n",
        "draw_candles(\n",
        "    ax,\n",
        "    pred_future_df,\n",
        "    start_x=len(context_df),\n",
        "    up_edge='#FFFFFF',\n",
        "    up_face='#FFFFFF',\n",
        "    down_edge='#000000',\n",
        "    down_face='#000000',\n",
        "    wick_color='#F5F5F5',\n",
        "    width=0.50,\n",
        "    lw=1.35,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "split_x = len(context_df) - 0.5\n",
        "ax.axvline(split_x, color='white', linestyle='--', linewidth=0.9, alpha=0.6)\n",
        "\n",
        "combined_index = context_df.index.append(actual_future_df.index)\n",
        "total_bars = len(combined_index)\n",
        "tick_step = max(1, total_bars // 10)\n",
        "ticks = list(range(0, total_bars, tick_step))\n",
        "if ticks[-1] != total_bars - 1:\n",
        "    ticks.append(total_bars - 1)\n",
        "\n",
        "labels = [combined_index[i].strftime('%m-%d %H:%M') for i in ticks]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(labels, rotation=26, ha='right', color='white', fontsize=9)\n",
        "\n",
        "ax.tick_params(axis='y', colors='white')\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_color('#666666')\n",
        "\n",
        "ax.grid(color='#252525', linewidth=0.6, alpha=0.35)\n",
        "ax.set_title(\n",
        "    f'MSFT 1m ({latest[\"fold\"]}) - History vs 15-step Recursive Forecast',\n",
        "    color='white',\n",
        "    pad=14,\n",
        ")\n",
        "ax.set_ylabel('Price', color='white')\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='History bullish (green)'),\n",
        "    Patch(facecolor='#E74C3C', edgecolor='#E74C3C', label='History bearish (red)'),\n",
        "    Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted bullish (white)'),\n",
        "    Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted bearish (black)'),\n",
        "]\n",
        "leg = ax.legend(handles=legend_elements, facecolor='black', edgecolor='#707070', framealpha=1.0, loc='upper left')\n",
        "for text in leg.get_texts():\n",
        "    text.set_color('white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- This version keeps recursive forecasting (no direct multi-horizon head) per your PRD constraint.\n",
        "- Targets are transformed candle-price returns (not technical indicators).\n",
        "- If `delayed_sip` is unavailable for your account, notebook fails with a clear setup message by design.\n",
        "- Compare model metrics against persistence/flat baselines before trusting visual fit.\n",
        "- Free-plan `iex` is single-venue and can be sparse versus SIP; expect less coverage/noisier microstructure than full consolidated feeds.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
