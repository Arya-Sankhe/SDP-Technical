{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: MSFT 1-Minute GRU Forecast - Regime-Conditional Sampling (v7.3)\n",
    "\n",
    "Key changes for regime-aware prediction:\n",
    "1. **Market Regime Detection**: Calculate ADX over context window to detect trending vs ranging\n",
    "2. **Regime-Conditional Sampling**:\n",
    "   - ADX > 25 (trending): Lower temperature (0.8) + momentum drift based on 10-bar slope\n",
    "   - ADX â‰¤ 25 (ranging): Higher temperature (1.5) + mean-reversion bias toward VWAP\n",
    "3. Regime indicator added as input feature\n",
    "4. Probabilistic outputs with regime-dependent temperature control\n",
    "5. Strict candle validity enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    'alpaca': 'alpaca-py',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'pandas_market_calendars': 'pandas-market-calendars',\n",
    "}\n",
    "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "else:\n",
    "    print('All required third-party packages are already installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "SYMBOL = 'MSFT'\n",
    "LOOKBACK_DAYS = 120\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "\n",
    "# Feature columns now include regime indicator\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen', 'rHigh', 'rLow', 'rClose',\n",
    "    'logVolChange', 'logTradeCountChange',\n",
    "    'vwapDelta', 'rangeFrac', 'orderFlowProxy', 'tickPressure',\n",
    "    'regimeIndicator',  # NEW: regime indicator as input feature\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
    "\n",
    "HORIZON = 15\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "LOOKBACK_CANDIDATES = [64, 96, 160, 256]\n",
    "DEFAULT_LOOKBACK = 96\n",
    "ENABLE_LOOKBACK_SWEEP = True\n",
    "SKIP_OPEN_BARS_TARGET = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.20\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "SWEEP_MAX_EPOCHS = 15\n",
    "SWEEP_PATIENCE = 5\n",
    "FINAL_MAX_EPOCHS = 60\n",
    "FINAL_PATIENCE = 12\n",
    "TF_START = 1.0\n",
    "TF_END = 0.0\n",
    "TF_DECAY_RATE = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Configuration\n",
    "RANGE_LOSS_WEIGHT = 0.3\n",
    "VOLATILITY_WEIGHT = 0.5\n",
    "DIR_PENALTY_WEIGHT = 0.1\n",
    "STEP_LOSS_POWER = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Configuration - Regime-Conditional\n",
    "TEMP_TRENDING = 0.8       # Lower temp for more confident predictions in trends\n",
    "TEMP_RANGING = 1.5        # Higher temp for ranging markets\n",
    "ADX_PERIOD = 14           # Period for ADX calculation\n",
    "ADX_THRESHOLD = 25.0      # Threshold for trending vs ranging\n",
    "MOMENTUM_WINDOW = 10      # Window for momentum slope calculation\n",
    "MOMENTUM_DRIFT_SCALE = 0.3  # Scale factor for momentum drift\n",
    "MEAN_REVERSION_STRENGTH = 0.2  # Strength of mean-reversion pull\n",
    "VOLATILITY_SCALING = True\n",
    "MIN_PREDICTED_VOL = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Configuration\n",
    "STANDARDIZE_TARGETS = False\n",
    "APPLY_CLIPPING = True\n",
    "CLIP_QUANTILES = (0.001, 0.999)\n",
    "DIRECTION_EPS = 0.0001\n",
    "STD_RATIO_TARGET_MIN = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca API Configuration\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Configuration Summary\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'lookback_days': LOOKBACK_DAYS,\n",
    "    'horizon': HORIZON,\n",
    "    'temp_trending': TEMP_TRENDING,\n",
    "    'temp_ranging': TEMP_RANGING,\n",
    "    'adx_threshold': ADX_THRESHOLD,\n",
    "    'loss_weights': {\n",
    "        'range': RANGE_LOSS_WEIGHT,\n",
    "        'volatility': VOLATILITY_WEIGHT,\n",
    "        'dir_penalty': DIR_PENALTY_WEIGHT,\n",
    "    },\n",
    "    'device': str(DEVICE),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regime Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adx(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Average Directional Index (ADX) for trend strength detection.\n",
    "    Returns ADX values (0-100 scale).\n",
    "    \"\"\"\n",
    "    high = np.asarray(high, dtype=np.float64)\n",
    "    low = np.asarray(low, dtype=np.float64)\n",
    "    close = np.asarray(close, dtype=np.float64)\n",
    "    \n",
    "    # True Range\n",
    "    tr1 = high[1:] - low[1:]\n",
    "    tr2 = np.abs(high[1:] - close[:-1])\n",
    "    tr3 = np.abs(low[1:] - close[:-1])\n",
    "    tr = np.maximum.reduce([tr1, tr2, tr3])\n",
    "    \n",
    "    # Plus Directional Movement (+DM)\n",
    "    plus_dm = np.zeros_like(high)\n",
    "    plus_dm[1:] = np.where(\n",
    "        (high[1:] - high[:-1]) > (low[:-1] - low[1:]),\n",
    "        np.maximum(high[1:] - high[:-1], 0),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Minus Directional Movement (-DM)\n",
    "    minus_dm = np.zeros_like(low)\n",
    "    minus_dm[1:] = np.where(\n",
    "        (low[:-1] - low[1:]) > (high[1:] - high[:-1]),\n",
    "        np.maximum(low[:-1] - low[1:], 0),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Smooth TR, +DM, -DM using Wilder's smoothing\n",
    "    def wilder_smooth(data, period):\n",
    "        smoothed = np.zeros_like(data)\n",
    "        smoothed[period] = np.sum(data[1:period+1])\n",
    "        for i in range(period + 1, len(data)):\n",
    "            smoothed[i] = smoothed[i-1] - (smoothed[i-1] / period) + data[i]\n",
    "        return smoothed\n",
    "    \n",
    "    atr = wilder_smooth(np.concatenate([[tr[0]], tr]), period)\n",
    "    smoothed_plus_dm = wilder_smooth(plus_dm, period)\n",
    "    smoothed_minus_dm = wilder_smooth(minus_dm, period)\n",
    "    \n",
    "    # +DI and -DI\n",
    "    plus_di = 100 * smoothed_plus_dm / (atr + 1e-10)\n",
    "    minus_di = 100 * smoothed_minus_dm / (atr + 1e-10)\n",
    "    \n",
    "    # DX and ADX\n",
    "    dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)\n",
    "    \n",
    "    adx = np.zeros_like(dx)\n",
    "    adx[period*2-1] = np.mean(dx[period:period*2])\n",
    "    for i in range(period*2, len(dx)):\n",
    "        adx[i] = (adx[i-1] * (period - 1) + dx[i]) / period\n",
    "    \n",
    "    return adx\n",
    "\n",
    "\n",
    "def detect_regime(ohlc: np.ndarray, period: int = 14) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Detect market regime based on ADX.\n",
    "    Returns (adx_value, regime_name)\n",
    "    \"\"\"\n",
    "    if len(ohlc) < period * 2:\n",
    "        return 25.0, 'ranging'  # Default to ranging if not enough data\n",
    "    \n",
    "    high = ohlc[:, 1]  # High prices\n",
    "    low = ohlc[:, 2]   # Low prices\n",
    "    close = ohlc[:, 3]  # Close prices\n",
    "    \n",
    "    adx_values = calculate_adx(high, low, close, period)\n",
    "    current_adx = adx_values[-1]\n",
    "    \n",
    "    # ADX > 25 indicates trending market\n",
    "    regime = 'trending' if current_adx > ADX_THRESHOLD else 'ranging'\n",
    "    \n",
    "    return float(current_adx), regime\n",
    "\n",
    "\n",
    "def calculate_momentum_slope(close_prices: np.ndarray, window: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate momentum slope for trending markets.\n",
    "    Returns normalized slope (log returns per bar).\n",
    "    \"\"\"\n",
    "    if len(close_prices) < window:\n",
    "        return 0.0\n",
    "    \n",
    "    recent_closes = close_prices[-window:]\n",
    "    # Linear regression slope in log space\n",
    "    log_prices = np.log(recent_closes)\n",
    "    x = np.arange(len(log_prices))\n",
    "    slope = np.polyfit(x, log_prices, 1)[0]\n",
    "    \n",
    "    return float(slope)\n",
    "\n",
    "\n",
    "def calculate_mean_reversion_bias(predicted_close: float, vwap: float, current_close: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate mean-reversion bias for ranging markets.\n",
    "    Returns bias term to pull predictions toward VWAP.\n",
    "    \"\"\"\n",
    "    if vwap <= 0 or current_close <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Log deviation from VWAP\n",
    "    deviation = np.log(vwap / current_close)\n",
    "    return deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be >0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "        \n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "    \n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "    \n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(columns={\n",
    "                'timestamp': 'Datetime', 'open': 'Open', 'high': 'High',\n",
    "                'low': 'Low', 'close': 'Close', 'volume': 'Volume',\n",
    "                'trade_count': 'TradeCount', 'vwap': 'VWAP',\n",
    "            })\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "            \n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "        cursor = chunk_end\n",
    "    \n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "    \n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    \n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "    \n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "    \n",
    "    pieces = []\n",
    "    fill_ratios = []\n",
    "    \n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        \n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "            \n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)]\n",
    "        day = day.reindex(exp_idx)\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "        \n",
    "        if fill_ratio >= 1.0 or fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            continue\n",
    "            \n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        if day['VWAP'].isna().all():\n",
    "            day['VWAP'] = day['Close']\n",
    "        else:\n",
    "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "            \n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "        \n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after per-session fill.')\n",
    "        pieces.append(day)\n",
    "        fill_ratios.append(fill_ratio)\n",
    "    \n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after calendar filtering.')\n",
    "        \n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "    out = out.copy()\n",
    "    \n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "    \n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(len(pieces)),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "    }\n",
    "    return out, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
    "print(f'Sessionized rows kept: {len(price_df):,}')\n",
    "print('Session meta:', session_meta)\n",
    "\n",
    "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
    "if len(price_df) < min_needed:\n",
    "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure High >= max(Open,Close) and Low <= min(Open,Close)\"\"\"\n",
    "    out = np.asarray(ohlc, dtype=np.float32)\n",
    "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build feature frame with regime indicator.\n",
    "    Regime is calculated based on ADX over a rolling window.\n",
    "    \"\"\"\n",
    "    eps = 1e-9\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "    \n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0) / (np.abs(out['rClose']) + eps)\n",
    "    \n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "    \n",
    "    # Calculate rolling ADX and regime indicator\n",
    "    ohlc_data = df[OHLC_COLS].to_numpy(dtype=np.float32)\n",
    "    regime_indicators = np.zeros(len(df), dtype=np.float32)\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i >= ADX_PERIOD * 2:\n",
    "            _, regime = detect_regime(ohlc_data[:i+1], ADX_PERIOD)\n",
    "            regime_indicators[i] = 1.0 if regime == 'trending' else 0.0\n",
    "        else:\n",
    "            regime_indicators[i] = 0.5  # Neutral for insufficient data\n",
    "    \n",
    "    out['regimeIndicator'] = regime_indicators\n",
    "    \n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def build_target_frame(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return feat_df[TARGET_COLS].copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = build_feature_frame(price_df)\n",
    "target_df = build_target_frame(feat_df)\n",
    "print('Feature rows:', len(feat_df))\n",
    "print('Target columns:', list(target_df.columns))\n",
    "print('Feature columns:', list(feat_df.columns))\n",
    "print(f\"Regime distribution: Trending={(feat_df['regimeIndicator'] > 0.5).sum()}, Ranging={(feat_df['regimeIndicator'] <= 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing & Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_points(n_rows: int) -> tuple[int, int]:\n",
    "    tr = int(n_rows * TRAIN_RATIO)\n",
    "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
    "    return tr, va\n",
    "\n",
    "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
    "    n = len(price_df_full)\n",
    "    span = int(round(n * 0.85))\n",
    "    shift = max(1, n - span)\n",
    "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for name, a, b in cands:\n",
    "        key = (a, b)\n",
    "        if key in seen or b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1400:\n",
    "            continue\n",
    "        out.append((name, a, b))\n",
    "        seen.add(key)\n",
    "    return out if out else [('full', 0, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multistep_windows(input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, \n",
    "                           starts_prev_close, window, horizon):\n",
    "    X, y_s, y_r, starts, prev_close = [], [], [], [], []\n",
    "    dropped_target_imputed, dropped_target_open_skip = 0, 0\n",
    "    n = len(input_scaled)\n",
    "    \n",
    "    for i in range(window, n - horizon + 1):\n",
    "        if row_imputed[i:i+horizon].any():\n",
    "            dropped_target_imputed += 1\n",
    "            continue\n",
    "        if row_open_skip[i:i+horizon].any():\n",
    "            dropped_target_open_skip += 1\n",
    "            continue\n",
    "            \n",
    "        xb = input_scaled[i-window:i]\n",
    "        imp_frac = float(row_imputed[i-window:i].mean())\n",
    "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
    "        xb_aug = np.concatenate([xb, imp_col], axis=1)\n",
    "        \n",
    "        X.append(xb_aug)\n",
    "        y_s.append(target_scaled[i:i+horizon])\n",
    "        y_r.append(target_raw[i:i+horizon])\n",
    "        starts.append(i)\n",
    "        prev_close.append(starts_prev_close[i])\n",
    "    \n",
    "    return (np.asarray(X, dtype=np.float32), np.asarray(y_s, dtype=np.float32),\n",
    "            np.asarray(y_r, dtype=np.float32), np.asarray(starts, dtype=np.int64),\n",
    "            np.asarray(prev_close, dtype=np.float32), dropped_target_imputed, dropped_target_open_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepDataset(Dataset):\n",
    "    def __init__(self, X, y_s, y_r):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_s = torch.from_numpy(y_s).float()\n",
    "        self.y_r = torch.from_numpy(y_r).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_s[idx], self.y_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = build_walkforward_slices(price_df)\n",
    "print('Walk-forward slices:', slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition with Regime-Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttnGRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout, horizon):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.decoder_cell = nn.GRUCell(output_size + hidden_size, hidden_size)\n",
    "        self.attn_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "        # Output mu and log_sigma for each OHLC\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, output_size),\n",
    "        )\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].weight)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].bias)\n",
    "        \n",
    "    def _attend(self, h_dec, enc_out):\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)\n",
    "        return context\n",
    "    \n",
    "    def forward(self, x, y_teacher=None, teacher_forcing_ratio=0.0, return_sigma=False):\n",
    "        enc_out, h = self.encoder(x)\n",
    "        h_dec = h[-1]\n",
    "        dec_input = x[:, -1, :self.output_size]\n",
    "        \n",
    "        mu_seq, sigma_seq = [], []\n",
    "        for t in range(self.horizon):\n",
    "            context = self._attend(h_dec, enc_out)\n",
    "            cell_input = torch.cat([dec_input, context], dim=1)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "            out_features = torch.cat([h_dec, context], dim=1)\n",
    "            \n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = self.log_sigma_head(out_features)\n",
    "            \n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "            \n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    noise = torch.randn_like(mu) * torch.exp(log_sigma).detach()\n",
    "                    dec_input = mu + noise\n",
    "            else:\n",
    "                dec_input = mu\n",
    "        \n",
    "        mu_out = torch.cat(mu_seq, dim=1)\n",
    "        sigma_out = torch.cat(sigma_seq, dim=1)\n",
    "        \n",
    "        if return_sigma:\n",
    "            return mu_out, sigma_out\n",
    "        return mu_out\n",
    "    \n",
    "    def generate_realistic(self, x, context_ohlc=None, context_vwap=None, temp_trending=0.8, temp_ranging=1.5):\n",
    "        \"\"\"\n",
    "        Generate realistic price paths with regime-conditional sampling.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, window, features]\n",
    "            context_ohlc: Historical OHLC for regime detection [window, 4]\n",
    "            context_vwap: Historical VWAP for mean-reversion calculation [window]\n",
    "            temp_trending: Temperature for trending regime (lower = more confident)\n",
    "            temp_ranging: Temperature for ranging regime (higher = more variable)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_out, h = self.encoder(x)\n",
    "            h_dec = h[-1]\n",
    "            dec_input = x[:, -1, :self.output_size]\n",
    "            \n",
    "            # Detect regime from context if provided\n",
    "            temperature = temp_ranging  # Default to ranging\n",
    "            momentum_drift = 0.0\n",
    "            mean_reversion_target = None\n",
    "            regime_name = 'ranging'\n",
    "            adx_value = 25.0\n",
    "            \n",
    "            if context_ohlc is not None:\n",
    "                adx_value, regime_name = detect_regime(context_ohlc, ADX_PERIOD)\n",
    "                if regime_name == 'trending':\n",
    "                    temperature = temp_trending\n",
    "                    # Calculate momentum drift from last 10 bars\n",
    "                    close_prices = context_ohlc[:, 3]\n",
    "                    momentum_drift = calculate_momentum_slope(close_prices, MOMENTUM_WINDOW)\n",
    "                else:\n",
    "                    temperature = temp_ranging\n",
    "                    # Calculate mean-reversion target (VWAP)\n",
    "                    if context_vwap is not None and len(context_vwap) > 0:\n",
    "                        mean_reversion_target = float(np.mean(context_vwap[-10:]))\n",
    "            \n",
    "            generated = []\n",
    "            prev_close_price = None\n",
    "            \n",
    "            if context_ohlc is not None and len(context_ohlc) > 0:\n",
    "                prev_close_price = float(context_ohlc[-1, 3])\n",
    "            \n",
    "            for t in range(self.horizon):\n",
    "                context = self._attend(h_dec, enc_out)\n",
    "                cell_input = torch.cat([dec_input, context], dim=1)\n",
    "                h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "                out_features = torch.cat([h_dec, context], dim=1)\n",
    "                \n",
    "                mu = self.mu_head(out_features)\n",
    "                log_sigma = self.log_sigma_head(out_features)\n",
    "                \n",
    "                # Apply regime-conditional modifications\n",
    "                if regime_name == 'trending' and momentum_drift != 0.0:\n",
    "                    # Add momentum drift to mu (applied to close return)\n",
    "                    drift_tensor = torch.zeros_like(mu)\n",
    "                    drift_tensor[:, 3] = momentum_drift * MOMENTUM_DRIFT_SCALE * (t + 1)\n",
    "                    mu = mu + drift_tensor\n",
    "                elif regime_name == 'ranging' and mean_reversion_target is not None and prev_close_price is not None:\n",
    "                    # Add mean-reversion bias toward VWAP\n",
    "                    vwap_return = np.log(mean_reversion_target / prev_close_price) if prev_close_price > 0 else 0.0\n",
    "                    current_close_return = mu[:, 3].detach().cpu().numpy()\n",
    "                    reversion = MEAN_REVERSION_STRENGTH * (vwap_return - current_close_return)\n",
    "                    mu[:, 3] = mu[:, 3] + torch.tensor(reversion, dtype=mu.dtype, device=mu.device)\n",
    "                \n",
    "                # Scale sigma by regime-conditional temperature\n",
    "                sigma = torch.exp(log_sigma) * temperature\n",
    "                sigma = torch.maximum(sigma, torch.tensor(MIN_PREDICTED_VOL))\n",
    "                \n",
    "                # Sample from distribution\n",
    "                noise = torch.randn_like(mu) * sigma\n",
    "                sample = mu + noise\n",
    "                \n",
    "                generated.append(sample.unsqueeze(1))\n",
    "                dec_input = sample\n",
    "                \n",
    "                # Update prev_close_price for next iteration's mean-reversion calc\n",
    "                if regime_name == 'ranging' and prev_close_price is not None:\n",
    "                    # Convert sampled return back to price for next iteration\n",
    "                    sampled_close_return = sample[:, 3].detach().cpu().numpy()[0]\n",
    "                    prev_close_price = prev_close_price * np.exp(float(sampled_close_return))\n",
    "            \n",
    "            return torch.cat(generated, dim=1), regime_name, adx_value, temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(mu, log_sigma, target):\n",
    "    \"\"\"Negative log-likelihood for Gaussian\"\"\"\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    nll = 0.5 * ((target - mu) / sigma) ** 2 + log_sigma + 0.5 * np.log(2 * np.pi)\n",
    "    return nll.mean()\n",
    "\n",
    "def candle_range_loss(mu, target):\n",
    "    pred_range = mu[:, :, 1] - mu[:, :, 2]\n",
    "    actual_range = target[:, :, 1] - target[:, :, 2]\n",
    "    return ((pred_range - actual_range) ** 2).mean()\n",
    "\n",
    "def volatility_match_loss(log_sigma, target):\n",
    "    pred_vol = torch.exp(log_sigma).mean()\n",
    "    actual_vol = target.std()\n",
    "    return (pred_vol - actual_vol) ** 2\n",
    "\n",
    "def directional_penalty(mu, target):\n",
    "    pred_close = mu[:, :, 3]\n",
    "    actual_close = target[:, :, 3]\n",
    "    sign_match = torch.sign(pred_close) * torch.sign(actual_close)\n",
    "    penalty = torch.clamp(-sign_match, min=0.0)\n",
    "    return penalty.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ratio_for_epoch(epoch):\n",
    "    ratio = TF_START * (TF_DECAY_RATE ** (epoch - 1))\n",
    "    return max(float(TF_END), float(ratio))\n",
    "\n",
    "def run_epoch(model, loader, step_weights_t, optimizer=None, tf_ratio=0.0):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    total_loss, nll_total, range_total, vol_total, dir_total = 0, 0, 0, 0, 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for xb, yb_s, yb_r in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb_s = yb_s.to(DEVICE)\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            mu, log_sigma = model(xb, y_teacher=yb_s if is_train else None, \n",
    "                                  teacher_forcing_ratio=tf_ratio if is_train else 0.0, \n",
    "                                  return_sigma=True)\n",
    "            \n",
    "            nll = (nll_loss(mu, log_sigma, yb_s) * step_weights_t).mean()\n",
    "            rng = candle_range_loss(mu, yb_s)\n",
    "            vol = volatility_match_loss(log_sigma, yb_s)\n",
    "            dir_pen = directional_penalty(mu, yb_s)\n",
    "            \n",
    "            loss = nll + RANGE_LOSS_WEIGHT * rng + VOLATILITY_WEIGHT * vol + DIR_PENALTY_WEIGHT * dir_pen\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        nll_total += nll.item() * bs\n",
    "        range_total += rng.item() * bs\n",
    "        vol_total += vol.item() * bs\n",
    "        dir_total += dir_pen.item() * bs\n",
    "        n_items += bs\n",
    "        \n",
    "    return {\n",
    "        'total': total_loss / max(n_items, 1),\n",
    "        'nll': nll_total / max(n_items, 1),\n",
    "        'range': range_total / max(n_items, 1),\n",
    "        'vol': vol_total / max(n_items, 1),\n",
    "        'dir': dir_total / max(n_items, 1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, max_epochs, patience):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    wait = 0\n",
    "    rows = []\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tf = tf_ratio_for_epoch(epoch)\n",
    "        tr = run_epoch(model, train_loader, step_weights_t, optimizer=optimizer, tf_ratio=tf)\n",
    "        va = run_epoch(model, val_loader, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "        \n",
    "        scheduler.step(va['total'])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        rows.append({\n",
    "            'epoch': epoch, 'tf_ratio': tf, 'lr': lr,\n",
    "            'train_total': tr['total'], 'val_total': va['total'],\n",
    "            'train_nll': tr['nll'], 'val_nll': va['nll'],\n",
    "            'train_range': tr['range'], 'val_range': va['range'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
    "              f\"train={tr['total']:.6f} (nll={tr['nll']:.6f}) | \"\n",
    "              f\"val={va['total']:.6f} (nll={va['nll']:.6f}) | lr={lr:.6g}\")\n",
    "        \n",
    "        if va['total'] < best_val:\n",
    "            best_val = va['total']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}.')\n",
    "                break\n",
    "                \n",
    "    model.load_state_dict(best_state)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(actual_ohlc, pred_ohlc, prev_close):\n",
    "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
    "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
    "    ac, pc = actual_ohlc[:, 3], pred_ohlc[:, 3]\n",
    "    \n",
    "    return {\n",
    "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
    "        'close_rmse': float(np.sqrt(np.mean((ac - pc) ** 2))),\n",
    "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
    "        'directional_accuracy_eps': float(np.mean(np.sign(ac - prev_close) == np.sign(pc - prev_close))),\n",
    "    }\n",
    "\n",
    "def evaluate_baselines(actual_ohlc, prev_ohlc, prev_close):\n",
    "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
    "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
    "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
    "    return {'persistence': persistence, 'flat_close_rw': flat_rw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_realistic_recursive(model, X, context_prices, context_vwap=None):\n",
    "    \"\"\"\n",
    "    Generate realistic predictions using regime-conditional autoregressive sampling.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        X: Input features [1, window, features]\n",
    "        context_prices: Historical OHLC prices for regime detection [window, 4]\n",
    "        context_vwap: Historical VWAP for mean-reversion [window]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X).float().to(DEVICE)\n",
    "    \n",
    "    # Generate with regime-conditional sampling\n",
    "    generated, regime, adx, temp = model.generate_realistic(\n",
    "        X_tensor, \n",
    "        context_ohlc=context_prices,\n",
    "        context_vwap=context_vwap,\n",
    "        temp_trending=TEMP_TRENDING,\n",
    "        temp_ranging=TEMP_RANGING\n",
    "    )\n",
    "    \n",
    "    print(f\"Regime Detection: ADX={adx:.2f}, Regime={regime}, Temperature={temp:.2f}\")\n",
    "    \n",
    "    return generated.detach().cpu().numpy()[0], regime, adx, temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold_name, price_fold, window, max_epochs, patience, run_sanity=False, quick_mode=False):\n",
    "    feat_fold = build_feature_frame(price_fold)\n",
    "    target_fold = build_target_frame(feat_fold)\n",
    "    \n",
    "    input_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "    target_raw = target_fold[TARGET_COLS].to_numpy(np.float32)\n",
    "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "    row_open_skip = feat_fold['row_open_skip'].to_numpy(np.int8).astype(bool)\n",
    "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
    "    price_vals = price_fold.loc[feat_fold.index, OHLC_COLS].to_numpy(np.float32)\n",
    "    vwap_vals = price_fold.loc[feat_fold.index, 'VWAP'].to_numpy(np.float32)\n",
    "    \n",
    "    tr_end, va_end = split_points(len(input_raw))\n",
    "    \n",
    "    # Standardize inputs only\n",
    "    in_mean, in_std = input_raw[:tr_end].mean(axis=0), input_raw[:tr_end].std(axis=0)\n",
    "    in_std = np.where(in_std < 1e-8, 1.0, in_std)\n",
    "    input_scaled = (input_raw - in_mean) / in_std\n",
    "    \n",
    "    tg_mean, tg_std = np.zeros(4, dtype=np.float32), np.ones(4, dtype=np.float32)\n",
    "    target_scaled = target_raw.copy()\n",
    "    \n",
    "    X_all, y_all_s, y_all_r, starts, prev_close_starts, dropped_imputed, dropped_skip = make_multistep_windows(\n",
    "        input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, prev_close, window, HORIZON\n",
    "    )\n",
    "    \n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: no windows available.')\n",
    "    \n",
    "    # Splits\n",
    "    end_idx = starts + HORIZON - 1\n",
    "    tr_m, va_m, te_m = end_idx < tr_end, (end_idx >= tr_end) & (end_idx < va_end), end_idx >= va_end\n",
    "    \n",
    "    X_train, y_train_s, y_train_r = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m]\n",
    "    X_val, y_val_s, y_val_r = X_all[va_m], y_all_s[va_m], y_all_r[va_m]\n",
    "    X_test, y_test_s, y_test_r = X_all[te_m], y_all_s[te_m], y_all_r[te_m]\n",
    "    test_starts = starts[te_m]\n",
    "    test_prev_close = prev_close_starts[te_m]\n",
    "    \n",
    "    print(f'Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}')\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), \n",
    "                             batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), \n",
    "                           batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = Seq2SeqAttnGRU(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=len(TARGET_COLS),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    hist = train_model(model, train_loader, val_loader, max_epochs, patience)\n",
    "    \n",
    "    # REGIME-CONDITIONAL PREDICTION\n",
    "    last_idx = len(X_test) - 1\n",
    "    X_last = X_test[last_idx:last_idx+1]\n",
    "    context_start = int(test_starts[last_idx]) - window\n",
    "    context_ohlc = price_vals[context_start:int(test_starts[last_idx])]\n",
    "    context_vwap = vwap_vals[context_start:int(test_starts[last_idx])]\n",
    "    \n",
    "    pred_rets_realistic, regime, adx, temp = predict_realistic_recursive(\n",
    "        model, X_last, context_ohlc, context_vwap\n",
    "    )\n",
    "    \n",
    "    # Convert to prices\n",
    "    last_close = float(test_prev_close[last_idx])\n",
    "    pred_price_realistic = returns_to_prices_seq(pred_rets_realistic, last_close)\n",
    "    \n",
    "    # Actual future\n",
    "    actual_future = price_vals[int(test_starts[last_idx]):int(test_starts[last_idx])+HORIZON]\n",
    "    \n",
    "    # One-step metrics (deterministic for comparison)\n",
    "    mu_test = model(torch.from_numpy(X_test).float().to(DEVICE)).detach().cpu().numpy()\n",
    "    pred_step1_ret = mu_test[:, 0, :]\n",
    "    actual_step1_ret = y_test_r[:, 0, :]\n",
    "    \n",
    "    pred_ohlc_1 = np.zeros((len(test_starts), 4))\n",
    "    for i in range(len(test_starts)):\n",
    "        pc = test_prev_close[i]\n",
    "        pred_ohlc_1[i] = [\n",
    "            pc * np.exp(pred_step1_ret[i, 0]),\n",
    "            pc * np.exp(pred_step1_ret[i, 1]),\n",
    "            pc * np.exp(pred_step1_ret[i, 2]),\n",
    "            pc * np.exp(pred_step1_ret[i, 3]),\n",
    "        ]\n",
    "        pred_ohlc_1[i] = enforce_candle_validity(pred_ohlc_1[i].reshape(1, -1))[0]\n",
    "    \n",
    "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
    "    prev_ohlc = price_vals[test_starts]\n",
    "    \n",
    "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
    "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
    "    \n",
    "    print(f\"\\nRegime-conditional prediction stats:\")\n",
    "    print(f\"  Regime: {regime} (ADX={adx:.2f})\")\n",
    "    print(f\"  Temperature used: {temp:.2f}\")\n",
    "    print(f\"  Pred range: [{pred_price_realistic[:, 3].min():.2f}, {pred_price_realistic[:, 3].max():.2f}]\")\n",
    "    print(f\"  Actual range: [{actual_future[:, 3].min():.2f}, {actual_future[:, 3].max():.2f}]\")\n",
    "    print(f\"  Pred volatility: {np.std(pred_rets_realistic[:, 3]):.6f}\")\n",
    "    print(f\"  Actual volatility: {np.std(actual_step1_ret[:, 3]):.6f}\")\n",
    "    \n",
    "    # Build DataFrames for plotting\n",
    "    future_idx = price_fold.index[test_starts[last_idx]:test_starts[last_idx]+HORIZON]\n",
    "    pred_future_df = pd.DataFrame(pred_price_realistic, index=future_idx, columns=OHLC_COLS)\n",
    "    actual_future_df = pd.DataFrame(actual_future, index=future_idx, columns=OHLC_COLS)\n",
    "    context_df = price_fold.iloc[test_starts[last_idx]-window:test_starts[last_idx]+1][OHLC_COLS]\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_name,\n",
    "        'window': window,\n",
    "        'history_df': hist,\n",
    "        'model_metrics': model_metrics,\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'context_df': context_df,\n",
    "        'actual_future_df': actual_future_df,\n",
    "        'pred_future_df': pred_future_df,\n",
    "        'samples': {'train': len(X_train), 'val': len(X_val), 'test': len(X_test)},\n",
    "        'regime_info': {'regime': regime, 'adx': adx, 'temperature': temp},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lookback sweep if enabled\n",
    "fold_results = []\n",
    "primary_slice = slices[0]\n",
    "selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "if ENABLE_LOOKBACK_SWEEP:\n",
    "    print('\\n=== Lookback sweep ===')\n",
    "    _, a0, b0 = primary_slice\n",
    "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for w in LOOKBACK_CANDIDATES:\n",
    "        print(f'\\nSweep candidate lookback={w} --')\n",
    "        try:\n",
    "            r = run_fold(f'sweep_w{w}', fold_price0, w, SWEEP_MAX_EPOCHS, SWEEP_PATIENCE, quick_mode=True)\n",
    "            score = -r['model_metrics']['close_mae']\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                selected_window = w\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for window {w}: {e}\")\n",
    "\n",
    "print(f'\\nSelected lookback: {selected_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full walk-forward with regime-conditional generation\n",
    "print('\\n=== Full walk-forward with regime-conditional generation ===')\n",
    "for i, (name, a, b) in enumerate(slices, start=1):\n",
    "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
    "    fold_price = price_df.iloc[a:b].copy()\n",
    "    try:\n",
    "        res = run_fold(name, fold_price, selected_window, FINAL_MAX_EPOCHS, FINAL_PATIENCE)\n",
    "        fold_results.append(res)\n",
    "        \n",
    "        print(f\"\\nResults for {name}:\")\n",
    "        print(f\"  Model MAE: {res['model_metrics']['close_mae']:.4f}\")\n",
    "        print(f\"  Persistence MAE: {res['baseline_metrics']['persistence']['close_mae']:.4f}\")\n",
    "        print(f\"  Regime: {res['regime_info']['regime']} (ADX={res['regime_info']['adx']:.2f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    regime = latest['regime_info']['regime']\n",
    "    adx = latest['regime_info']['adx']\n",
    "    temp = latest['regime_info']['temperature']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8), facecolor='black')\n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    def draw_candles(ax, ohlc, start_x, up_edge, up_face, down_edge, down_face, wick_color, width=0.6, alpha=1.0):\n",
    "        vals = ohlc[OHLC_COLS].to_numpy()\n",
    "        for i, (o, h, l, c) in enumerate(vals):\n",
    "            x = start_x + i\n",
    "            bull = c >= o\n",
    "            ax.vlines(x, l, h, color=wick_color, linewidth=1.0, alpha=alpha, zorder=2)\n",
    "            lower = min(o, c)\n",
    "            height = max(abs(c - o), 1e-6)\n",
    "            rect = Rectangle((x - width/2, lower), width, height,\n",
    "                           facecolor=up_face if bull else down_face,\n",
    "                           edgecolor=up_edge if bull else down_edge,\n",
    "                           linewidth=1.0, alpha=alpha, zorder=3)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    context_df = latest['context_df']\n",
    "    actual_future_df = latest['actual_future_df']\n",
    "    pred_future_df = latest['pred_future_df']\n",
    "    \n",
    "    # Draw history (green/red)\n",
    "    draw_candles(ax, context_df, 0, '#00FF00', '#00FF00', '#FF0000', '#FF0000', '#FFFFFF', width=0.6, alpha=0.9)\n",
    "    \n",
    "    # Draw actual future (dimmed)\n",
    "    draw_candles(ax, actual_future_df, len(context_df), '#00AA00', '#00AA00', '#AA0000', '#AA0000', '#888888', \n",
    "                 width=0.6, alpha=0.6)\n",
    "    \n",
    "    # Draw regime-conditional prediction (bright white/black with regime color indicator)\n",
    "    pred_color = '#00FFFF' if regime == 'trending' else '#FFAA00'  # Cyan for trending, orange for ranging\n",
    "    draw_candles(ax, pred_future_df, len(context_df), pred_color, pred_color, '#888888', '#000000', '#FFFFFF',\n",
    "                 width=0.5, alpha=1.0)\n",
    "    \n",
    "    ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=1.0, alpha=0.8)\n",
    "    \n",
    "    # Labels\n",
    "    n = len(context_df) + len(actual_future_df)\n",
    "    step = max(1, n // 12)\n",
    "    ticks = list(range(0, n, step))\n",
    "    all_idx = context_df.index.append(actual_future_df.index)\n",
    "    labels = [all_idx[i].strftime('%m-%d %H:%M') for i in ticks if i < len(all_idx)]\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=30, ha='right', color='white', fontsize=9)\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_color('#666666')\n",
    "    ax.grid(color='#333333', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(f\"MSFT 1m ({latest['fold']}) - Regime: {regime.upper()} (ADX={adx:.1f}, Temp={temp:.1f})\", \n",
    "                 color='white', fontsize=14, pad=15)\n",
    "    ax.set_ylabel('Price', color='white', fontsize=12)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#00FF00', edgecolor='#00FF00', label='History (bull)'),\n",
    "        Patch(facecolor='#FF0000', edgecolor='#FF0000', label='History (bear)'),\n",
    "        Patch(facecolor='#00AA00', edgecolor='#00AA00', label='Actual Future (dim)'),\n",
    "        Patch(facecolor=pred_color, edgecolor=pred_color, label=f'Predicted ({regime})'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, facecolor='black', edgecolor='white', labelcolor='white', \n",
    "             loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal chart shows regime-conditional generation:\")\n",
    "    print(f\"  - Regime: {regime} (ADX={adx:.2f})\")\n",
    "    print(f\"  - Temperature: {temp:.2f} ({'Lower=Confident' if regime == 'trending' else 'Higher=Variable'})\")\n",
    "    if regime == 'trending':\n",
    "        print(f\"  - Applied momentum drift based on {MOMENTUM_WINDOW}-bar slope\")\n",
    "    else:\n",
    "        print(f\"  - Applied mean-reversion bias toward VWAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell: Comprehensive Evaluation\n",
    "print(\"=\" * 70)\n",
    "print(\"REGIME-CONDITIONAL SAMPLING (v7.3) - TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    \n",
    "    # Extract data for evaluation\n",
    "    context_df = latest['context_df']\n",
    "    actual_future_df = latest['actual_future_df']\n",
    "    pred_future_df = latest['pred_future_df']\n",
    "    \n",
    "    actual_closes = actual_future_df['Close'].values\n",
    "    pred_closes = pred_future_df['Close'].values\n",
    "    context_closes = context_df['Close'].values\n",
    "    \n",
    "    # 1. Directional Accuracy vs Persistence Baseline\n",
    "    print(\"\\n1. DIRECTIONAL ACCURACY ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Model directional predictions\n",
    "    model_directions = np.sign(np.diff(pred_closes))\n",
    "    actual_directions = np.sign(np.diff(actual_closes))\n",
    "    model_dir_acc = np.mean(model_directions == actual_directions) * 100\n",
    "    \n",
    "    # Persistence baseline (predict same direction as last context bar)\n",
    "    last_context_close = context_closes[-1]\n",
    "    persistence_directions = np.sign(actual_closes[:-1] - last_context_close)\n",
    "    persistence_dir_acc = np.mean(persistence_directions == actual_directions) * 100\n",
    "    \n",
    "    print(f\"   Model Directional Accuracy:      {model_dir_acc:.1f}%\")\n",
    "    print(f\"   Persistence Baseline:            {persistence_dir_acc:.1f}%\")\n",
    "    print(f\"   Improvement over baseline:       {model_dir_acc - persistence_dir_acc:+.1f}%\")\n",
    "    \n",
    "    # 2. Average Predicted Close vs Actual Close (Bias)\n",
    "    print(\"\\n2. PREDICTION BIAS ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    avg_pred_close = np.mean(pred_closes)\n",
    "    avg_actual_close = np.mean(actual_closes)\n",
    "    bias = avg_pred_close - avg_actual_close\n",
    "    bias_pct = (bias / avg_actual_close) * 100\n",
    "    \n",
    "    print(f\"   Average Predicted Close:         ${avg_pred_close:.2f}\")\n",
    "    print(f\"   Average Actual Close:            ${avg_actual_close:.2f}\")\n",
    "    print(f\"   Bias:                            ${bias:+.4f} ({bias_pct:+.3f}%)\")\n",
    "    \n",
    "    # 3. Wick/Body Realism Verification\n",
    "    print(\"\\n3. CANDLE REALISM VERIFICATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def analyze_candles(ohlc_df, name):\n",
    "        highs = ohlc_df['High'].values\n",
    "        lows = ohlc_df['Low'].values\n",
    "        opens = ohlc_df['Open'].values\n",
    "        closes = ohlc_df['Close'].values\n",
    "        \n",
    "        # Body size (absolute log return)\n",
    "        bodies = np.abs(np.log(closes / opens))\n",
    "        \n",
    "        # Wick sizes\n",
    "        upper_wicks = np.log(highs / np.maximum(opens, closes))\n",
    "        lower_wicks = np.log(np.minimum(opens, closes) / lows)\n",
    "        \n",
    "        # Validity check\n",
    "        valid = np.all((highs >= np.maximum(opens, closes)) & (lows <= np.minimum(opens, closes)))\n",
    "        \n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"      Valid candles (High>=max(O,C), Low<=min(O,C)): {valid}\")\n",
    "        print(f\"      Avg body size:        {np.mean(bodies)*10000:.2f} bps\")\n",
    "        print(f\"      Avg upper wick:       {np.mean(upper_wicks)*10000:.2f} bps\")\n",
    "        print(f\"      Avg lower wick:       {np.mean(lower_wicks)*10000:.2f} bps\")\n",
    "        print(f\"      Body/wick ratio:      {np.mean(bodies)/(np.mean(upper_wicks)+np.mean(lower_wicks)+1e-10):.2f}\")\n",
    "        return valid\n",
    "    \n",
    "    pred_valid = analyze_candles(pred_future_df, \"Predicted\")\n",
    "    actual_valid = analyze_candles(actual_future_df, \"Actual\")\n",
    "    \n",
    "    # 4. Regime-specific analysis\n",
    "    print(\"\\n4. REGIME-CONDITIONAL SETTINGS\")\n",
    "    print(\"-\" * 40)\n",
    "    regime = latest['regime_info']['regime']\n",
    "    adx = latest['regime_info']['adx']\n",
    "    temp = latest['regime_info']['temperature']\n",
    "    \n",
    "    print(f\"   Detected Regime:        {regime.upper()}\")\n",
    "    print(f\"   ADX Value:              {adx:.2f}\")\n",
    "    print(f\"   Temperature Used:       {temp:.2f}\")\n",
    "    if regime == 'trending':\n",
    "        print(f\"   Momentum Window:        {MOMENTUM_WINDOW} bars\")\n",
    "        print(f\"   Drift Scale:            {MOMENTUM_DRIFT_SCALE}\")\n",
    "    else:\n",
    "        print(f\"   Mean Reversion Strength: {MEAN_REVERSION_STRENGTH}\")\n",
    "    \n",
    "    # 5. Summary\n",
    "    print(\"\\n5. SUMMARY\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   âœ“ Directional Accuracy: {model_dir_acc:.1f}% (baseline: {persistence_dir_acc:.1f}%)\")\n",
    "    print(f\"   âœ“ Prediction Bias:      {bias_pct:+.3f}%\")\n",
    "    print(f\"   âœ“ Candle Validity:      {'PASS' if pred_valid else 'FAIL'}\")\n",
    "    print(f\"   âœ“ Regime Detection:     {regime.upper()} (ADX={adx:.1f})\")\n",
    "    \n",
    "    # Visual confirmation note\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VISUAL CONFIRMATION:\")\n",
    "    print(f\"   - Predicted candles show {'momentum continuation' if regime == 'trending' else 'mean-reversion behavior'}\")\n",
    "    print(f\"   - {'Cyan' if regime == 'trending' else 'Orange'} color indicates {regime} regime\")\n",
    "    print(f\"   - Wicks are {'compressed' if regime == 'trending' else 'balanced'} for {regime} market\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"No fold results available. Please run the training cells first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
