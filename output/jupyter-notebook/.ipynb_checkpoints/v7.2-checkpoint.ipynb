{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: MSFT 1-Minute GRU Forecast - Dual-Head Architecture (v7.2)\n",
    "\n",
    "Key changes for Dual-Head Architecture:\n",
    "1. **Head A (Direction)**: Binary classifier for trend direction (close[t+horizon] > close[t]) using BCE loss\n",
    "2. **Head B (Candle Generator)**: Predicts OHLC returns conditioned on Head A's output\n",
    "3. **Combined Loss**: BCE(direction) + NLL(candles) + 0.5*MSE(volatility)\n",
    "4. **Inference Bias**: Direction head's confidence biases the mu of candle generator\n",
    "5. Probabilistic outputs with autoregressive sampling and candle validity enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    'alpaca': 'alpaca-py',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'pandas_market_calendars': 'pandas-market-calendars',\n",
    "}\n",
    "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "else:\n",
    "    print('All required third-party packages are already installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "SYMBOL = 'MSFT'\n",
    "LOOKBACK_DAYS = 120\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen', 'rHigh', 'rLow', 'rClose',\n",
    "    'logVolChange', 'logTradeCountChange',\n",
    "    'vwapDelta', 'rangeFrac', 'orderFlowProxy', 'tickPressure',\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
    "\n",
    "HORIZON = 15\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "LOOKBACK_CANDIDATES = [64, 96, 160, 256]\n",
    "DEFAULT_LOOKBACK = 96\n",
    "ENABLE_LOOKBACK_SWEEP = True\n",
    "SKIP_OPEN_BARS_TARGET = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.20\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "SWEEP_MAX_EPOCHS = 15\n",
    "SWEEP_PATIENCE = 5\n",
    "FINAL_MAX_EPOCHS = 60\n",
    "FINAL_PATIENCE = 12\n",
    "TF_START = 1.0\n",
    "TF_END = 0.0\n",
    "TF_DECAY_RATE = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dual-Head Loss Configuration\n",
    "BCE_WEIGHT = 1.0          # Weight for direction classification loss\n",
    "NLL_WEIGHT = 1.0          # Weight for candle NLL loss\n",
    "VOL_MSE_WEIGHT = 0.5      # Weight for volatility MSE loss\n",
    "RANGE_LOSS_WEIGHT = 0.3   # Weight for range penalty\n",
    "DIR_PENALTY_WEIGHT = 0.1  # Weight for directional alignment penalty\n",
    "STEP_LOSS_POWER = 1.5\n",
    "\n",
    "# Inference bias configuration\n",
    "DIRECTION_BIAS_SCALE = 0.001  # Scale factor for directional drift bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Configuration\n",
    "SAMPLING_TEMPERATURE = 1.5\n",
    "VOLATILITY_SCALING = True\n",
    "MIN_PREDICTED_VOL = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Configuration\n",
    "STANDARDIZE_TARGETS = False\n",
    "APPLY_CLIPPING = True\n",
    "CLIP_QUANTILES = (0.001, 0.999)\n",
    "DIRECTION_EPS = 0.0001\n",
    "STD_RATIO_TARGET_MIN = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca API Configuration\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Configuration Summary\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'lookback_days': LOOKBACK_DAYS,\n",
    "    'horizon': HORIZON,\n",
    "    'sampling_temperature': SAMPLING_TEMPERATURE,\n",
    "    'dual_head_weights': {\n",
    "        'bce': BCE_WEIGHT,\n",
    "        'nll': NLL_WEIGHT,\n",
    "        'vol_mse': VOL_MSE_WEIGHT,\n",
    "        'range': RANGE_LOSS_WEIGHT,\n",
    "        'dir_penalty': DIR_PENALTY_WEIGHT,\n",
    "    },\n",
    "    'direction_bias_scale': DIRECTION_BIAS_SCALE,\n",
    "    'device': str(DEVICE),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be >0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "        \n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "    \n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "    \n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(columns={\n",
    "                'timestamp': 'Datetime', 'open': 'Open', 'high': 'High',\n",
    "                'low': 'Low', 'close': 'Close', 'volume': 'Volume',\n",
    "                'trade_count': 'TradeCount', 'vwap': 'VWAP',\n",
    "            })\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "            \n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "        cursor = chunk_end\n",
    "    \n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "    \n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    \n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "    \n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "    \n",
    "    pieces = []\n",
    "    fill_ratios = []\n",
    "    \n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        \n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "            \n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)]\n",
    "        day = day.reindex(exp_idx)\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "        \n",
    "        if fill_ratio >= 1.0 or fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            continue\n",
    "            \n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        if day['VWAP'].isna().all():\n",
    "            day['VWAP'] = day['Close']\n",
    "        else:\n",
    "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "            \n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "        \n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after per-session fill.')\n",
    "        pieces.append(day)\n",
    "        fill_ratios.append(fill_ratio)\n",
    "    \n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after calendar filtering.')\n",
    "        \n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "    out = out.copy()\n",
    "    \n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "    \n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(len(pieces)),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "    }\n",
    "    return out, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
    "print(f'Sessionized rows kept: {len(price_df):,}')\n",
    "print('Session meta:', session_meta)\n",
    "\n",
    "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
    "if len(price_df) < min_needed:\n",
    "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure High >= max(Open, Close) and Low <= min(Open, Close)\"\"\"\n",
    "    out = np.asarray(ohlc, dtype=np.float32)\n",
    "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    eps = 1e-9\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "    \n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0) / (np.abs(out['rClose']) + eps)\n",
    "    \n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "    \n",
    "    # Direction target for Head A\n",
    "    out['direction_target'] = (df['Close'] > prev_close).astype(np.float32)\n",
    "    \n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def build_target_frame(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return feat_df[TARGET_COLS].copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = build_feature_frame(price_df)\n",
    "target_df = build_target_frame(feat_df)\n",
    "print('Feature rows:', len(feat_df))\n",
    "print('Target columns:', list(target_df.columns))\n",
    "print('Direction distribution:', feat_df['direction_target'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing & Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_points(n_rows: int) -> tuple[int, int]:\n",
    "    tr = int(n_rows * TRAIN_RATIO)\n",
    "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
    "    return tr, va\n",
    "\n",
    "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
    "    n = len(price_df_full)\n",
    "    span = int(round(n * 0.85))\n",
    "    shift = max(1, n - span)\n",
    "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for name, a, b in cands:\n",
    "        key = (a, b)\n",
    "        if key in seen or b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1400:\n",
    "            continue\n",
    "        out.append((name, a, b))\n",
    "        seen.add(key)\n",
    "    return out if out else [('full', 0, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multistep_windows(input_scaled, target_scaled, target_raw, direction_targets, \n",
    "                           row_imputed, row_open_skip, starts_prev_close, window, horizon):\n",
    "    X, y_s, y_r, y_dir, starts, prev_close = [], [], [], [], [], []\n",
    "    dropped_target_imputed, dropped_target_open_skip = 0, 0\n",
    "    n = len(input_scaled)\n",
    "    \n",
    "    for i in range(window, n - horizon + 1):\n",
    "        if row_imputed[i:i+horizon].any():\n",
    "            dropped_target_imputed += 1\n",
    "            continue\n",
    "        if row_open_skip[i:i+horizon].any():\n",
    "            dropped_target_open_skip += 1\n",
    "            continue\n",
    "            \n",
    "        xb = input_scaled[i-window:i]\n",
    "        imp_frac = float(row_imputed[i-window:i].mean())\n",
    "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
    "        xb_aug = np.concatenate([xb, imp_col], axis=1)\n",
    "        \n",
    "        X.append(xb_aug)\n",
    "        y_s.append(target_scaled[i:i+horizon])\n",
    "        y_r.append(target_raw[i:i+horizon])\n",
    "        # Direction target: will close[t+horizon] > close[t]?\n",
    "        y_dir.append(direction_targets[i+horizon-1])\n",
    "        starts.append(i)\n",
    "        prev_close.append(starts_prev_close[i])\n",
    "    \n",
    "    return (np.asarray(X, dtype=np.float32), np.asarray(y_s, dtype=np.float32),\n",
    "            np.asarray(y_r, dtype=np.float32), np.asarray(y_dir, dtype=np.float32),\n",
    "            np.asarray(starts, dtype=np.int64),\n",
    "            np.asarray(prev_close, dtype=np.float32), dropped_target_imputed, dropped_target_open_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepDataset(Dataset):\n",
    "    def __init__(self, X, y_s, y_r, y_dir):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_s = torch.from_numpy(y_s).float()\n",
    "        self.y_r = torch.from_numpy(y_r).float()\n",
    "        self.y_dir = torch.from_numpy(y_dir).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_s[idx], self.y_r[idx], self.y_dir[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = build_walkforward_slices(price_df)\n",
    "print('Walk-forward slices:', slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual-Head Model Definition\n",
    "\n",
    "Head A: Direction classifier (Binary cross-entropy)\n",
    "\n",
    "Head B: Candle generator (NLL + MSE volatility), conditioned on Head A output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualHeadGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-Head Architecture:\n",
    "    - Head A (Direction): Binary classifier for trend direction\n",
    "    - Head B (Candle Generator): Predicts OHLC returns conditioned on Head A\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout, horizon):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Shared encoder\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        \n",
    "        # Head A: Direction classifier (from final hidden state)\n",
    "        self.direction_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Decoder for candle generation\n",
    "        self.decoder_cell = nn.GRUCell(output_size + hidden_size + 1, hidden_size)  # +1 for direction prob\n",
    "        self.attn_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "        # Head B: Candle generator outputs (mu and log_sigma)\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, output_size),\n",
    "        )\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].weight)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].bias)\n",
    "        \n",
    "    def _attend(self, h_dec, enc_out):\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)\n",
    "        return context\n",
    "    \n",
    "    def forward(self, x, y_teacher=None, teacher_forcing_ratio=0.0, return_sigma=False):\n",
    "        enc_out, h = self.encoder(x)\n",
    "        h_dec = h[-1]\n",
    "        \n",
    "        # Head A: Direction prediction (single scalar for horizon)\n",
    "        direction_logits = self.direction_head(h_dec).squeeze(-1)  # [batch]\n",
    "        direction_prob = torch.sigmoid(direction_logits)  # [batch]\n",
    "        \n",
    "        # Decode candles conditioned on direction probability\n",
    "        dec_input = x[:, -1, :self.output_size]\n",
    "        mu_seq, sigma_seq = [], []\n",
    "        \n",
    "        for t in range(self.horizon):\n",
    "            context = self._attend(h_dec, enc_out)\n",
    "            # Concatenate direction probability to condition generation\n",
    "            dir_feat = direction_prob.unsqueeze(1)  # [batch, 1]\n",
    "            cell_input = torch.cat([dec_input, context, dir_feat], dim=1)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "            out_features = torch.cat([h_dec, context], dim=1)\n",
    "            \n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = self.log_sigma_head(out_features)\n",
    "            \n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "            \n",
    "            # Teacher forcing or autoregressive\n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    noise = torch.randn_like(mu) * torch.exp(log_sigma).detach()\n",
    "                    dec_input = mu + noise\n",
    "            else:\n",
    "                dec_input = mu\n",
    "        \n",
    "        mu_out = torch.cat(mu_seq, dim=1)\n",
    "        sigma_out = torch.cat(sigma_seq, dim=1)\n",
    "        \n",
    "        if return_sigma:\n",
    "            return mu_out, sigma_out, direction_logits\n",
    "        return mu_out, direction_logits\n",
    "    \n",
    "    def generate_realistic(self, x, temperature=1.0, historical_vol=None, apply_direction_bias=True):\n",
    "        \"\"\"\n",
    "        Generate realistic price paths with controlled stochasticity.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, window, features]\n",
    "            temperature: Controls volatility (1.0 = learned vol)\n",
    "            historical_vol: Optional historical volatility for scaling\n",
    "            apply_direction_bias: If True, bias mu based on direction confidence\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_out, h = self.encoder(x)\n",
    "            h_dec = h[-1]\n",
    "            \n",
    "            # Get direction prediction\n",
    "            direction_logits = self.direction_head(h_dec).squeeze(-1)\n",
    "            direction_prob = torch.sigmoid(direction_logits)\n",
    "            direction_confidence = torch.abs(direction_prob - 0.5) * 2  # Scale to [0, 1]\n",
    "            \n",
    "            dec_input = x[:, -1, :self.output_size]\n",
    "            generated = []\n",
    "            \n",
    "            for t in range(self.horizon):\n",
    "                context = self._attend(h_dec, enc_out)\n",
    "                dir_feat = direction_prob.unsqueeze(1)\n",
    "                cell_input = torch.cat([dec_input, context, dir_feat], dim=1)\n",
    "                h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "                out_features = torch.cat([h_dec, context], dim=1)\n",
    "                \n",
    "                mu = self.mu_head(out_features)\n",
    "                log_sigma = self.log_sigma_head(out_features)\n",
    "                \n",
    "                # Apply directional bias: if UP with high confidence, drift mu up\n",
    "                if apply_direction_bias:\n",
    "                    # Direction: +1 for UP (prob > 0.5), -1 for DOWN\n",
    "                    direction_sign = torch.sign(direction_prob - 0.5)  # [batch]\n",
    "                    bias = direction_sign.unsqueeze(1) * direction_confidence.unsqueeze(1) * DIRECTION_BIAS_SCALE\n",
    "                    mu = mu + bias\n",
    "                \n",
    "                # Scale sigma by temperature\n",
    "                sigma = torch.exp(log_sigma) * temperature\n",
    "                \n",
    "                if historical_vol is not None and t < 5:\n",
    "                    sigma = torch.ones_like(sigma) * historical_vol\n",
    "                \n",
    "                sigma = torch.maximum(sigma, torch.tensor(MIN_PREDICTED_VOL, device=sigma.device))\n",
    "                \n",
    "                noise = torch.randn_like(mu) * sigma\n",
    "                sample = mu + noise\n",
    "                \n",
    "                generated.append(sample.unsqueeze(1))\n",
    "                dec_input = sample\n",
    "            \n",
    "            return torch.cat(generated, dim=1), direction_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Dual-Head Architecture\n",
    "\n",
    "Loss = BCE(direction) + NLL(candles) + 0.5*MSE(volatility) + range_penalty + dir_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(mu, log_sigma, target):\n",
    "    \"\"\"Negative log-likelihood for Gaussian\"\"\"\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    nll = 0.5 * ((target - mu) / sigma) ** 2 + log_sigma + 0.5 * np.log(2 * np.pi)\n",
    "    return nll.mean()\n",
    "\n",
    "def volatility_mse_loss(log_sigma, target):\n",
    "    \"\"\"\n",
    "    MSE between predicted volatility and actual error magnitude.\n",
    "    This encourages predicted uncertainty to match actual errors.\n",
    "    \"\"\"\n",
    "    pred_vol = torch.exp(log_sigma).mean(dim=(1, 2))  # [batch]\n",
    "    actual_error = torch.abs(target - torch.zeros_like(target)).mean(dim=(1, 2))  # [batch]\n",
    "    return F.mse_loss(pred_vol, actual_error)\n",
    "\n",
    "def candle_range_loss(mu, target):\n",
    "    pred_range = mu[:, :, 1] - mu[:, :, 2]  # High - Low\n",
    "    actual_range = target[:, :, 1] - target[:, :, 2]\n",
    "    return ((pred_range - actual_range) ** 2).mean()\n",
    "\n",
    "def directional_penalty(mu, target):\n",
    "    \"\"\"Penalize when predicted close move direction differs from actual\"\"\"\n",
    "    pred_close = mu[:, :, 3]\n",
    "    actual_close = target[:, :, 3]\n",
    "    sign_match = torch.sign(pred_close) * torch.sign(actual_close)\n",
    "    penalty = torch.clamp(-sign_match, min=0.0)\n",
    "    return penalty.mean()\n",
    "\n",
    "def direction_bce_loss(direction_logits, direction_targets):\n",
    "    \"\"\"Binary cross-entropy for direction classification\"\"\"\n",
    "    return F.binary_cross_entropy_with_logits(direction_logits, direction_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ratio_for_epoch(epoch):\n",
    "    ratio = TF_START * (TF_DECAY_RATE ** (epoch - 1))\n",
    "    return max(float(TF_END), float(ratio))\n",
    "\n",
    "def run_epoch(model, loader, step_weights_t, optimizer=None, tf_ratio=0.0):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    total_loss = 0\n",
    "    bce_total, nll_total, vol_mse_total = 0, 0, 0\n",
    "    range_total, dir_total = 0, 0\n",
    "    dir_acc_total = 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for xb, yb_s, yb_r, yb_dir in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb_s = yb_s.to(DEVICE)\n",
    "        yb_dir = yb_dir.to(DEVICE)\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            mu, log_sigma, dir_logits = model(\n",
    "                xb, y_teacher=yb_s if is_train else None, \n",
    "                teacher_forcing_ratio=tf_ratio if is_train else 0.0, \n",
    "                return_sigma=True\n",
    "            )\n",
    "            \n",
    "            # Dual-head combined loss\n",
    "            bce = direction_bce_loss(dir_logits, yb_dir)\n",
    "            nll = (nll_loss(mu, log_sigma, yb_s) * step_weights_t).mean()\n",
    "            vol_mse = volatility_mse_loss(log_sigma, yb_s)\n",
    "            rng = candle_range_loss(mu, yb_s)\n",
    "            dir_pen = directional_penalty(mu, yb_s)\n",
    "            \n",
    "            loss = (BCE_WEIGHT * bce + \n",
    "                    NLL_WEIGHT * nll + \n",
    "                    VOL_MSE_WEIGHT * vol_mse + \n",
    "                    RANGE_LOSS_WEIGHT * rng + \n",
    "                    DIR_PENALTY_WEIGHT * dir_pen)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "        # Calculate direction accuracy\n",
    "        dir_preds = (torch.sigmoid(dir_logits) > 0.5).float()\n",
    "        dir_acc = (dir_preds == yb_dir).float().mean()\n",
    "                \n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        bce_total += bce.item() * bs\n",
    "        nll_total += nll.item() * bs\n",
    "        vol_mse_total += vol_mse.item() * bs\n",
    "        range_total += rng.item() * bs\n",
    "        dir_total += dir_pen.item() * bs\n",
    "        dir_acc_total += dir_acc.item() * bs\n",
    "        n_items += bs\n",
    "        \n",
    "    return {\n",
    "        'total': total_loss / max(n_items, 1),\n",
    "        'bce': bce_total / max(n_items, 1),\n",
    "        'nll': nll_total / max(n_items, 1),\n",
    "        'vol_mse': vol_mse_total / max(n_items, 1),\n",
    "        'range': range_total / max(n_items, 1),\n",
    "        'dir': dir_total / max(n_items, 1),\n",
    "        'dir_acc': dir_acc_total / max(n_items, 1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, max_epochs, patience):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    wait = 0\n",
    "    rows = []\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tf = tf_ratio_for_epoch(epoch)\n",
    "        tr = run_epoch(model, train_loader, step_weights_t, optimizer=optimizer, tf_ratio=tf)\n",
    "        va = run_epoch(model, val_loader, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "        \n",
    "        scheduler.step(va['total'])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        rows.append({\n",
    "            'epoch': epoch, 'tf_ratio': tf, 'lr': lr,\n",
    "            'train_total': tr['total'], 'val_total': va['total'],\n",
    "            'train_bce': tr['bce'], 'val_bce': va['bce'],\n",
    "            'train_nll': tr['nll'], 'val_nll': va['nll'],\n",
    "            'train_dir_acc': tr['dir_acc'], 'val_dir_acc': va['dir_acc'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
    "              f\"train={tr['total']:.6f} (bce={tr['bce']:.4f}, nll={tr['nll']:.4f}, dir_acc={tr['dir_acc']:.2%}) | \"\n",
    "              f\"val={va['total']:.6f} (bce={va['bce']:.4f}, nll={va['nll']:.4f}, dir_acc={va['dir_acc']:.2%}) | lr={lr:.6g}\")\n",
    "        \n",
    "        if va['total'] < best_val:\n",
    "            best_val = va['total']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}.')\n",
    "                break\n",
    "                \n",
    "    model.load_state_dict(best_state)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(actual_ohlc, pred_ohlc, prev_close):\n",
    "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
    "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
    "    ac, pc = actual_ohlc[:, 3], pred_ohlc[:, 3]\n",
    "    \n",
    "    return {\n",
    "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
    "        'close_rmse': float(np.sqrt(np.mean((ac - pc) ** 2))),\n",
    "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
    "        'directional_accuracy_eps': float(np.mean(np.sign(ac - prev_close) == np.sign(pc - prev_close))),\n",
    "    }\n",
    "\n",
    "def evaluate_baselines(actual_ohlc, prev_ohlc, prev_close):\n",
    "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
    "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
    "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
    "    return {'persistence': persistence, 'flat_close_rw': flat_rw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_realistic_recursive(model, X, context_prices, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate realistic predictions using autoregressive sampling with dual-head bias.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    log_returns = np.log(context_prices[1:] / context_prices[:-1])\n",
    "    historical_vol = float(np.std(log_returns)) if len(log_returns) > 1 else 0.001\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X).float().to(DEVICE)\n",
    "    \n",
    "    generated, direction_prob = model.generate_realistic(\n",
    "        X_tensor, temperature=temperature, \n",
    "        historical_vol=historical_vol,\n",
    "        apply_direction_bias=True\n",
    "    )\n",
    "    \n",
    "    return generated.detach().cpu().numpy()[0], direction_prob.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold_name, price_fold, window, max_epochs, patience, run_sanity=False, quick_mode=False):\n",
    "    feat_fold = build_feature_frame(price_fold)\n",
    "    target_fold = build_target_frame(feat_fold)\n",
    "    \n",
    "    input_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "    target_raw = target_fold[TARGET_COLS].to_numpy(np.float32)\n",
    "    direction_targets = feat_fold['direction_target'].to_numpy(np.float32)\n",
    "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "    row_open_skip = feat_fold['row_open_skip'].to_numpy(np.int8).astype(bool)\n",
    "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
    "    price_vals = price_fold.loc[feat_fold.index, OHLC_COLS].to_numpy(np.float32)\n",
    "    \n",
    "    tr_end, va_end = split_points(len(input_raw))\n",
    "    \n",
    "    in_mean, in_std = input_raw[:tr_end].mean(axis=0), input_raw[:tr_end].std(axis=0)\n",
    "    in_std = np.where(in_std < 1e-8, 1.0, in_std)\n",
    "    input_scaled = (input_raw - in_mean) / in_std\n",
    "    \n",
    "    tg_mean, tg_std = np.zeros(4, dtype=np.float32), np.ones(4, dtype=np.float32)\n",
    "    target_scaled = target_raw.copy()\n",
    "    \n",
    "    X_all, y_all_s, y_all_r, y_all_dir, starts, prev_close_starts, dropped_imputed, dropped_skip = make_multistep_windows(\n",
    "        input_scaled, target_scaled, target_raw, direction_targets, \n",
    "        row_imputed, row_open_skip, prev_close, window, HORIZON\n",
    "    )\n",
    "    \n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: no windows available.')\n",
    "    \n",
    "    end_idx = starts + HORIZON - 1\n",
    "    tr_m, va_m, te_m = end_idx < tr_end, (end_idx >= tr_end) & (end_idx < va_end), end_idx >= va_end\n",
    "    \n",
    "    X_train, y_train_s, y_train_r, y_train_dir = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m], y_all_dir[tr_m]\n",
    "    X_val, y_val_s, y_val_r, y_val_dir = X_all[va_m], y_all_s[va_m], y_all_r[va_m], y_all_dir[va_m]\n",
    "    X_test, y_test_s, y_test_r, y_test_dir = X_all[te_m], y_all_s[te_m], y_all_r[te_m], y_all_dir[te_m]\n",
    "    test_starts = starts[te_m]\n",
    "    test_prev_close = prev_close_starts[te_m]\n",
    "    \n",
    "    print(f'Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}')\n",
    "    print(f'Direction distribution - Train: UP={(y_train_dir==1).sum()}, DOWN={(y_train_dir==0).sum()}')\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        MultiStepDataset(X_train, y_train_s, y_train_r, y_train_dir), \n",
    "        batch_size=BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        MultiStepDataset(X_val, y_val_s, y_val_r, y_val_dir), \n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = DualHeadGRU(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=len(TARGET_COLS),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    hist = train_model(model, train_loader, val_loader, max_epochs, patience)\n",
    "    \n",
    "    # Realistic prediction with dual-head bias\n",
    "    last_idx = len(X_test) - 1\n",
    "    X_last = X_test[last_idx:last_idx+1]\n",
    "    context_start = int(test_starts[last_idx]) - window\n",
    "    context_prices = price_vals[context_start:int(test_starts[last_idx]), 3]\n",
    "    \n",
    "    pred_rets_realistic, direction_conf = predict_realistic_recursive(\n",
    "        model, X_last, context_prices, temperature=SAMPLING_TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    last_close = float(test_prev_close[last_idx])\n",
    "    pred_price_realistic = returns_to_prices_seq(pred_rets_realistic, last_close)\n",
    "    \n",
    "    actual_future = price_vals[int(test_starts[last_idx]):int(test_starts[last_idx])+HORIZON]\n",
    "    \n",
    "    # Direction accuracy vs baseline\n",
    "    actual_dir = float(actual_future[-1, 3] > last_close)\n",
    "    pred_dir = 1 if direction_conf > 0.5 else 0\n",
    "    dir_correct = int(pred_dir == actual_dir)\n",
    "    \n",
    "    # One-step metrics\n",
    "    mu_test, dir_logits_test = model(torch.from_numpy(X_test).float().to(DEVICE))\n",
    "    mu_test = mu_test.detach().cpu().numpy()\n",
    "    pred_step1_ret = mu_test[:, 0, :]\n",
    "    actual_step1_ret = y_test_r[:, 0, :]\n",
    "    \n",
    "    pred_ohlc_1 = np.zeros((len(test_starts), 4))\n",
    "    for i in range(len(test_starts)):\n",
    "        pc = test_prev_close[i]\n",
    "        pred_ohlc_1[i] = [\n",
    "            pc * np.exp(pred_step1_ret[i, 0]),\n",
    "            pc * np.exp(pred_step1_ret[i, 1]),\n",
    "            pc * np.exp(pred_step1_ret[i, 2]),\n",
    "            pc * np.exp(pred_step1_ret[i, 3]),\n",
    "        ]\n",
    "        pred_ohlc_1[i] = enforce_candle_validity(pred_ohlc_1[i].reshape(1, -1))[0]\n",
    "    \n",
    "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
    "    prev_ohlc = price_vals[test_starts]\n",
    "    \n",
    "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
    "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
    "    \n",
    "    print(f\"\\nDual-Head Prediction Stats:\")\n",
    "    print(f\"  Direction confidence: {direction_conf:.4f} ({'UP' if direction_conf > 0.5 else 'DOWN'})\")\n",
    "    print(f\"  Direction correct: {bool(dir_correct)}\")\n",
    "    print(f\"  Pred close range: [{pred_price_realistic[:, 3].min():.2f}, {pred_price_realistic[:, 3].max():.2f}]\")\n",
    "    print(f\"  Actual close range: [{actual_future[:, 3].min():.2f}, {actual_future[:, 3].max():.2f}]\")\n",
    "    print(f\"  Pred volatility: {np.std(pred_rets_realistic[:, 3]):.6f}\")\n",
    "    print(f\"  Actual volatility: {np.std(actual_step1_ret[:, 3]):.6f}\")\n",
    "    \n",
    "    future_idx = price_fold.index[test_starts[last_idx]:test_starts[last_idx]+HORIZON]\n",
    "    pred_future_df = pd.DataFrame(pred_price_realistic, index=future_idx, columns=OHLC_COLS)\n",
    "    actual_future_df = pd.DataFrame(actual_future, index=future_idx, columns=OHLC_COLS)\n",
    "    context_df = price_fold.iloc[test_starts[last_idx]-window:test_starts[last_idx]+1][OHLC_COLS]\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_name,\n",
    "        'window': window,\n",
    "        'history_df': hist,\n",
    "        'model_metrics': model_metrics,\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'context_df': context_df,\n",
    "        'actual_future_df': actual_future_df,\n",
    "        'pred_future_df': pred_future_df,\n",
    "        'direction_conf': direction_conf,\n",
    "        'dir_correct': dir_correct,\n",
    "        'samples': {'train': len(X_train), 'val': len(X_val), 'test': len(X_test)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lookback sweep if enabled\n",
    "fold_results = []\n",
    "primary_slice = slices[0]\n",
    "selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "if ENABLE_LOOKBACK_SWEEP:\n",
    "    print('\\n=== Lookback sweep ===')\n",
    "    _, a0, b0 = primary_slice\n",
    "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for w in LOOKBACK_CANDIDATES:\n",
    "        print(f'\\nSweep candidate lookback={w} --')\n",
    "        try:\n",
    "            r = run_fold(f'sweep_w{w}', fold_price0, w, SWEEP_MAX_EPOCHS, SWEEP_PATIENCE, quick_mode=True)\n",
    "            score = -r['model_metrics']['close_mae']\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                selected_window = w\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for window {w}: {e}\")\n",
    "\n",
    "print(f'\\nSelected lookback: {selected_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full walk-forward with dual-head architecture\n",
    "print('\\n=== Full walk-forward with Dual-Head architecture ===')\n",
    "for i, (name, a, b) in enumerate(slices, start=1):\n",
    "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
    "    fold_price = price_df.iloc[a:b].copy()\n",
    "    try:\n",
    "        res = run_fold(name, fold_price, selected_window, FINAL_MAX_EPOCHS, FINAL_PATIENCE)\n",
    "        fold_results.append(res)\n",
    "        \n",
    "        print(f\"\\nResults for {name}:\")\n",
    "        print(f\"  Model MAE: {res['model_metrics']['close_mae']:.4f}\")\n",
    "        print(f\"  Persistence MAE: {res['baseline_metrics']['persistence']['close_mae']:.4f}\")\n",
    "        print(f\"  Direction confidence: {res['direction_conf']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8), facecolor='black')\n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    def draw_candles(ax, ohlc, start_x, up_edge, up_face, down_edge, down_face, wick_color, width=0.6, alpha=1.0):\n",
    "        vals = ohlc[OHLC_COLS].to_numpy()\n",
    "        for i, (o, h, l, c) in enumerate(vals):\n",
    "            x = start_x + i\n",
    "            bull = c >= o\n",
    "            ax.vlines(x, l, h, color=wick_color, linewidth=1.0, alpha=alpha, zorder=2)\n",
    "            lower = min(o, c)\n",
    "            height = max(abs(c - o), 1e-6)\n",
    "            rect = Rectangle((x - width/2, lower), width, height,\n",
    "                           facecolor=up_face if bull else down_face,\n",
    "                           edgecolor=up_edge if bull else down_edge,\n",
    "                           linewidth=1.0, alpha=alpha, zorder=3)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    context_df = latest['context_df']\n",
    "    actual_future_df = latest['actual_future_df']\n",
    "    pred_future_df = latest['pred_future_df']\n",
    "    \n",
    "    # Draw history (green/red)\n",
    "    draw_candles(ax, context_df, 0, '#00FF00', '#00FF00', '#FF0000', '#FF0000', '#FFFFFF', width=0.6, alpha=0.9)\n",
    "    \n",
    "    # Draw actual future (dimmed)\n",
    "    draw_candles(ax, actual_future_df, len(context_df), '#00AA00', '#00AA00', '#AA0000', '#AA0000', '#888888', \n",
    "                 width=0.6, alpha=0.6)\n",
    "    \n",
    "    # Draw realistic prediction (bright white/black with glow effect)\n",
    "    draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#888888', '#000000', '#FFFFFF',\n",
    "                 width=0.5, alpha=1.0)\n",
    "    \n",
    "    ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=1.0, alpha=0.8)\n",
    "    \n",
    "    # Labels\n",
    "    n = len(context_df) + len(actual_future_df)\n",
    "    step = max(1, n // 12)\n",
    "    ticks = list(range(0, n, step))\n",
    "    all_idx = context_df.index.append(actual_future_df.index)\n",
    "    labels = [all_idx[i].strftime('%m-%d %H:%M') for i in ticks if i < len(all_idx)]\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=30, ha='right', color='white', fontsize=9)\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_color('#666666')\n",
    "    ax.grid(color='#333333', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    dir_text = f\"Direction: {'UP' if latest['direction_conf'] > 0.5 else 'DOWN'} ({latest['direction_conf']:.2%})\"\n",
    "    ax.set_title(f\"MSFT 1m ({latest['fold']}) - Dual-Head Forecast | {dir_text}\", \n",
    "                 color='white', fontsize=14, pad=15)\n",
    "    ax.set_ylabel('Price', color='white', fontsize=12)\n",
    "    \n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#00FF00', edgecolor='#00FF00', label='History (bull)'),\n",
    "        Patch(facecolor='#FF0000', edgecolor='#FF0000', label='History (bear)'),\n",
    "        Patch(facecolor='#00AA00', edgecolor='#00AA00', label='Actual Future (dim)'),\n",
    "        Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted (bull)'),\n",
    "        Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted (bear)'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, facecolor='black', edgecolor='white', labelcolor='white', \n",
    "             loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal chart shows dual-head generation with temperature {SAMPLING_TEMPERATURE}\")\n",
    "    print(f\"Direction bias scale: {DIRECTION_BIAS_SCALE} (applied during inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell: Evaluation Metrics\n",
    "\n",
    "This cell evaluates:\n",
    "1. Directional accuracy vs persistence baseline\n",
    "2. Average predicted close vs actual close (bias)\n",
    "3. Visual confirmation of realistic candle wicks/bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell: Comprehensive Evaluation\n",
    "print(\"=\" * 60)\n",
    "print(\"DUAL-HEAD MODEL EVALUATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if fold_results:\n",
    "    for result in fold_results:\n",
    "        fold_name = result['fold']\n",
    "        print(f\"\\n--- {fold_name} ---\")\n",
    "        \n",
    "        # 1. Directional Accuracy vs Persistence Baseline\n",
    "        model_dir_acc = result['model_metrics']['directional_accuracy_eps']\n",
    "        persist_dir_acc = result['baseline_metrics']['persistence']['directional_accuracy_eps']\n",
    "        print(f\"\\n1. DIRECTIONAL ACCURACY:\")\n",
    "        print(f\"   Model:           {model_dir_acc:.2%}\")\n",
    "        print(f\"   Persistence:     {persist_dir_acc:.2%}\")\n",
    "        print(f\"   Improvement:     {model_dir_acc - persist_dir_acc:+.2%}\")\n",
    "        \n",
    "        # Direction head specific accuracy\n",
    "        print(f\"   Direction Head:  {result['direction_conf']:.4f} confidence\")\n",
    "        print(f\"   Direction Match: {'YES' if result['dir_correct'] else 'NO'}\")\n",
    "        \n",
    "        # 2. Bias Analysis (Average predicted close vs actual)\n",
    "        pred_closes = result['pred_future_df']['Close'].values\n",
    "        actual_closes = result['actual_future_df']['Close'].values\n",
    "        avg_pred = pred_closes.mean()\n",
    "        avg_actual = actual_closes.mean()\n",
    "        bias = avg_pred - avg_actual\n",
    "        mae = np.mean(np.abs(pred_closes - actual_closes))\n",
    "        \n",
    "        print(f\"\\n2. BIAS ANALYSIS:\")\n",
    "        print(f\"   Avg Predicted Close: ${avg_pred:.2f}\")\n",
    "        print(f\"   Avg Actual Close:    ${avg_actual:.2f}\")\n",
    "        print(f\"   Bias:                ${bias:+.2f}\")\n",
    "        print(f\"   MAE:                 ${mae:.2f}\")\n",
    "        \n",
    "        # 3. Candle Realism Check\n",
    "        pred_ohlc = result['pred_future_df'].values\n",
    "        valid_candles = 0\n",
    "        body_sizes = []\n",
    "        wick_sizes = []\n",
    "        \n",
    "        for o, h, l, c in pred_ohlc:\n",
    "            # Check validity\n",
    "            if h >= max(o, c) and l <= min(o, c):\n",
    "                valid_candles += 1\n",
    "            # Calculate body and wick sizes\n",
    "            body_size = abs(c - o)\n",
    "            upper_wick = h - max(o, c)\n",
    "            lower_wick = min(o, c) - l\n",
    "            body_sizes.append(body_size)\n",
    "            wick_sizes.append(upper_wick + lower_wick)\n",
    "        \n",
    "        print(f\"\\n3. CANDLE REALISM:\")\n",
    "        print(f\"   Valid candles:       {valid_candles}/{len(pred_ohlc)} ({valid_candles/len(pred_ohlc):.1%})\")\n",
    "        print(f\"   Avg body size:       ${np.mean(body_sizes):.2f}\")\n",
    "        print(f\"   Avg total wick:      ${np.mean(wick_sizes):.2f}\")\n",
    "        print(f\"   Body/Wick ratio:     {np.mean(body_sizes)/(np.mean(wick_sizes)+1e-6):.2f}\")\n",
    "        \n",
    "        # 4. Volatility Comparison\n",
    "        pred_vol = np.std(np.diff(pred_closes))\n",
    "        actual_vol = np.std(np.diff(actual_closes))\n",
    "        print(f\"\\n4. VOLATILITY COMPARISON:\")\n",
    "        print(f\"   Predicted volatility: {pred_vol:.6f}\")\n",
    "        print(f\"   Actual volatility:    {actual_vol:.6f}\")\n",
    "        print(f\"   Ratio (pred/act):     {pred_vol/(actual_vol+1e-6):.2f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DUAL-HEAD ARCHITECTURE VERIFICATION:\")\n",
    "    print(f\"  - Head A (Direction): Binary classification with BCE loss\")\n",
    "    print(f\"  - Head B (Candles): NLL + 0.5*MSE(volatility) loss\")\n",
    "    print(f\"  - Inference bias: Direction confidence biases mu by {DIRECTION_BIAS_SCALE}\")\n",
    "    print(f\"  - Autoregressive generation with temperature {SAMPLING_TEMPERATURE}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No results available. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a **Dual-Head Architecture** for MSFT 1-minute forecasting:\n",
    "\n",
    "### Architecture Changes (v7.2):\n",
    "1. **Head A (Direction)**: Binary classifier predicting if `close[t+horizon] > close[t]`\n",
    "2. **Head B (Candle Generator)**: Predicts full OHLC returns, conditioned on Head A's output\n",
    "3. **Combined Loss**: `BCE(direction) + NLL(candles) + 0.5*MSE(volatility)`\n",
    "\n",
    "### Key Features:\n",
    "- Direction probability concatenated to decoder input for conditioning\n",
    "- During inference, direction confidence biases the mu (mean) predictions\n",
    "- Maintains autoregressive generation with temperature control\n",
    "- Candle validity enforcement ensures `High >= max(Open,Close)` and `Low <= min(Open,Close)`\n",
    "\n",
    "### Usage Notes:\n",
    "- Increase `DIRECTION_BIAS_SCALE` for stronger directional influence\n",
    "- Adjust `BCE_WEIGHT`, `NLL_WEIGHT`, `VOL_MSE_WEIGHT` to balance the dual objectives\n",
    "- The direction head provides interpretable trend predictions alongside detailed candles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
