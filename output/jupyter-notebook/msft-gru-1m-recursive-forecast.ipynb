{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7aef4bc9",
      "metadata": {},
      "source": [
        "# Experiment: MSFT 1-Minute GRU Recursive Forecast (PRD)\n",
        "\n",
        "Objective:\n",
        "- Train a PyTorch GRU on MSFT 1-minute OHLC candles from Alpaca (target: last 60 days).\n",
        "- Use 500-candle windows to predict the next candle.\n",
        "- Recursively forecast 15 future candles from one seed window.\n",
        "- Visualize predicted candles in the requested color style.\n",
        "\n",
        "Success criteria:\n",
        "- Notebook runs top-to-bottom and reports one-step and recursive metrics.\n",
        "- Final chart clearly distinguishes history (green/red) vs predictions (white/black).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13096861",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required third-party packages are already installed.\n",
            "If torch is missing, install it separately with a CUDA wheel suitable for your RTX 3070 and driver.\n"
          ]
        }
      ],
      "source": [
        "# Optional dependency bootstrap (kept lightweight).\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required = {\n",
        "    'alpaca': 'alpaca-py',\n",
        "    'numpy': 'numpy',\n",
        "    'pandas': 'pandas',\n",
        "    'matplotlib': 'matplotlib',\n",
        "    'sklearn': 'scikit-learn',\n",
        "}\n",
        "\n",
        "missing = [pkg for module_name, pkg in required.items() if importlib.util.find_spec(module_name) is None]\n",
        "if missing:\n",
        "    print('Installing missing packages:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
        "else:\n",
        "    print('All required third-party packages are already installed.')\n",
        "\n",
        "print('If torch is missing, install it separately with a CUDA wheel suitable for your RTX 3070 and driver.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1daf778b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce RTX 3070\n",
            "CUDA: 12.1\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports and reproducibility\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from alpaca.data.enums import DataFeed\n",
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests import StockBarsRequest\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('CUDA:', torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e5275f",
      "metadata": {},
      "source": [
        "## Plan\n",
        "\n",
        "- Data: download MSFT 1-minute OHLC candles over 60 days from Alpaca.\n",
        "- Free-plan safety: throttle requests to 120/min (below Alpaca basic limit of 200/min).\n",
        "- Features: raw OHLC only (no indicators).\n",
        "- Model: 2-layer GRU (hidden=128, dropout=0.2) -> linear head to next OHLC.\n",
        "- Training: time-aware split, train-only normalization, MSE loss, Adam, early stopping.\n",
        "- Inference: recursive 15-step forecast using own predictions as future inputs.\n",
        "- Visualization: history as green/red, predicted as white/black.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a46062ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'symbol': 'MSFT', 'interval': '1m', 'lookback_days': 60, 'window': 500, 'horizon': 15, 'device': 'cuda'}\n"
          ]
        }
      ],
      "source": [
        "# Experiment configuration\n",
        "SYMBOL = 'MSFT'\n",
        "LOOKBACK_DAYS = 60\n",
        "FEATURES = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "WINDOW = 500\n",
        "HORIZON = 15\n",
        "\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 12\n",
        "PATIENCE = 3\n",
        "\n",
        "# Alpaca config\n",
        "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()  # free-plan safe default\n",
        "REQUEST_CHUNK_DAYS = 5\n",
        "MAX_REQUESTS_PER_MINUTE = 120  # below free-plan 200/min cap\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "print({\n",
        "    'symbol': SYMBOL,\n",
        "    'lookback_days': LOOKBACK_DAYS,\n",
        "    'window': WINDOW,\n",
        "    'horizon': HORIZON,\n",
        "    'alpaca_feed': ALPACA_FEED,\n",
        "    'request_chunk_days': REQUEST_CHUNK_DAYS,\n",
        "    'max_requests_per_minute': MAX_REQUESTS_PER_MINUTE,\n",
        "    'device': str(DEVICE),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "997dd11e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to get ticker 'MSFT' reason: Failed to perform, curl: (6) Could not resolve host: guce.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n",
            "\n",
            "1 Failed download:\n",
            "['MSFT']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "Failed to get ticker 'MSFT' reason: Failed to perform, curl: (6) Could not resolve host: guce.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n",
            "\n",
            "1 Failed download:\n",
            "['MSFT']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "Failed to get ticker 'MSFT' reason: Failed to perform, curl: (6) Could not resolve host: guce.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n",
            "\n",
            "1 Failed download:\n",
            "['MSFT']: YFTzMissingError('possibly delisted; no timezone found')\n",
            "Failed to get ticker 'MSFT' reason: Failed to perform, curl: (6) Could not resolve host: guce.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.\n",
            "\n",
            "1 Failed download:\n",
            "['MSFT']: YFTzMissingError('possibly delisted; no timezone found')\n"
          ]
        }
      ],
      "source": [
        "# Data download and cleaning (Alpaca)\n",
        "class RequestPacer:\n",
        "    def __init__(self, max_calls_per_minute: int):\n",
        "        if max_calls_per_minute <= 0:\n",
        "            raise ValueError('max_calls_per_minute must be > 0')\n",
        "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
        "        self.last_call_ts = 0.0\n",
        "\n",
        "    def wait(self) -> None:\n",
        "        now = time.monotonic()\n",
        "        elapsed = now - self.last_call_ts\n",
        "        if elapsed < self.min_interval:\n",
        "            time.sleep(self.min_interval - elapsed)\n",
        "        self.last_call_ts = time.monotonic()\n",
        "\n",
        "\n",
        "def _require_alpaca_credentials() -> tuple[str, str]:\n",
        "    api_key = os.getenv('ALPACA_API_KEY')\n",
        "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
        "    if not api_key or not secret_key:\n",
        "        raise RuntimeError(\n",
        "            'Missing Alpaca credentials. Set ALPACA_API_KEY and ALPACA_SECRET_KEY before running this notebook.'\n",
        "        )\n",
        "    return api_key, secret_key\n",
        "\n",
        "\n",
        "def _resolve_feed(feed_name: str) -> DataFeed:\n",
        "    mapping = {\n",
        "        'iex': DataFeed.IEX,\n",
        "        'sip': DataFeed.SIP,\n",
        "        'delayed_sip': DataFeed.DELAYED_SIP,\n",
        "    }\n",
        "    key = feed_name.strip().lower()\n",
        "    if key not in mapping:\n",
        "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
        "    return mapping[key]\n",
        "\n",
        "\n",
        "def fetch_ohlc_1m_alpaca(\n",
        "    symbol: str,\n",
        "    lookback_days: int,\n",
        "    feed_name: str,\n",
        "    chunk_days: int,\n",
        "    max_calls_per_minute: int,\n",
        "    max_retries: int = 5,\n",
        ") -> tuple[pd.DataFrame, int]:\n",
        "    api_key, secret_key = _require_alpaca_credentials()\n",
        "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
        "\n",
        "    feed = _resolve_feed(feed_name)\n",
        "    pacer = RequestPacer(max_calls_per_minute=max_calls_per_minute)\n",
        "\n",
        "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
        "    # Conservative offset for free/basic access when using SIP-based feeds.\n",
        "    if feed_name in {'sip', 'delayed_sip'}:\n",
        "        end_ts = end_ts - timedelta(minutes=20)\n",
        "\n",
        "    start_ts = end_ts - timedelta(days=lookback_days)\n",
        "\n",
        "    frames: list[pd.DataFrame] = []\n",
        "    cursor = start_ts\n",
        "    api_call_count = 0\n",
        "\n",
        "    while cursor < end_ts:\n",
        "        chunk_end = min(cursor + timedelta(days=chunk_days), end_ts)\n",
        "        bars = None\n",
        "\n",
        "        for attempt in range(1, max_retries + 1):\n",
        "            pacer.wait()\n",
        "            api_call_count += 1\n",
        "            try:\n",
        "                req = StockBarsRequest(\n",
        "                    symbol_or_symbols=[symbol],\n",
        "                    timeframe=TimeFrame.Minute,\n",
        "                    start=cursor,\n",
        "                    end=chunk_end,\n",
        "                    feed=feed,\n",
        "                    limit=10_000,\n",
        "                )\n",
        "                bars = client.get_stock_bars(req).df\n",
        "                break\n",
        "            except Exception as exc:\n",
        "                msg = str(exc).lower()\n",
        "                if ('429' in msg or 'rate limit' in msg) and attempt < max_retries:\n",
        "                    backoff = min(2 ** attempt, 30)\n",
        "                    print(f'Rate limit response on chunk {cursor} -> {chunk_end}; sleeping {backoff}s (attempt {attempt}/{max_retries}).')\n",
        "                    time.sleep(backoff)\n",
        "                    continue\n",
        "\n",
        "                if ('subscription' in msg or 'forbidden' in msg) and feed_name != 'iex':\n",
        "                    raise RuntimeError(\n",
        "                        f'Feed {feed_name!r} is not available for this account. Use ALPACA_FEED=iex for free-plan compatibility.'\n",
        "                    ) from exc\n",
        "\n",
        "                raise\n",
        "\n",
        "        if bars is not None and not bars.empty:\n",
        "            chunk_df = bars.reset_index()\n",
        "            chunk_df = chunk_df.rename(\n",
        "                columns={\n",
        "                    'timestamp': 'Datetime',\n",
        "                    'open': 'Open',\n",
        "                    'high': 'High',\n",
        "                    'low': 'Low',\n",
        "                    'close': 'Close',\n",
        "                }\n",
        "            )\n",
        "\n",
        "            needed = ['Datetime', 'Open', 'High', 'Low', 'Close']\n",
        "            missing = [c for c in needed if c not in chunk_df.columns]\n",
        "            if missing:\n",
        "                raise RuntimeError(f'Alpaca response missing required columns: {missing}')\n",
        "\n",
        "            chunk_df['Datetime'] = pd.to_datetime(chunk_df['Datetime'], utc=True).dt.tz_convert(None)\n",
        "            chunk_df = chunk_df[needed].dropna().set_index('Datetime').sort_index()\n",
        "            frames.append(chunk_df)\n",
        "\n",
        "        cursor = chunk_end\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError('No bars returned from Alpaca. Verify symbol, credentials, feed, and market session.')\n",
        "\n",
        "    df = pd.concat(frames, axis=0).sort_index()\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    df = df.dropna(subset=FEATURES)\n",
        "\n",
        "    return df.astype(np.float32), api_call_count\n",
        "\n",
        "\n",
        "raw_df, api_calls = fetch_ohlc_1m_alpaca(\n",
        "    symbol=SYMBOL,\n",
        "    lookback_days=LOOKBACK_DAYS,\n",
        "    feed_name=ALPACA_FEED,\n",
        "    chunk_days=REQUEST_CHUNK_DAYS,\n",
        "    max_calls_per_minute=MAX_REQUESTS_PER_MINUTE,\n",
        "    max_retries=MAX_RETRIES,\n",
        ")\n",
        "\n",
        "span_days = (raw_df.index.max() - raw_df.index.min()).total_seconds() / 86400\n",
        "print(f'Rows: {len(raw_df):,}')\n",
        "print(f'Time span in returned data: {span_days:.1f} days')\n",
        "print(f'Alpaca API calls made: {api_calls} (throttled to <= {MAX_REQUESTS_PER_MINUTE} requests/min)')\n",
        "if span_days < LOOKBACK_DAYS * 0.9:\n",
        "    print('Warning: Returned span is shorter than 60 days. This can happen due to feed/session constraints.')\n",
        "\n",
        "min_needed = WINDOW + HORIZON + 200\n",
        "if len(raw_df) < min_needed:\n",
        "    raise RuntimeError(f'Not enough rows ({len(raw_df)}) for window={WINDOW} and horizon={HORIZON}. Need at least {min_needed}.')\n",
        "\n",
        "raw_df.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba77b46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build sequences with train/val/test time split and train-only normalization\n",
        "def split_indices(n_rows: int, train_ratio: float, val_ratio: float) -> tuple[int, int]:\n",
        "    train_end = int(n_rows * train_ratio)\n",
        "    val_end = int(n_rows * (train_ratio + val_ratio))\n",
        "    return train_end, val_end\n",
        "\n",
        "\n",
        "def make_windows(values: np.ndarray, window: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    X, y, target_idx = [], [], []\n",
        "    for i in range(window, len(values)):\n",
        "        X.append(values[i - window : i])\n",
        "        y.append(values[i])\n",
        "        target_idx.append(i)\n",
        "\n",
        "    return (\n",
        "        np.asarray(X, dtype=np.float32),\n",
        "        np.asarray(y, dtype=np.float32),\n",
        "        np.asarray(target_idx, dtype=np.int64),\n",
        "    )\n",
        "\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "raw_values = raw_df[FEATURES].to_numpy(dtype=np.float32)\n",
        "train_end, val_end = split_indices(len(raw_df), TRAIN_RATIO, VAL_RATIO)\n",
        "\n",
        "train_mean = raw_values[:train_end].mean(axis=0)\n",
        "train_std = raw_values[:train_end].std(axis=0)\n",
        "train_std = np.where(train_std < 1e-8, 1.0, train_std)\n",
        "\n",
        "scaled_values = (raw_values - train_mean) / train_std\n",
        "\n",
        "X_all, y_all, seq_idx = make_windows(scaled_values, WINDOW)\n",
        "\n",
        "train_mask = seq_idx < train_end\n",
        "val_mask = (seq_idx >= train_end) & (seq_idx < val_end)\n",
        "test_mask = seq_idx >= val_end\n",
        "\n",
        "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
        "X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
        "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
        "\n",
        "train_loader = DataLoader(SequenceDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader = DataLoader(SequenceDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader = DataLoader(SequenceDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "print('Split rows:', {'train': train_end, 'val': val_end - train_end, 'test': len(raw_df) - val_end})\n",
        "print('Windowed samples:', {'train': len(X_train), 'val': len(X_val), 'test': len(X_test)})\n",
        "print('X_train shape:', X_train.shape, 'y_train shape:', y_train.shape)\n",
        "if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
        "    raise RuntimeError('One or more splits are empty. Increase data range or adjust split ratios.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b459e0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRU model + training loop\n",
        "class GRUForecaster(nn.Module):\n",
        "    def __init__(self, input_size: int = 4, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out, _ = self.gru(x)\n",
        "        return self.head(out[:, -1, :])\n",
        "\n",
        "\n",
        "def run_epoch(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, optimizer: torch.optim.Optimizer | None = None) -> float:\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_items = 0\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb = yb.to(DEVICE)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        batch_size = xb.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_items += batch_size\n",
        "\n",
        "    return total_loss / max(total_items, 1)\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int, lr: float, patience: int):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "    wait = 0\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = run_epoch(model, train_loader, loss_fn, optimizer)\n",
        "        val_loss = run_epoch(model, val_loader, loss_fn, optimizer=None)\n",
        "\n",
        "        history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
        "        print(f'Epoch {epoch:02d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}')\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}.')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209c98cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "model = GRUForecaster(\n",
        "    input_size=len(FEATURES),\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        ").to(DEVICE)\n",
        "\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE,\n",
        "    patience=PATIENCE,\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history)\n",
        "display(history_df.tail())\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history_df['epoch'], history_df['train_loss'], label='Train', color='black')\n",
        "plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation', color='gray')\n",
        "plt.title('GRU Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97bc2b0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-step evaluation on test set\n",
        "def inverse_scale(arr: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return arr * std + mean\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_preds(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yhat = model(xb).cpu().numpy()\n",
        "        preds.append(yhat)\n",
        "        targets.append(yb.numpy())\n",
        "\n",
        "    return np.vstack(preds), np.vstack(targets)\n",
        "\n",
        "\n",
        "pred_test_scaled, y_test_scaled = collect_preds(model, test_loader)\n",
        "pred_test = inverse_scale(pred_test_scaled, train_mean, train_std)\n",
        "y_test_raw = inverse_scale(y_test_scaled, train_mean, train_std)\n",
        "\n",
        "test_target_indices = seq_idx[test_mask]\n",
        "prev_close = raw_values[test_target_indices - 1, 3]\n",
        "actual_close = y_test_raw[:, 3]\n",
        "pred_close = pred_test[:, 3]\n",
        "\n",
        "close_mae = mean_absolute_error(actual_close, pred_close)\n",
        "close_rmse = mean_squared_error(actual_close, pred_close, squared=False)\n",
        "ohlc_mae = mean_absolute_error(y_test_raw.reshape(-1), pred_test.reshape(-1))\n",
        "ohlc_rmse = mean_squared_error(y_test_raw.reshape(-1), pred_test.reshape(-1), squared=False)\n",
        "dir_acc = np.mean(np.sign(actual_close - prev_close) == np.sign(pred_close - prev_close))\n",
        "\n",
        "one_step_metrics = {\n",
        "    'close_mae': float(close_mae),\n",
        "    'close_rmse': float(close_rmse),\n",
        "    'ohlc_mae': float(ohlc_mae),\n",
        "    'ohlc_rmse': float(ohlc_rmse),\n",
        "    'directional_accuracy': float(dir_acc),\n",
        "}\n",
        "\n",
        "print('One-step test metrics:')\n",
        "for k, v in one_step_metrics.items():\n",
        "    print(f'  {k}: {v:.6f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd5f338",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recursive 15-step forecast\n",
        "@torch.no_grad()\n",
        "def recursive_forecast(model: nn.Module, seed_window_scaled: np.ndarray, horizon: int) -> np.ndarray:\n",
        "    model.eval()\n",
        "    window = seed_window_scaled.copy()\n",
        "    preds = []\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        x = torch.from_numpy(window).float().unsqueeze(0).to(DEVICE)\n",
        "        next_step = model(x).cpu().numpy()[0]\n",
        "        preds.append(next_step)\n",
        "        window = np.vstack([window[1:], next_step])\n",
        "\n",
        "    return np.asarray(preds, dtype=np.float32)\n",
        "\n",
        "\n",
        "anchor_end = len(raw_df) - HORIZON - 1\n",
        "if anchor_end - WINDOW + 1 < 0:\n",
        "    raise RuntimeError('Not enough rows for recursive seed window.')\n",
        "\n",
        "seed_scaled = scaled_values[anchor_end - WINDOW + 1 : anchor_end + 1]\n",
        "recursive_scaled = recursive_forecast(model, seed_scaled, HORIZON)\n",
        "recursive_pred = inverse_scale(recursive_scaled, train_mean, train_std)\n",
        "\n",
        "future_actual = raw_values[anchor_end + 1 : anchor_end + 1 + HORIZON]\n",
        "future_index = raw_df.index[anchor_end + 1 : anchor_end + 1 + HORIZON]\n",
        "\n",
        "pred_future_df = pd.DataFrame(recursive_pred, index=future_index, columns=FEATURES)\n",
        "actual_future_df = pd.DataFrame(future_actual, index=future_index, columns=FEATURES)\n",
        "\n",
        "recursive_close_mae = mean_absolute_error(actual_future_df['Close'], pred_future_df['Close'])\n",
        "recursive_close_rmse = mean_squared_error(actual_future_df['Close'], pred_future_df['Close'], squared=False)\n",
        "\n",
        "print('Recursive 15-step metrics (close):')\n",
        "print(f'  mae:  {recursive_close_mae:.6f}')\n",
        "print(f'  rmse: {recursive_close_rmse:.6f}')\n",
        "\n",
        "display(pd.concat({\n",
        "    'actual_close': actual_future_df['Close'],\n",
        "    'pred_close': pred_future_df['Close'],\n",
        "}, axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15288e2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Candlestick rendering with requested color mapping\n",
        "# - History: green (bullish) / red (bearish)\n",
        "# - Predicted: white (bullish) / black (bearish)\n",
        "def draw_candles(\n",
        "    ax,\n",
        "    ohlc: pd.DataFrame,\n",
        "    start_x: int,\n",
        "    up_edge: str,\n",
        "    up_face: str,\n",
        "    down_edge: str,\n",
        "    down_face: str,\n",
        "    wick_color: str,\n",
        "    width: float = 0.62,\n",
        "    lw: float = 1.0,\n",
        "    alpha: float = 1.0,\n",
        "):\n",
        "    values = ohlc[['Open', 'High', 'Low', 'Close']].to_numpy()\n",
        "\n",
        "    for i, (o, h, l, c) in enumerate(values):\n",
        "        x = start_x + i\n",
        "        bullish = c >= o\n",
        "\n",
        "        ax.vlines(x, l, h, color=wick_color, linewidth=lw, alpha=alpha, zorder=2)\n",
        "\n",
        "        lower = min(o, c)\n",
        "        height = abs(c - o)\n",
        "        if height < 1e-8:\n",
        "            height = 1e-6\n",
        "\n",
        "        face = up_face if bullish else down_face\n",
        "        edge = up_edge if bullish else down_edge\n",
        "\n",
        "        rect = Rectangle(\n",
        "            (x - width / 2, lower),\n",
        "            width,\n",
        "            height,\n",
        "            facecolor=face,\n",
        "            edgecolor=edge,\n",
        "            linewidth=lw,\n",
        "            alpha=alpha,\n",
        "            zorder=3,\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "\n",
        "context_bars = 170\n",
        "context_start = max(0, anchor_end - context_bars + 1)\n",
        "context_df = raw_df.iloc[context_start : anchor_end + 1][FEATURES].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(17, 8), facecolor='black')\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "# Historical candles (actual): green/red\n",
        "draw_candles(\n",
        "    ax,\n",
        "    context_df,\n",
        "    start_x=0,\n",
        "    up_edge='#2ECC71',\n",
        "    up_face='#2ECC71',\n",
        "    down_edge='#E74C3C',\n",
        "    down_face='#E74C3C',\n",
        "    wick_color='#D9D9D9',\n",
        "    width=0.58,\n",
        "    lw=1.0,\n",
        "    alpha=0.95,\n",
        ")\n",
        "\n",
        "# Actual future candles (for comparison): muted green/red\n",
        "draw_candles(\n",
        "    ax,\n",
        "    actual_future_df,\n",
        "    start_x=len(context_df),\n",
        "    up_edge='#1E8F4E',\n",
        "    up_face='#1E8F4E',\n",
        "    down_edge='#B03A2E',\n",
        "    down_face='#B03A2E',\n",
        "    wick_color='#9A9A9A',\n",
        "    width=0.58,\n",
        "    lw=1.0,\n",
        "    alpha=0.75,\n",
        ")\n",
        "\n",
        "# Predicted future candles: white if bullish (green-equivalent), black if bearish (red-equivalent)\n",
        "draw_candles(\n",
        "    ax,\n",
        "    pred_future_df,\n",
        "    start_x=len(context_df),\n",
        "    up_edge='#FFFFFF',\n",
        "    up_face='#FFFFFF',\n",
        "    down_edge='#000000',\n",
        "    down_face='#000000',\n",
        "    wick_color='#F0F0F0',\n",
        "    width=0.50,\n",
        "    lw=1.35,\n",
        "    alpha=1.0,\n",
        ")\n",
        "\n",
        "split_x = len(context_df) - 0.5\n",
        "ax.axvline(split_x, color='white', linestyle='--', linewidth=0.8, alpha=0.5)\n",
        "\n",
        "combined_index = context_df.index.append(actual_future_df.index)\n",
        "total_bars = len(combined_index)\n",
        "tick_step = max(1, total_bars // 10)\n",
        "ticks = list(range(0, total_bars, tick_step))\n",
        "if ticks[-1] != total_bars - 1:\n",
        "    ticks.append(total_bars - 1)\n",
        "\n",
        "labels = [combined_index[i].strftime('%m-%d %H:%M') for i in ticks]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(labels, rotation=28, ha='right', color='white', fontsize=9)\n",
        "\n",
        "ax.tick_params(axis='y', colors='white')\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_color('#606060')\n",
        "\n",
        "ax.grid(color='#222222', linewidth=0.6, alpha=0.35)\n",
        "ax.set_title('MSFT 1-Minute Candles: History + 15-Step Recursive GRU Forecast', color='white', pad=16)\n",
        "ax.set_ylabel('Price', color='white')\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='History bullish (green)'),\n",
        "    Patch(facecolor='#E74C3C', edgecolor='#E74C3C', label='History bearish (red)'),\n",
        "    Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted bullish (white)'),\n",
        "    Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted bearish (black)'),\n",
        "]\n",
        "leg = ax.legend(handles=legend_elements, facecolor='black', edgecolor='#707070', framealpha=1.0, loc='upper left')\n",
        "for text in leg.get_texts():\n",
        "    text.set_color('white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "741c4917",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "- `one_step_metrics` reports t+1 quality over the test split.\n",
        "- `recursive_close_mae/rmse` report 15-step recursive error accumulation.\n",
        "- Final chart uses requested style: history green/red and predicted candles white/black.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "- If your account/feed allows, compare `ALPACA_FEED=iex` vs `ALPACA_FEED=delayed_sip` quality.\n",
        "- Add walk-forward evaluation windows.\n",
        "- After baseline lock-in, extend to multi-symbol training and then crypto.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (msft-gru)",
      "language": "python",
      "name": "msft-gru"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
