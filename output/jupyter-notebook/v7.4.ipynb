{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: MSFT 1-Minute Transformer + Long Memory Forecast (v7.4)\n",
    "\n",
    "Key changes from v7 GRU baseline:\n",
    "1. **Transformer Encoder** (4 heads, 2 layers, d_model=256) replacing GRU encoder\n",
    "2. **Extended lookback** from 96 to 512 bars for longer memory\n",
    "3. **Positional encoding** for sequence order awareness\n",
    "4. **Memory attention** - decoder attends to past volatile events in lookback window\n",
    "5. Keep GRU decoder (autoregressive) using Transformer encoder outputs as context\n",
    "6. Probabilistic outputs (mu + log_sigma) with temperature control\n",
    "7. Strict candle validity enforcement at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    'alpaca': 'alpaca-py',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'pandas_market_calendars': 'pandas-market-calendars',\n",
    "}\n",
    "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "else:\n",
    "    print('All required third-party packages are already installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Configuration\n",
    "SYMBOL = 'MSFT'\n",
    "LOOKBACK_DAYS = 120\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen', 'rHigh', 'rLow', 'rClose',\n",
    "    'logVolChange', 'logTradeCountChange',\n",
    "    'vwapDelta', 'rangeFrac', 'orderFlowProxy', 'tickPressure',\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
    "\n",
    "HORIZON = 15\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "# v7.4: Extended lookback candidates for longer memory (was [64, 96, 160, 256])\n",
    "LOOKBACK_CANDIDATES = [256, 384, 512, 640]\n",
    "DEFAULT_LOOKBACK = 512  # v7.4: Extended from 96 to 512\n",
    "ENABLE_LOOKBACK_SWEEP = True\n",
    "SKIP_OPEN_BARS_TARGET = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration - v7.4 Transformer Settings\n",
    "D_MODEL = 256  # Transformer dimension (was HIDDEN_SIZE)\n",
    "NUM_HEADS = 4  # Attention heads\n",
    "NUM_ENCODER_LAYERS = 2  # Transformer encoder layers\n",
    "NUM_DECODER_LAYERS = 2  # GRU decoder layers (depth)\n",
    "DROPOUT = 0.20\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Memory attention configuration\n",
    "MEMORY_ATTENTION_DIM = 128  # Dimension for memory attention mechanism\n",
    "VOLATILITY_THRESHOLD = 0.9  # Percentile threshold for volatile event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "SWEEP_MAX_EPOCHS = 15\n",
    "SWEEP_PATIENCE = 5\n",
    "FINAL_MAX_EPOCHS = 60\n",
    "FINAL_PATIENCE = 12\n",
    "TF_START = 1.0\n",
    "TF_END = 0.0\n",
    "TF_DECAY_RATE = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Configuration\n",
    "RANGE_LOSS_WEIGHT = 0.3\n",
    "VOLATILITY_WEIGHT = 0.5\n",
    "DIR_PENALTY_WEIGHT = 0.1\n",
    "STEP_LOSS_POWER = 1.5\n",
    "MEMORY_ATTENTION_WEIGHT = 0.1  # v7.4: Weight for memory attention loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Configuration\n",
    "SAMPLING_TEMPERATURE = 1.5\n",
    "VOLATILITY_SCALING = True\n",
    "MIN_PREDICTED_VOL = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Configuration\n",
    "STANDARDIZE_TARGETS = False\n",
    "APPLY_CLIPPING = True\n",
    "CLIP_QUANTILES = (0.001, 0.999)\n",
    "DIRECTION_EPS = 0.0001\n",
    "STD_RATIO_TARGET_MIN = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca API Configuration\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Configuration Summary\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'lookback_days': LOOKBACK_DAYS,\n",
    "    'horizon': HORIZON,\n",
    "    'default_lookback': DEFAULT_LOOKBACK,  # v7.4: now 512\n",
    "    'transformer_d_model': D_MODEL,\n",
    "    'transformer_heads': NUM_HEADS,\n",
    "    'transformer_layers': NUM_ENCODER_LAYERS,\n",
    "    'sampling_temperature': SAMPLING_TEMPERATURE,\n",
    "    'loss_weights': {\n",
    "        'range': RANGE_LOSS_WEIGHT,\n",
    "        'volatility': VOLATILITY_WEIGHT,\n",
    "        'dir_penalty': DIR_PENALTY_WEIGHT,\n",
    "        'memory_attention': MEMORY_ATTENTION_WEIGHT,\n",
    "    },\n",
    "    'device': str(DEVICE),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be >0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "        \n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "    \n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "    \n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(columns={\n",
    "                'timestamp': 'Datetime', 'open': 'Open', 'high': 'High',\n",
    "                'low': 'Low', 'close': 'Close', 'volume': 'Volume',\n",
    "                'trade_count': 'TradeCount', 'vwap': 'VWAP',\n",
    "            })\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "            \n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "        cursor = chunk_end\n",
    "    \n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "    \n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    \n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "    \n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "    \n",
    "    pieces = []\n",
    "    fill_ratios = []\n",
    "    \n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        \n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "            \n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)]\n",
    "        day = day.reindex(exp_idx)\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "        \n",
    "        if fill_ratio >= 1.0 or fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            continue\n",
    "            \n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        if day['VWAP'].isna().all():\n",
    "            day['VWAP'] = day['Close']\n",
    "        else:\n",
    "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "            \n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "        \n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after per-session fill.')\n",
    "        pieces.append(day)\n",
    "        fill_ratios.append(fill_ratio)\n",
    "    \n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after calendar filtering.')\n",
    "        \n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "    out = out.copy()\n",
    "    \n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "    \n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(len(pieces)),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "    }\n",
    "    return out, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
    "print(f'Sessionized rows kept: {len(price_df):,}')\n",
    "print('Session meta:', session_meta)\n",
    "\n",
    "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
    "if len(price_df) < min_needed:\n",
    "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ensure High >= max(Open,Close) and Low <= min(Open,Close)\"\"\"\n",
    "    out = np.asarray(ohlc, dtype=np.float32)\n",
    "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    eps = 1e-9\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "    \n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0) / (np.abs(out['rClose']) + eps)\n",
    "    \n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "    \n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def build_target_frame(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return feat_df[TARGET_COLS].copy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = build_feature_frame(price_df)\n",
    "target_df = build_target_frame(feat_df)\n",
    "print('Feature rows:', len(feat_df))\n",
    "print('Target columns:', list(target_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing & Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_points(n_rows: int) -> tuple[int, int]:\n",
    "    tr = int(n_rows * TRAIN_RATIO)\n",
    "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
    "    return tr, va\n",
    "\n",
    "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
    "    n = len(price_df_full)\n",
    "    span = int(round(n * 0.85))\n",
    "    shift = max(1, n - span)\n",
    "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for name, a, b in cands:\n",
    "        key = (a, b)\n",
    "        if key in seen or b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1400:\n",
    "            continue\n",
    "        out.append((name, a, b))\n",
    "        seen.add(key)\n",
    "    return out if out else [('full', 0, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multistep_windows(input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, \n",
    "                           starts_prev_close, window, horizon):\n",
    "    X, y_s, y_r, starts, prev_close = [], [], [], [], []\n",
    "    dropped_target_imputed, dropped_target_open_skip = 0, 0\n",
    "    n = len(input_scaled)\n",
    "    \n",
    "    for i in range(window, n - horizon + 1):\n",
    "        if row_imputed[i:i+horizon].any():\n",
    "            dropped_target_imputed += 1\n",
    "            continue\n",
    "        if row_open_skip[i:i+horizon].any():\n",
    "            dropped_target_open_skip += 1\n",
    "            continue\n",
    "            \n",
    "        xb = input_scaled[i-window:i]\n",
    "        imp_frac = float(row_imputed[i-window:i].mean())\n",
    "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
    "        xb_aug = np.concatenate([xb, imp_col], axis=1)\n",
    "        \n",
    "        X.append(xb_aug)\n",
    "        y_s.append(target_scaled[i:i+horizon])\n",
    "        y_r.append(target_raw[i:i+horizon])\n",
    "        starts.append(i)\n",
    "        prev_close.append(starts_prev_close[i])\n",
    "    \n",
    "    return (np.asarray(X, dtype=np.float32), np.asarray(y_s, dtype=np.float32),\n",
    "            np.asarray(y_r, dtype=np.float32), np.asarray(starts, dtype=np.int64),\n",
    "            np.asarray(prev_close, dtype=np.float32), dropped_target_imputed, dropped_target_open_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepDataset(Dataset):\n",
    "    def __init__(self, X, y_s, y_r):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_s = torch.from_numpy(y_s).float()\n",
    "        self.y_r = torch.from_numpy(y_r).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_s[idx], self.y_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = build_walkforward_slices(price_df)\n",
    "print('Walk-forward slices:', slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v7.4 Model Definition: Transformer Encoder + Memory Attention + GRU Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for transformer.\"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [seq_len, batch_size, d_model]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolatileEventMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory attention mechanism that identifies and attends to past volatile events\n",
    "    in the lookback window. Helps decoder focus on significant market movements.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, memory_dim: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.memory_dim = memory_dim\n",
    "        \n",
    "        # Project encoder outputs to memory space\n",
    "        self.memory_proj = nn.Linear(d_model, memory_dim)\n",
    "        \n",
    "        # Volatility detection: projects to scalar volatility score\n",
    "        self.volatility_scorer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for memory access\n",
    "        self.query_proj = nn.Linear(d_model, memory_dim)\n",
    "        self.key_proj = nn.Linear(memory_dim, memory_dim)\n",
    "        self.value_proj = nn.Linear(memory_dim, memory_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(memory_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "        )\n",
    "    \n",
    "    def detect_volatile_events(self, enc_out: torch.Tensor, threshold_percentile: float = 0.9):\n",
    "        \"\"\"\n",
    "        Detect volatile events based on feature magnitude.\n",
    "        Returns: volatility_scores [batch, seq_len], volatile_mask [batch, seq_len]\n",
    "        \"\"\"\n",
    "        # Calculate volatility score for each timestep\n",
    "        volatility_scores = self.volatility_scorer(enc_out).squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        # Determine threshold per batch\n",
    "        thresholds = torch.quantile(volatility_scores, threshold_percentile, dim=1, keepdim=True)\n",
    "        volatile_mask = volatility_scores > thresholds\n",
    "        \n",
    "        return volatility_scores, volatile_mask\n",
    "    \n",
    "    def forward(self, decoder_hidden: torch.Tensor, enc_out: torch.Tensor, \n",
    "                threshold_percentile: float = 0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_hidden: [batch, d_model] - current decoder state\n",
    "            enc_out: [batch, seq_len, d_model] - transformer encoder outputs\n",
    "        Returns:\n",
    "            memory_context: [batch, d_model] - attended memory context\n",
    "            attention_weights: [batch, seq_len] - attention distribution over memory\n",
    "            volatile_mask: [batch, seq_len] - binary mask of volatile events\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = enc_out.shape\n",
    "        \n",
    "        # Detect volatile events\n",
    "        volatility_scores, volatile_mask = self.detect_volatile_events(enc_out, threshold_percentile)\n",
    "        \n",
    "        # Project encoder outputs to memory space\n",
    "        memory = self.memory_proj(enc_out)  # [batch, seq_len, memory_dim]\n",
    "        \n",
    "        # Compute attention from decoder to memory\n",
    "        query = self.query_proj(decoder_hidden).unsqueeze(2)  # [batch, memory_dim, 1]\n",
    "        keys = self.key_proj(memory)  # [batch, seq_len, memory_dim]\n",
    "        values = self.value_proj(memory)  # [batch, seq_len, memory_dim]\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.bmm(keys, query).squeeze(2) / math.sqrt(self.memory_dim)  # [batch, seq_len]\n",
    "        \n",
    "        # Boost scores for volatile events\n",
    "        volatility_boost = volatile_mask.float() * 2.0  # Boost volatile events\n",
    "        scores = scores + volatility_boost\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # [batch, seq_len]\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), values).squeeze(1)  # [batch, memory_dim]\n",
    "        \n",
    "        # Project back to d_model\n",
    "        memory_context = self.output_proj(context)  # [batch, d_model]\n",
    "        \n",
    "        return memory_context, attention_weights, volatile_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMemoryAttnModel(nn.Module):\n",
    "    \"\"\"\n",
    "    v7.4: Transformer Encoder + Memory Attention + GRU Decoder\n",
    "    \n",
    "    - Transformer encoder (4 heads, 2 layers, d_model=256) processes long lookback\n",
    "    - Positional encoding for sequence order\n",
    "    - Memory attention: decoder attends to past volatile events\n",
    "    - GRU decoder for autoregressive generation\n",
    "    - Probabilistic outputs (mu + sigma) with temperature control\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, d_model: int = 256, \n",
    "                 num_heads: int = 4, num_encoder_layers: int = 2, num_decoder_layers: int = 2,\n",
    "                 dropout: float = 0.2, horizon: int = 15, memory_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection to d_model\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=2048, dropout=dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,  # PyTorch transformer uses [seq, batch, features] by default\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Memory attention mechanism\n",
    "        self.memory_attention = VolatileEventMemory(d_model, memory_dim)\n",
    "        \n",
    "        # Decoder: GRU with memory context\n",
    "        self.decoder_init = nn.Linear(d_model, num_decoder_layers * d_model)\n",
    "        self.decoder_cell = nn.GRUCell(output_size + d_model * 2, d_model)  # +d_model*2 for context + memory\n",
    "        self.decoder_hidden_proj = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_model) for _ in range(num_decoder_layers - 1)\n",
    "        ])\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        \n",
    "        # Standard attention for encoder-decoder\n",
    "        self.attn_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Output heads: mu and log_sigma for each OHLC\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),  # *3 for h_dec + context + memory\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, output_size),\n",
    "        )\n",
    "        \n",
    "        # Initialize sigma head for moderate volatility\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].weight)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].bias)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode input sequence with transformer.\n",
    "        x: [batch, seq_len, input_size]\n",
    "        Returns: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Project input\n",
    "        x = self.input_proj(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Transpose for transformer: [seq_len, batch, d_model]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        enc_out = self.transformer_encoder(x)  # [seq_len, batch, d_model]\n",
    "        \n",
    "        # Transpose back: [batch, seq_len, d_model]\n",
    "        enc_out = enc_out.transpose(0, 1)\n",
    "        \n",
    "        return enc_out\n",
    "    \n",
    "    def _attend(self, h_dec: torch.Tensor, enc_out: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Standard encoder-decoder attention.\"\"\"\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)  # [batch, d_model, 1]\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2) / math.sqrt(self.d_model)  # [batch, seq_len]\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)  # [batch, d_model]\n",
    "        return context\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, y_teacher: torch.Tensor = None, \n",
    "                teacher_forcing_ratio: float = 0.0, return_sigma: bool = False, \n",
    "                return_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass with optional teacher forcing.\n",
    "        x: [batch, seq_len, input_size]\n",
    "        y_teacher: [batch, horizon, output_size] - ground truth for teacher forcing\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Encode\n",
    "        enc_out = self.encode(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Initialize decoder hidden state from encoded sequence\n",
    "        pooled = enc_out.mean(dim=1)  # [batch, d_model]\n",
    "        h_dec = self.decoder_init(pooled).view(batch_size, self.num_decoder_layers, self.d_model)\n",
    "        h_dec = h_dec[:, 0, :]  # Use first layer's init\n",
    "        \n",
    "        # Initial decoder input: last timestep OHLC returns from input\n",
    "        dec_input = x[:, -1, :self.output_size]\n",
    "        \n",
    "        mu_seq, sigma_seq, attn_weights_list = [], [], []\n",
    "        \n",
    "        for t in range(self.horizon):\n",
    "            # Standard attention\n",
    "            context = self._attend(h_dec, enc_out)\n",
    "            \n",
    "            # Memory attention to volatile events\n",
    "            memory_context, attn_weights, volatile_mask = self.memory_attention(h_dec, enc_out)\n",
    "            attn_weights_list.append(attn_weights)\n",
    "            \n",
    "            # Combine contexts\n",
    "            combined_context = torch.cat([context, memory_context], dim=1)\n",
    "            \n",
    "            # Decoder cell\n",
    "            cell_input = torch.cat([dec_input, combined_context], dim=1)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "            \n",
    "            # Output\n",
    "            out_features = torch.cat([h_dec, context, memory_context], dim=1)\n",
    "            \n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = self.log_sigma_head(out_features)\n",
    "            \n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "            \n",
    "            # Teacher forcing or autoregressive\n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    noise = torch.randn_like(mu) * torch.exp(log_sigma).detach()\n",
    "                    dec_input = mu + noise\n",
    "            else:\n",
    "                dec_input = mu\n",
    "        \n",
    "        mu_out = torch.cat(mu_seq, dim=1)  # [batch, horizon, output_size]\n",
    "        sigma_out = torch.cat(sigma_seq, dim=1)\n",
    "        \n",
    "        outputs = (mu_out, sigma_out) if return_sigma else (mu_out,)\n",
    "        if return_attention:\n",
    "            outputs += (attn_weights_list,)\n",
    "        return outputs[0] if len(outputs) == 1 else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate_realistic(self, x: torch.Tensor, temperature: float = 1.0, \n",
    "                          historical_vol: float = None, return_attention: bool = False):\n",
    "        \"\"\"\n",
    "        Generate realistic price paths with controlled stochasticity.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, input_size]\n",
    "            temperature: controls volatility (>1.0 = more wild)\n",
    "            historical_vol: if provided, scale noise to match this volatility\n",
    "            return_attention: if True, also return attention weights\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_size = x.size(0)\n",
    "            \n",
    "            # Encode\n",
    "            enc_out = self.encode(x)\n",
    "            \n",
    "            # Initialize decoder\n",
    "            pooled = enc_out.mean(dim=1)\n",
    "            h_dec = self.decoder_init(pooled).view(batch_size, self.num_decoder_layers, self.d_model)\n",
    "            h_dec = h_dec[:, 0, :]\n",
    "            \n",
    "            dec_input = x[:, -1, :self.output_size]\n",
    "            \n",
    "            generated = []\n",
    "            attention_history = []\n",
    "            \n",
    "            for t in range(self.horizon):\n",
    "                # Standard attention\n",
    "                context = self._attend(h_dec, enc_out)\n",
    "                \n",
    "                # Memory attention\n",
    "                memory_context, attn_weights, volatile_mask = self.memory_attention(h_dec, enc_out)\n",
    "                attention_history.append({\n",
    "                    'weights': attn_weights.cpu(),\n",
    "                    'volatile_mask': volatile_mask.cpu(),\n",
    "                })\n",
    "                \n",
    "                # Combine contexts\n",
    "                combined_context = torch.cat([context, memory_context], dim=1)\n",
    "                \n",
    "                # Decoder\n",
    "                cell_input = torch.cat([dec_input, combined_context], dim=1)\n",
    "                h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "                \n",
    "                # Output\n",
    "                out_features = torch.cat([h_dec, context, memory_context], dim=1)\n",
    "                mu = self.mu_head(out_features)\n",
    "                log_sigma = self.log_sigma_head(out_features)\n",
    "                \n",
    "                # Scale sigma by temperature\n",
    "                sigma = torch.exp(log_sigma) * temperature\n",
    "                \n",
    "                # Optional: override with historical volatility for first few steps\n",
    "                if historical_vol is not None and t < 5:\n",
    "                    sigma = torch.ones_like(sigma) * historical_vol\n",
    "                \n",
    "                # Ensure minimum volatility\n",
    "                sigma = torch.maximum(sigma, torch.tensor(MIN_PREDICTED_VOL))\n",
    "                \n",
    "                # Sample from distribution\n",
    "                noise = torch.randn_like(mu) * sigma\n",
    "                sample = mu + noise\n",
    "                \n",
    "                generated.append(sample.unsqueeze(1))\n",
    "                dec_input = sample  # Autoregressive feedback\n",
    "            \n",
    "            result = torch.cat(generated, dim=1)\n",
    "            if return_attention:\n",
    "                return result, attention_history\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(mu, log_sigma, target):\n",
    "    \"\"\"Negative log-likelihood for Gaussian\"\"\"\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    nll = 0.5 * ((target - mu) / sigma) ** 2 + log_sigma + 0.5 * np.log(2 * np.pi)\n",
    "    return nll.mean()\n",
    "\n",
    "def candle_range_loss(mu, target):\n",
    "    pred_range = mu[:, :, 1] - mu[:, :, 2]  # High - Low\n",
    "    actual_range = target[:, :, 1] - target[:, :, 2]\n",
    "    return ((pred_range - actual_range) ** 2).mean()\n",
    "\n",
    "def volatility_match_loss(log_sigma, target):\n",
    "    \"\"\"Encourage predicted uncertainty to match actual error magnitude\"\"\"\n",
    "    pred_vol = torch.exp(log_sigma).mean()\n",
    "    actual_vol = target.std()\n",
    "    return (pred_vol - actual_vol) ** 2\n",
    "\n",
    "def directional_penalty(mu, target):\n",
    "    pred_close = mu[:, :, 3]\n",
    "    actual_close = target[:, :, 3]\n",
    "    sign_match = torch.sign(pred_close) * torch.sign(actual_close)\n",
    "    penalty = torch.clamp(-sign_match, min=0.0)\n",
    "    return penalty.mean()\n",
    "\n",
    "def memory_attention_loss(attn_weights_list, horizon):\n",
    "    \"\"\"\n",
    "    v7.4: Encourage model to use memory attention effectively.\n",
    "    Penalize uniform attention (encourage focus on specific events).\n",
    "    \"\"\"\n",
    "    if not attn_weights_list:\n",
    "        return torch.tensor(0.0, device=DEVICE)\n",
    "    \n",
    "    total_entropy = 0.0\n",
    "    for attn in attn_weights_list:\n",
    "        # Entropy: -sum(p * log(p)) - lower entropy means sharper focus\n",
    "        entropy = -(attn * torch.log(attn + 1e-8)).sum(dim=1).mean()\n",
    "        total_entropy += entropy\n",
    "    \n",
    "    # We want to minimize entropy (sharper focus), so return entropy\n",
    "    return total_entropy / len(attn_weights_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ratio_for_epoch(epoch):\n",
    "    ratio = TF_START * (TF_DECAY_RATE ** (epoch - 1))\n",
    "    return max(float(TF_END), float(ratio))\n",
    "\n",
    "def run_epoch(model, loader, step_weights_t, optimizer=None, tf_ratio=0.0):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    total_loss, nll_total, range_total, vol_total, dir_total, mem_total = 0, 0, 0, 0, 0, 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for xb, yb_s, yb_r in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb_s = yb_s.to(DEVICE)\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            # v7.4: Get mu, sigma, and attention weights\n",
    "            mu, sigma_out, attn_list = model(\n",
    "                xb, \n",
    "                y_teacher=yb_s if is_train else None, \n",
    "                teacher_forcing_ratio=tf_ratio if is_train else 0.0, \n",
    "                return_sigma=True,\n",
    "                return_attention=True\n",
    "            )\n",
    "            \n",
    "            # Weighted losses\n",
    "            nll = (nll_loss(mu, sigma_out, yb_s) * step_weights_t).mean()\n",
    "            rng = candle_range_loss(mu, yb_s)\n",
    "            vol = volatility_match_loss(sigma_out, yb_s)\n",
    "            dir_pen = directional_penalty(mu, yb_s)\n",
    "            mem_loss = memory_attention_loss(attn_list, HORIZON)\n",
    "            \n",
    "            loss = (nll + RANGE_LOSS_WEIGHT * rng + VOLATILITY_WEIGHT * vol + \n",
    "                    DIR_PENALTY_WEIGHT * dir_pen + MEMORY_ATTENTION_WEIGHT * mem_loss)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        nll_total += nll.item() * bs\n",
    "        range_total += rng.item() * bs\n",
    "        vol_total += vol.item() * bs\n",
    "        dir_total += dir_pen.item() * bs\n",
    "        mem_total += mem_loss.item() * bs\n",
    "        n_items += bs\n",
    "        \n",
    "    return {\n",
    "        'total': total_loss / max(n_items, 1),\n",
    "        'nll': nll_total / max(n_items, 1),\n",
    "        'range': range_total / max(n_items, 1),\n",
    "        'vol': vol_total / max(n_items, 1),\n",
    "        'dir': dir_total / max(n_items, 1),\n",
    "        'mem': mem_total / max(n_items, 1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, max_epochs, patience):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    wait = 0\n",
    "    rows = []\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tf = tf_ratio_for_epoch(epoch)\n",
    "        tr = run_epoch(model, train_loader, step_weights_t, optimizer=optimizer, tf_ratio=tf)\n",
    "        va = run_epoch(model, val_loader, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "        \n",
    "        scheduler.step(va['total'])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        rows.append({\n",
    "            'epoch': epoch, 'tf_ratio': tf, 'lr': lr,\n",
    "            'train_total': tr['total'], 'val_total': va['total'],\n",
    "            'train_nll': tr['nll'], 'val_nll': va['nll'],\n",
    "            'train_range': tr['range'], 'val_range': va['range'],\n",
    "            'train_mem': tr['mem'], 'val_mem': va['mem'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
    "              f\"train={tr['total']:.6f} (nll={tr['nll']:.6f}, mem={tr['mem']:.4f}) | \"\n",
    "              f\"val={va['total']:.6f} (nll={va['nll']:.6f}) | lr={lr:.6g}\")\n",
    "        \n",
    "        if va['total'] < best_val:\n",
    "            best_val = va['total']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}.')\n",
    "                break\n",
    "                \n",
    "    model.load_state_dict(best_state)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(actual_ohlc, pred_ohlc, prev_close):\n",
    "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
    "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
    "    ac, pc = actual_ohlc[:, 3], pred_ohlc[:, 3]\n",
    "    \n",
    "    # Directional accuracy vs persistence baseline\n",
    "    actual_direction = np.sign(ac - prev_close)\n",
    "    pred_direction = np.sign(pc - prev_close)\n",
    "    dir_acc = float(np.mean(actual_direction == pred_direction))\n",
    "    \n",
    "    # Bias: average predicted close vs actual close\n",
    "    bias = float(np.mean(pc - ac))\n",
    "    \n",
    "    return {\n",
    "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
    "        'close_rmse': float(np.sqrt(np.mean((ac - pc) ** 2))),\n",
    "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
    "        'directional_accuracy': dir_acc,\n",
    "        'directional_accuracy_eps': float(np.mean(np.sign(ac - prev_close) == np.sign(pc - prev_close))),\n",
    "        'bias': bias,\n",
    "        'avg_pred_close': float(np.mean(pc)),\n",
    "        'avg_actual_close': float(np.mean(ac)),\n",
    "    }\n",
    "\n",
    "def evaluate_baselines(actual_ohlc, prev_ohlc, prev_close):\n",
    "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
    "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
    "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
    "    return {'persistence': persistence, 'flat_close_rw': flat_rw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_realistic_recursive(model, X, context_prices, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate realistic predictions using autoregressive sampling with memory attention.\n",
    "    context_prices: last LOOKBACK prices to calculate realized vol for scaling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate realized volatility from context for scaling\n",
    "    log_returns = np.log(context_prices[1:] / context_prices[:-1])\n",
    "    historical_vol = float(np.std(log_returns)) if len(log_returns) > 1 else 0.001\n",
    "    \n",
    "    print(f\"Historical realized vol: {historical_vol:.6f}, Temperature: {temperature}\")\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X).float().to(DEVICE)\n",
    "    \n",
    "    # Generate with memory attention tracking\n",
    "    generated, attention_history = model.generate_realistic(\n",
    "        X_tensor, temperature=temperature, \n",
    "        historical_vol=historical_vol,\n",
    "        return_attention=True\n",
    "    )\n",
    "    \n",
    "    # Print memory attention stats\n",
    "    avg_volatile_attention = 0\n",
    "    for attn_info in attention_history:\n",
    "        weights = attn_info['weights']\n",
    "        volatile = attn_info['volatile_mask']\n",
    "        avg_volatile_attention += (weights * volatile.float()).sum(dim=1).mean().item()\n",
    "    avg_volatile_attention /= len(attention_history)\n",
    "    print(f\"Average attention to volatile events: {avg_volatile_attention:.4f}\")\n",
    "    \n",
    "    return generated.detach().cpu().numpy()[0], attention_history  # [horizon, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(fold_name, price_fold, window, max_epochs, patience, run_sanity=False, quick_mode=False):\n",
    "    feat_fold = build_feature_frame(price_fold)\n",
    "    target_fold = build_target_frame(feat_fold)\n",
    "    \n",
    "    input_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "    target_raw = target_fold[TARGET_COLS].to_numpy(np.float32)\n",
    "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "    row_open_skip = feat_fold['row_open_skip'].to_numpy(np.int8).astype(bool)\n",
    "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
    "    price_vals = price_fold.loc[feat_fold.index, OHLC_COLS].to_numpy(np.float32)\n",
    "    \n",
    "    tr_end, va_end = split_points(len(input_raw))\n",
    "    \n",
    "    # Standardize inputs only\n",
    "    in_mean, in_std = input_raw[:tr_end].mean(axis=0), input_raw[:tr_end].std(axis=0)\n",
    "    in_std = np.where(in_std < 1e-8, 1.0, in_std)\n",
    "    input_scaled = (input_raw - in_mean) / in_std\n",
    "    \n",
    "    # No target scaling (raw returns)\n",
    "    tg_mean, tg_std = np.zeros(4, dtype=np.float32), np.ones(4, dtype=np.float32)\n",
    "    target_scaled = target_raw.copy()\n",
    "    \n",
    "    X_all, y_all_s, y_all_r, starts, prev_close_starts, dropped_imputed, dropped_skip = make_multistep_windows(\n",
    "        input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, prev_close, window, HORIZON\n",
    "    )\n",
    "    \n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: no windows available.')\n",
    "    \n",
    "    # Splits\n",
    "    end_idx = starts + HORIZON - 1\n",
    "    tr_m, va_m, te_m = end_idx < tr_end, (end_idx >= tr_end) & (end_idx < va_end), end_idx >= va_end\n",
    "    \n",
    "    X_train, y_train_s, y_train_r = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m]\n",
    "    X_val, y_val_s, y_val_r = X_all[va_m], y_all_s[va_m], y_all_r[va_m]\n",
    "    X_test, y_test_s, y_test_r = X_all[te_m], y_all_s[te_m], y_all_r[te_m]\n",
    "    test_starts = starts[te_m]\n",
    "    test_prev_close = prev_close_starts[te_m]\n",
    "    \n",
    "    print(f'Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}')\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), \n",
    "                             batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), \n",
    "                           batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # v7.4: Transformer + Memory Attention Model\n",
    "    model = TransformerMemoryAttnModel(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=len(TARGET_COLS),\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "        memory_dim=MEMORY_ATTENTION_DIM,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    hist = train_model(model, train_loader, val_loader, max_epochs, patience)\n",
    "    \n",
    "    # REALISTIC PREDICTION (autoregressive with noise and memory attention)\n",
    "    # Pick last test sample for visualization\n",
    "    last_idx = len(X_test) - 1\n",
    "    X_last = X_test[last_idx:last_idx+1]\n",
    "    context_start = int(test_starts[last_idx]) - window\n",
    "    context_prices = price_vals[context_start:int(test_starts[last_idx]), 3]  # Close prices\n",
    "    \n",
    "    # Generate with temperature > 1 for more realistic \"wild\" market movement\n",
    "    pred_rets_realistic, attn_history = predict_realistic_recursive(\n",
    "        model, X_last, context_prices, temperature=SAMPLING_TEMPERATURE\n",
    "    )\n",
    "    \n",
    "    # Convert to prices\n",
    "    last_close = float(test_prev_close[last_idx])\n",
    "    pred_price_realistic = returns_to_prices_seq(pred_rets_realistic, last_close)\n",
    "    \n",
    "    # Actual future\n",
    "    actual_future = price_vals[int(test_starts[last_idx]):int(test_starts[last_idx])+HORIZON]\n",
    "    \n",
    "    # One-step metrics (deterministic for comparison)\n",
    "    mu_test = model(torch.from_numpy(X_test).float().to(DEVICE))[0].detach().cpu().numpy()\n",
    "    pred_step1_ret = mu_test[:, 0, :]\n",
    "    actual_step1_ret = y_test_r[:, 0, :]\n",
    "    \n",
    "    # Simple price reconstruction for metrics\n",
    "    pred_ohlc_1 = np.zeros((len(test_starts), 4))\n",
    "    for i in range(len(test_starts)):\n",
    "        pc = test_prev_close[i]\n",
    "        pred_ohlc_1[i] = [\n",
    "            pc * np.exp(pred_step1_ret[i, 0]),\n",
    "            pc * np.exp(pred_step1_ret[i, 1]),\n",
    "            pc * np.exp(pred_step1_ret[i, 2]),\n",
    "            pc * np.exp(pred_step1_ret[i, 3]),\n",
    "        ]\n",
    "        pred_ohlc_1[i] = enforce_candle_validity(pred_ohlc_1[i].reshape(1, -1))[0]\n",
    "    \n",
    "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
    "    prev_ohlc = price_vals[test_starts]\n",
    "    \n",
    "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
    "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
    "    \n",
    "    print(f\"\\nRealistic prediction stats:\")\n",
    "    print(f\"  Pred range: [{pred_price_realistic[:, 3].min():.2f}, {pred_price_realistic[:, 3].max():.2f}]\")\n",
    "    print(f\"  Actual range: [{actual_future[:, 3].min():.2f}, {actual_future[:, 3].max():.2f}]\")\n",
    "    print(f\"  Pred volatility: {np.std(pred_rets_realistic[:, 3]):.6f}\")\n",
    "    print(f\"  Actual volatility: {np.std(actual_step1_ret[:, 3]):.6f}\")\n",
    "    \n",
    "    # Build DataFrames for plotting\n",
    "    future_idx = price_fold.index[test_starts[last_idx]:test_starts[last_idx]+HORIZON]\n",
    "    pred_future_df = pd.DataFrame(pred_price_realistic, index=future_idx, columns=OHLC_COLS)\n",
    "    actual_future_df = pd.DataFrame(actual_future, index=future_idx, columns=OHLC_COLS)\n",
    "    context_df = price_fold.iloc[test_starts[last_idx]-window:test_starts[last_idx]+1][OHLC_COLS]\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_name,\n",
    "        'window': window,\n",
    "        'history_df': hist,\n",
    "        'model_metrics': model_metrics,\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'context_df': context_df,\n",
    "        'actual_future_df': actual_future_df,\n",
    "        'pred_future_df': pred_future_df,\n",
    "        'attention_history': attn_history,\n",
    "        'samples': {'train': len(X_train), 'val': len(X_val), 'test': len(X_test)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lookback sweep if enabled\n",
    "fold_results = []\n",
    "primary_slice = slices[0]\n",
    "selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "if ENABLE_LOOKBACK_SWEEP:\n",
    "    print('\\n=== Lookback sweep (v7.4: Extended lookback candidates) ===')\n",
    "    _, a0, b0 = primary_slice\n",
    "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for w in LOOKBACK_CANDIDATES:\n",
    "        print(f'\\nSweep candidate lookback={w} --')\n",
    "        try:\n",
    "            r = run_fold(f'sweep_w{w}', fold_price0, w, SWEEP_MAX_EPOCHS, SWEEP_PATIENCE, quick_mode=True)\n",
    "            score = -r['model_metrics']['close_mae']  # Simple selection\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                selected_window = w\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for window {w}: {e}\")\n",
    "\n",
    "print(f'\\nSelected lookback: {selected_window}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full walk-forward with realistic generation\n",
    "print('\\n=== Full walk-forward with Transformer + Memory Attention ===')\n",
    "for i, (name, a, b) in enumerate(slices, start=1):\n",
    "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
    "    fold_price = price_df.iloc[a:b].copy()\n",
    "    try:\n",
    "        res = run_fold(name, fold_price, selected_window, FINAL_MAX_EPOCHS, FINAL_PATIENCE)\n",
    "        fold_results.append(res)\n",
    "        \n",
    "        print(f\"\\nResults for {name}:\")\n",
    "        print(f\"  Model MAE: {res['model_metrics']['close_mae']:.4f}\")\n",
    "        print(f\"  Persistence MAE: {res['baseline_metrics']['persistence']['close_mae']:.4f}\")\n",
    "        print(f\"  Directional Accuracy: {res['model_metrics']['directional_accuracy']:.2%}\")\n",
    "        print(f\"  Bias: {res['model_metrics']['bias']:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8), facecolor='black')\n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    def draw_candles(ax, ohlc, start_x, up_edge, up_face, down_edge, down_face, wick_color, width=0.6, alpha=1.0):\n",
    "        vals = ohlc[OHLC_COLS].to_numpy()\n",
    "        for i, (o, h, l, c) in enumerate(vals):\n",
    "            x = start_x + i\n",
    "            bull = c >= o\n",
    "            ax.vlines(x, l, h, color=wick_color, linewidth=1.0, alpha=alpha, zorder=2)\n",
    "            lower = min(o, c)\n",
    "            height = max(abs(c - o), 1e-6)\n",
    "            rect = Rectangle((x - width/2, lower), width, height,\n",
    "                           facecolor=up_face if bull else down_face,\n",
    "                           edgecolor=up_edge if bull else down_edge,\n",
    "                           linewidth=1.0, alpha=alpha, zorder=3)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    context_df = latest['context_df']\n",
    "    actual_future_df = latest['actual_future_df']\n",
    "    pred_future_df = latest['pred_future_df']\n",
    "    \n",
    "    # Draw history (green/red)\n",
    "    draw_candles(ax, context_df, 0, '#00FF00', '#00FF00', '#FF0000', '#FF0000', '#FFFFFF', width=0.6, alpha=0.9)\n",
    "    \n",
    "    # Draw actual future (dimmed)\n",
    "    draw_candles(ax, actual_future_df, len(context_df), '#00AA00', '#00AA00', '#AA0000', '#AA0000', '#888888', \n",
    "                 width=0.6, alpha=0.6)\n",
    "    \n",
    "    # Draw realistic prediction (bright white/black with glow effect)\n",
    "    draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#888888', '#000000', '#FFFFFF',\n",
    "                 width=0.5, alpha=1.0)\n",
    "    \n",
    "    ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=1.0, alpha=0.8)\n",
    "    \n",
    "    # Labels\n",
    "    n = len(context_df) + len(actual_future_df)\n",
    "    step = max(1, n // 12)\n",
    "    ticks = list(range(0, n, step))\n",
    "    all_idx = context_df.index.append(actual_future_df.index)\n",
    "    labels = [all_idx[i].strftime('%m-%d %H:%M') for i in ticks if i < len(all_idx)]\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=30, ha='right', color='white', fontsize=9)\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_color('#666666')\n",
    "    ax.grid(color='#333333', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(f'MSFT 1m Transformer+Memory ({latest[\"fold\"]}) - Realistic Forecast (Temp={SAMPLING_TEMPERATURE})', \n",
    "                 color='white', fontsize=14, pad=15)\n",
    "    ax.set_ylabel('Price', color='white', fontsize=12)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#00FF00', edgecolor='#00FF00', label='History (bull)'),\n",
    "        Patch(facecolor='#FF0000', edgecolor='#FF0000', label='History (bear)'),\n",
    "        Patch(facecolor='#00AA00', edgecolor='#00AA00', label='Actual Future (dim)'),\n",
    "        Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted (bull)'),\n",
    "        Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted (bear)'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, facecolor='black', edgecolor='white', labelcolor='white', \n",
    "             loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal chart shows realistic candle generation with temperature {SAMPLING_TEMPERATURE}\")\n",
    "    print(f\"Transformer encoder with {NUM_HEADS} heads, {NUM_ENCODER_LAYERS} layers, d_model={D_MODEL}\")\n",
    "    print(f\"Memory attention tracking volatile events in {selected_window}-bar lookback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell: Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test cell for v7.4 Transformer + Long Memory model\n",
    "\n",
    "Validates:\n",
    "1. Directional accuracy vs persistence baseline\n",
    "2. Average predicted close vs actual close (bias)\n",
    "3. Visual confirmation of realistic candle wicks/bodies\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL VALIDATION TESTS - v7.4 Transformer + Long Memory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not fold_results:\n",
    "    print(\"WARNING: No fold results available. Run training first.\")\n",
    "else:\n",
    "    latest = fold_results[-1]\n",
    "    metrics = latest['model_metrics']\n",
    "    baseline = latest['baseline_metrics']['persistence']\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Lookback window: {latest['window']} bars\")\n",
    "    print(f\"  Transformer: {NUM_HEADS} heads, {NUM_ENCODER_LAYERS} layers, d_model={D_MODEL}\")\n",
    "    print(f\"  Sampling temperature: {SAMPLING_TEMPERATURE}\")\n",
    "    \n",
    "    # Test 1: Directional Accuracy vs Persistence\n",
    "    print(f\"\\n--- Test 1: Directional Accuracy ---\")\n",
    "    model_dir_acc = metrics['directional_accuracy']\n",
    "    persist_dir_acc = baseline['directional_accuracy']\n",
    "    dir_improvement = model_dir_acc - persist_dir_acc\n",
    "    \n",
    "    print(f\"  Model directional accuracy: {model_dir_acc:.2%}\")\n",
    "    print(f\"  Persistence baseline:       {persist_dir_acc:.2%}\")\n",
    "    print(f\"  Improvement:                {dir_improvement:+.2%}\")\n",
    "    \n",
    "    if dir_improvement >= 0:\n",
    "        print(f\"   PASS: Model meets or exceeds persistence baseline\")\n",
    "    else:\n",
    "        print(f\"   FAIL: Model below persistence baseline by {abs(dir_improvement):.2%}\")\n",
    "    \n",
    "    # Test 2: Bias Check\n",
    "    print(f\"\\n--- Test 2: Bias (Average Predicted vs Actual Close) ---\")\n",
    "    bias = metrics['bias']\n",
    "    avg_pred = metrics['avg_pred_close']\n",
    "    avg_actual = metrics['avg_actual_close']\n",
    "    bias_pct = abs(bias) / avg_actual * 100 if avg_actual > 0 else 0\n",
    "    \n",
    "    print(f\"  Average predicted close: ${avg_pred:.2f}\")\n",
    "    print(f\"  Average actual close:    ${avg_actual:.2f}\")\n",
    "    print(f\"  Bias:                    ${bias:.4f} ({bias_pct:.3f}%)\")\n",
    "    \n",
    "    if bias_pct < 1.0:\n",
    "        print(f\"   PASS: Bias less than 1%\")\n",
    "    elif bias_pct < 5.0:\n",
    "        print(f\"   WARNING: Bias between 1-5%\")\n",
    "    else:\n",
    "        print(f\"   FAIL: Bias exceeds 5%\")\n",
    "    \n",
    "    # Test 3: Candle Validity Check\n",
    "    print(f\"\\n--- Test 3: Candle Validity (Realistic Wicks/Bodies) ---\")\n",
    "    pred_df = latest['pred_future_df']\n",
    "    o, h, l, c = pred_df['Open'], pred_df['High'], pred_df['Low'], pred_df['Close']\n",
    "    \n",
    "    # Check High >= max(Open, Close)\n",
    "    high_valid = (h >= np.maximum(o, c)).all()\n",
    "    # Check Low <= min(Open, Close)\n",
    "    low_valid = (l <= np.minimum(o, c)).all()\n",
    "    # Check High >= Low\n",
    "    range_valid = (h >= l).all()\n",
    "    \n",
    "    print(f\"  High >= max(Open, Close): {high_valid}\")\n",
    "    print(f\"  Low <= min(Open, Close):  {low_valid}\")\n",
    "    print(f\"  High >= Low:              {range_valid}\")\n",
    "    \n",
    "    # Check for realistic wicks (shadows)\n",
    "    upper_wicks = h - np.maximum(o, c)\n",
    "    lower_wicks = np.minimum(o, c) - l\n",
    "    bodies = abs(c - o)\n",
    "    \n",
    "    print(f\"\\n  Wick Statistics:\")\n",
    "    print(f\"    Average upper wick: ${upper_wicks.mean():.3f}\")\n",
    "    print(f\"    Average lower wick: ${lower_wicks.mean():.3f}\")\n",
    "    print(f\"    Average body:       ${bodies.mean():.3f}\")\n",
    "    print(f\"    Max upper wick:     ${upper_wicks.max():.3f}\")\n",
    "    print(f\"    Max lower wick:     ${lower_wicks.max():.3f}\")\n",
    "    \n",
    "    if high_valid and low_valid and range_valid:\n",
    "        print(f\"   PASS: All candles are valid OHLC\")\n",
    "    else:\n",
    "        print(f\"   FAIL: Invalid candles detected\")\n",
    "    \n",
    "    # Test 4: Memory Attention Validation\n",
    "    if 'attention_history' in latest and latest['attention_history']:\n",
    "        print(f\"\\n--- Test 4: Memory Attention Analysis ---\")\n",
    "        attn_history = latest['attention_history']\n",
    "        \n",
    "        total_attention_to_volatile = 0\n",
    "        total_volatile_timesteps = 0\n",
    "        \n",
    "        for step, attn_info in enumerate(attn_history):\n",
    "            weights = attn_info['weights']\n",
    "            volatile = attn_info['volatile_mask']\n",
    "            \n",
    "            volatile_attention = (weights * volatile.float()).sum(dim=1).mean().item()\n",
    "            total_attention_to_volatile += volatile_attention\n",
    "            total_volatile_timesteps += volatile.float().sum().item() / volatile.numel()\n",
    "        \n",
    "        avg_volatile_attention = total_attention_to_volatile / len(attn_history)\n",
    "        avg_volatile_ratio = total_volatile_timesteps / len(attn_history)\n",
    "        \n",
    "        print(f\"  Average attention to volatile events: {avg_volatile_attention:.4f}\")\n",
    "        print(f\"  Average volatile timestep ratio:      {avg_volatile_ratio:.2%}\")\n",
    "        \n",
    "        if avg_volatile_attention > 0:\n",
    "            print(f\"   PASS: Memory attention is utilizing volatile event detection\")\n",
    "        else:\n",
    "            print(f\"   WARNING: Memory attention not showing volatile focus\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Model: Transformer + Long Memory (v7.4)\")\n",
    "    print(f\"Lookback: {latest['window']} bars | Horizon: {HORIZON} bars\")\n",
    "    print(f\"Close MAE: {metrics['close_mae']:.4f}\")\n",
    "    print(f\"Directional Accuracy: {model_dir_acc:.2%}\")\n",
    "    print(f\"Bias: ${bias:.4f} ({bias_pct:.3f}%)\")\n",
    "    print(f\"{'=' * 60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
