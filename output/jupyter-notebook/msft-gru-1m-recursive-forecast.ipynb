{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0_md",
   "metadata": {},
   "source": [
    "# Experiment: MSFT 1-Minute GRU Forecast (Direct Multi-Step v5)\n",
    "\n",
    "Key changes from v4:\n",
    "1. **No target standardization** \u2014 raw returns prevent zero-collapse\n",
    "2. **Scale-appropriate Huber** \u2014 delta=0.005 matched to 1e-3 return scale\n",
    "3. **Sigma floor in NLL** \u2014 prevents variance collapse\n",
    "4. **Directional penalty** \u2014 penalizes wrong sign prediction\n",
    "5. **More data** \u2014 120 lookback days (~75+ sessions)\n",
    "6. **Slower TF decay** \u2014 0.96^epoch keeps TF high longer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cabf09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required third-party packages are already installed.\n"
     ]
    }
   ],
   "source": [
    "# Optional dependency bootstrap\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    'alpaca': 'alpaca-py',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'pandas_market_calendars': 'pandas-market-calendars',\n",
    "}\n",
    "\n",
    "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
    "else:\n",
    "    print('All required third-party packages are already installed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e05fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "CUDA: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    print('CUDA:', torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3_md",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Tuned for direct multi-step forecasting v5: no target standardization, scale-appropriate losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SYMBOL = 'MSFT'\n",
    "LOOKBACK_DAYS = 120\n",
    "\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen',\n",
    "    'rHigh',\n",
    "    'rLow',\n",
    "    'rClose',\n",
    "    'logVolChange',\n",
    "    'logTradeCountChange',\n",
    "    'vwapDelta',\n",
    "    'rangeFrac',\n",
    "    'orderFlowProxy',\n",
    "    'tickPressure',\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
    "\n",
    "HORIZON = 15\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "LOOKBACK_CANDIDATES = [64, 96, 160, 256]\n",
    "DEFAULT_LOOKBACK = 96\n",
    "ENABLE_LOOKBACK_SWEEP = True\n",
    "SKIP_OPEN_BARS_TARGET = 6\n",
    "\n",
    "# Model capacity / regularization\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.15\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Training schedules\n",
    "SWEEP_MAX_EPOCHS = 12\n",
    "SWEEP_PATIENCE = 4\n",
    "FINAL_MAX_EPOCHS = 40\n",
    "FINAL_PATIENCE = 8\n",
    "TF_START = 1.0\n",
    "TF_END = 0.0\n",
    "TF_DECAY_RATE = 0.96  # slower exponential decay per epoch\n",
    "\n",
    "# Loss weights\n",
    "MU_HUBER_WEIGHT = 1.0  # equal weight to NLL\n",
    "DIR_PENALTY_WEIGHT = 0.10  # directional penalty\n",
    "HUBER_DELTA = 0.005  # tuned for return scale ~1e-3\n",
    "NLL_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)\n",
    "HUBER_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.0], dtype=np.float32)\n",
    "LOG_SIGMA_MAX = -2.0  # clamp sigma to reasonable range for raw returns\n",
    "STEP_LOSS_POWER = 0.5  # gentler horizon weighting\n",
    "\n",
    "# Target standardization -- OFF to prevent zero-collapse\n",
    "STANDARDIZE_TARGETS = False\n",
    "\n",
    "# Clipping\n",
    "APPLY_CLIPPING = True\n",
    "CLIP_QUANTILES = (0.005, 0.995)\n",
    "\n",
    "# Diagnostics\n",
    "DIRECTION_EPS = 0.02\n",
    "STD_RATIO_TARGET_MIN = 0.45\n",
    "\n",
    "# Session and feed\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.08\n",
    "\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'lookback_days': LOOKBACK_DAYS,\n",
    "    'lookback_candidates': LOOKBACK_CANDIDATES,\n",
    "    'horizon': HORIZON,\n",
    "    'feed': ALPACA_FEED,\n",
    "    'skip_open_bars_target': SKIP_OPEN_BARS_TARGET,\n",
    "    'max_session_fill_ratio': MAX_SESSION_FILL_RATIO,\n",
    "    'clip_quantiles': CLIP_QUANTILES,\n",
    "    'tf_decay_rate': TF_DECAY_RATE,\n",
    "    'standardize_targets': STANDARDIZE_TARGETS,\n",
    "    'huber_delta': HUBER_DELTA,\n",
    "    'device': str(DEVICE),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c962e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows from Alpaca: 15,107\n",
      "Sessionized rows kept: 14,640\n",
      "Alpaca API calls: 12 (<= 120/min target)\n",
      "Session meta: {'calendar_sessions_total': 41, 'kept_sessions': 38, 'dropped_empty_sessions': 2, 'dropped_high_fill_sessions': 1, 'avg_fill_ratio_kept': 0.006863312126470022, 'max_fill_ratio_kept': 0.0761904761904762, 'avg_session_minutes_kept': 385.2631578947368}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>TradeCount</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>is_imputed</th>\n",
       "      <th>session_id</th>\n",
       "      <th>bar_in_session</th>\n",
       "      <th>session_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-12-18 09:30:00</th>\n",
       "      <td>478.369995</td>\n",
       "      <td>479.700012</td>\n",
       "      <td>478.369995</td>\n",
       "      <td>479.549988</td>\n",
       "      <td>5056.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>478.753143</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-18 09:31:00</th>\n",
       "      <td>479.700012</td>\n",
       "      <td>480.010010</td>\n",
       "      <td>479.170013</td>\n",
       "      <td>479.170013</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>479.702515</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-18 09:32:00</th>\n",
       "      <td>479.429993</td>\n",
       "      <td>479.890015</td>\n",
       "      <td>479.429993</td>\n",
       "      <td>479.670013</td>\n",
       "      <td>3199.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>479.651733</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low       Close  Volume  \\\n",
       "2025-12-18 09:30:00  478.369995  479.700012  478.369995  479.549988  5056.0   \n",
       "2025-12-18 09:31:00  479.700012  480.010010  479.170013  479.170013  2555.0   \n",
       "2025-12-18 09:32:00  479.429993  479.890015  479.429993  479.670013  3199.0   \n",
       "\n",
       "                     TradeCount        VWAP  is_imputed  session_id  \\\n",
       "2025-12-18 09:30:00       122.0  478.753143           0           2   \n",
       "2025-12-18 09:31:00        80.0  479.702515           0           2   \n",
       "2025-12-18 09:32:00        62.0  479.651733           0           2   \n",
       "\n",
       "                     bar_in_session  session_len  \n",
       "2025-12-18 09:30:00               0          390  \n",
       "2025-12-18 09:31:00               1          390  \n",
       "2025-12-18 09:32:00               2          390  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>TradeCount</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>is_imputed</th>\n",
       "      <th>session_id</th>\n",
       "      <th>bar_in_session</th>\n",
       "      <th>session_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026-02-13 15:57:00</th>\n",
       "      <td>400.940002</td>\n",
       "      <td>401.350006</td>\n",
       "      <td>400.820007</td>\n",
       "      <td>401.234985</td>\n",
       "      <td>5515.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>401.025391</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>387</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-13 15:58:00</th>\n",
       "      <td>401.209991</td>\n",
       "      <td>401.244995</td>\n",
       "      <td>401.024994</td>\n",
       "      <td>401.165009</td>\n",
       "      <td>8393.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>401.135498</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>388</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-13 15:59:00</th>\n",
       "      <td>401.230011</td>\n",
       "      <td>401.339996</td>\n",
       "      <td>401.070007</td>\n",
       "      <td>401.179993</td>\n",
       "      <td>13429.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>401.196259</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>389</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Open        High         Low       Close   Volume  \\\n",
       "2026-02-13 15:57:00  400.940002  401.350006  400.820007  401.234985   5515.0   \n",
       "2026-02-13 15:58:00  401.209991  401.244995  401.024994  401.165009   8393.0   \n",
       "2026-02-13 15:59:00  401.230011  401.339996  401.070007  401.179993  13429.0   \n",
       "\n",
       "                     TradeCount        VWAP  is_imputed  session_id  \\\n",
       "2026-02-13 15:57:00       192.0  401.025391           0          40   \n",
       "2026-02-13 15:58:00       201.0  401.135498           0          40   \n",
       "2026-02-13 15:59:00       320.0  401.196259           0          40   \n",
       "\n",
       "                     bar_in_session  session_len  \n",
       "2026-02-13 15:57:00             387          390  \n",
       "2026-02-13 15:58:00             388          390  \n",
       "2026-02-13 15:59:00             389          390  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data pull + calendar-aware sessionization\n",
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be > 0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()\n",
    "\n",
    "\n",
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]\n",
    "\n",
    "\n",
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "\n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "\n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "\n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(\n",
    "                columns={\n",
    "                    'timestamp': 'Datetime',\n",
    "                    'open': 'Open',\n",
    "                    'high': 'High',\n",
    "                    'low': 'Low',\n",
    "                    'close': 'Close',\n",
    "                    'volume': 'Volume',\n",
    "                    'trade_count': 'TradeCount',\n",
    "                    'vwap': 'VWAP',\n",
    "                }\n",
    "            )\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "\n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            missing = [c for c in need if c not in d.columns]\n",
    "            if missing:\n",
    "                raise RuntimeError(f'Alpaca response missing columns: {missing}')\n",
    "\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "\n",
    "        cursor = chunk_end\n",
    "\n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls\n",
    "\n",
    "\n",
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "\n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "\n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "\n",
    "    pieces = []\n",
    "    kept = 0\n",
    "    dropped_high_fill = 0\n",
    "    dropped_empty = 0\n",
    "    fill_ratios = []\n",
    "    expected_lengths = []\n",
    "\n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "\n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)][RAW_COLS].copy()\n",
    "        day = day.reindex(exp_idx)\n",
    "\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "\n",
    "        if fill_ratio >= 1.0:\n",
    "            dropped_empty += 1\n",
    "            continue\n",
    "\n",
    "        if fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            dropped_high_fill += 1\n",
    "            continue\n",
    "\n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        if day['VWAP'].isna().all():\n",
    "            day['VWAP'] = day['Close']\n",
    "        else:\n",
    "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "\n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "\n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after per-session fill.')\n",
    "\n",
    "        pieces.append(day)\n",
    "        kept += 1\n",
    "        fill_ratios.append(fill_ratio)\n",
    "        expected_lengths.append(len(exp_idx))\n",
    "\n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after calendar filtering and fill-ratio threshold.')\n",
    "\n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "\n",
    "    out = out.copy()\n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "\n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(kept),\n",
    "        'dropped_empty_sessions': int(dropped_empty),\n",
    "        'dropped_high_fill_sessions': int(dropped_high_fill),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "        'max_fill_ratio_kept': float(np.max(fill_ratios)) if fill_ratios else float('nan'),\n",
    "        'avg_session_minutes_kept': float(np.mean(expected_lengths)) if expected_lengths else float('nan'),\n",
    "    }\n",
    "    return out, meta\n",
    "\n",
    "\n",
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "\n",
    "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
    "print(f'Sessionized rows kept: {len(price_df):,}')\n",
    "print(f'Alpaca API calls: {api_calls} (<= {MAX_REQUESTS_PER_MINUTE}/min target)')\n",
    "print('Session meta:', session_meta)\n",
    "\n",
    "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
    "if len(price_df) < min_needed:\n",
    "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')\n",
    "\n",
    "display(price_df.head(3))\n",
    "display(price_df.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6_feat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering + direct OHLC return targets\n",
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    out = np.asarray(ohlc, dtype=np.float32).copy()\n",
    "    o = out[:, 0]\n",
    "    h = out[:, 1]\n",
    "    l = out[:, 2]\n",
    "    c = out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)\n",
    "\n",
    "\n",
    "def one_step_returns_to_prices_batch(return_ohlc: np.ndarray, prev_close: np.ndarray) -> np.ndarray:\n",
    "    r = np.asarray(return_ohlc, dtype=np.float32)\n",
    "    p = np.asarray(prev_close, dtype=np.float32)\n",
    "    out = np.stack([\n",
    "        p * np.exp(r[:, 0]),\n",
    "        p * np.exp(r[:, 1]),\n",
    "        p * np.exp(r[:, 2]),\n",
    "        p * np.exp(r[:, 3]),\n",
    "    ], axis=1).astype(np.float32)\n",
    "    return enforce_candle_validity(out)\n",
    "\n",
    "\n",
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    eps = 1e-9\n",
    "\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "\n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0.0)\n",
    "\n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "\n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_target_frame(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Direct OHLC return targets - no structured intermediary\n",
    "    out = feat_df[TARGET_COLS].copy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "feat_df = build_feature_frame(price_df)\n",
    "target_df = build_target_frame(feat_df)\n",
    "print('Feature rows:', len(feat_df))\n",
    "print('Rows marked imputed-sensitive:', int(feat_df['row_imputed'].sum()))\n",
    "print('Rows skipped as opening-target bars:', int(feat_df['row_open_skip'].sum()))\n",
    "print('Target columns:', list(target_df.columns))\n",
    "display(feat_df[BASE_FEATURE_COLS + ['row_imputed', 'row_open_skip', 'prev_close']].head(3))\n",
    "display(target_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8c4bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk-forward slices: [('slice_1', 0, 12444), ('slice_2', 2196, 14640)]\n"
     ]
    }
   ],
   "source": [
    "# Windowing/splits + lookback sweep helpers\n",
    "def fit_standardizer(train_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    mean = train_values.mean(axis=0)\n",
    "    std = train_values.std(axis=0)\n",
    "    std = np.where(std < 1e-8, 1.0, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return ((values - mean) / std).astype(np.float32)\n",
    "\n",
    "\n",
    "def undo_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return (values * std + mean).astype(np.float32)\n",
    "\n",
    "\n",
    "def split_points(n_rows: int) -> tuple[int, int]:\n",
    "    tr = int(n_rows * TRAIN_RATIO)\n",
    "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
    "    return tr, va\n",
    "\n",
    "\n",
    "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
    "    n = len(price_df_full)\n",
    "    span = int(round(n * 0.85))\n",
    "    shift = max(1, n - span)\n",
    "\n",
    "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
    "\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for name, a, b in cands:\n",
    "        key = (a, b)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        if b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1400:\n",
    "            continue\n",
    "        out.append((name, a, b))\n",
    "        seen.add(key)\n",
    "\n",
    "    if not out:\n",
    "        raise RuntimeError('Unable to create walk-forward slices.')\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_multistep_windows(\n",
    "    input_scaled: np.ndarray,\n",
    "    target_scaled: np.ndarray,\n",
    "    target_raw: np.ndarray,\n",
    "    row_imputed: np.ndarray,\n",
    "    row_open_skip: np.ndarray,\n",
    "    starts_prev_close: np.ndarray,\n",
    "    window: int,\n",
    "    horizon: int,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, int]:\n",
    "    X = []\n",
    "    y_s = []\n",
    "    y_r = []\n",
    "    starts = []\n",
    "    prev_close = []\n",
    "    dropped_target_imputed = 0\n",
    "    dropped_target_open_skip = 0\n",
    "\n",
    "    n = len(input_scaled)\n",
    "    for i in range(window, n - horizon + 1):\n",
    "        if row_imputed[i : i + horizon].any():\n",
    "            dropped_target_imputed += 1\n",
    "            continue\n",
    "        if row_open_skip[i : i + horizon].any():\n",
    "            dropped_target_open_skip += 1\n",
    "            continue\n",
    "\n",
    "        xb = input_scaled[i - window : i]\n",
    "        imp_frac = float(row_imputed[i - window : i].mean())\n",
    "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
    "        xb_aug = np.concatenate([xb, imp_col], axis=1).astype(np.float32)\n",
    "\n",
    "        X.append(xb_aug)\n",
    "        y_s.append(target_scaled[i : i + horizon])\n",
    "        y_r.append(target_raw[i : i + horizon])\n",
    "        starts.append(i)\n",
    "        prev_close.append(starts_prev_close[i])\n",
    "\n",
    "    return (\n",
    "        np.asarray(X, dtype=np.float32),\n",
    "        np.asarray(y_s, dtype=np.float32),\n",
    "        np.asarray(y_r, dtype=np.float32),\n",
    "        np.asarray(starts, dtype=np.int64),\n",
    "        np.asarray(prev_close, dtype=np.float32),\n",
    "        dropped_target_imputed,\n",
    "        dropped_target_open_skip,\n",
    "    )\n",
    "\n",
    "\n",
    "class MultiStepDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y_s: np.ndarray, y_r: np.ndarray):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_s = torch.from_numpy(y_s).float()\n",
    "        self.y_r = torch.from_numpy(y_r).float()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y_s[idx], self.y_r[idx]\n",
    "\n",
    "\n",
    "slices = build_walkforward_slices(price_df)\n",
    "print('Walk-forward slices:', slices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with attention decoder + simplified losses (v5: scale-aware)\n",
    "class Seq2SeqAttnGRU(nn.Module):\n",
    "    \"\"\"GRU encoder-decoder with Luong dot-product attention.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int,\n",
    "                 num_layers: int, dropout: float, horizon: int):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Decoder cell takes output_size + hidden_size (context) as input\n",
    "        self.decoder_cell = nn.GRUCell(output_size + hidden_size, hidden_size)\n",
    "\n",
    "        # Attention projection\n",
    "        self.attn_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Output heads\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "        # Better initialization for mu_head \u2014 start near zero but with gradient signal\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "\n",
    "    def _attend(self, h_dec: torch.Tensor, enc_out: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Luong dot-product attention: context = weighted sum of encoder outputs.\"\"\"\n",
    "        # h_dec: (B, H), enc_out: (B, T, H)\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)       # (B, H, 1)\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2)    # (B, T)\n",
    "        weights = torch.softmax(scores, dim=1)            # (B, T)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)  # (B, H)\n",
    "        return context\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y_teacher: torch.Tensor | None = None,\n",
    "        teacher_forcing_ratio: float = 0.0,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        enc_out, h = self.encoder(x)   # enc_out: (B, T, H), h: (num_layers, B, H)\n",
    "        h_dec = h[-1]                  # (B, H)\n",
    "\n",
    "        dec_input = x[:, -1, : self.output_size]  # last timestep OHLC returns\n",
    "\n",
    "        mu_seq = []\n",
    "        log_sigma_seq = []\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "            # Attention context from encoder\n",
    "            context = self._attend(h_dec, enc_out)   # (B, H)\n",
    "\n",
    "            # Decoder step: input is [prev_output ; context]\n",
    "            cell_input = torch.cat([dec_input, context], dim=1)  # (B, output_size + H)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "\n",
    "            # Concatenate hidden + context for output heads\n",
    "            out_features = torch.cat([h_dec, context], dim=1)  # (B, 2H)\n",
    "\n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = torch.clamp(self.log_sigma_head(out_features), min=-6.0, max=LOG_SIGMA_MAX)\n",
    "\n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            log_sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "\n",
    "            # Determine next decoder input\n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    m = (torch.rand(x.size(0), device=x.device) < teacher_forcing_ratio).unsqueeze(1)\n",
    "                    dec_input = torch.where(m, y_teacher[:, t, :], mu)\n",
    "            else:\n",
    "                dec_input = mu\n",
    "\n",
    "        return (\n",
    "            torch.cat(mu_seq, dim=1),      # (B, horizon, output_size)\n",
    "            torch.cat(log_sigma_seq, dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "class WeightedGaussianNLL(nn.Module):\n",
    "    def __init__(self, weights: np.ndarray):\n",
    "        super().__init__()\n",
    "        w = torch.as_tensor(weights, dtype=torch.float32).view(1, 1, -1)\n",
    "        self.register_buffer('weights', w)\n",
    "\n",
    "    def forward(self, mu, log_sigma, target, step_weights):\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        sigma = torch.clamp(sigma, min=1e-4)  # floor: prevent collapse to zero\n",
    "        z = (target - mu) / sigma\n",
    "        nll = 0.5 * (z * z + 2.0 * torch.log(sigma))\n",
    "        return (nll * self.weights * step_weights).mean()\n",
    "\n",
    "\n",
    "class WeightedHuberMeanLoss(nn.Module):\n",
    "    def __init__(self, weights: np.ndarray, delta: float):\n",
    "        super().__init__()\n",
    "        self.delta = float(delta)\n",
    "        w = torch.as_tensor(weights, dtype=torch.float32).view(1, 1, -1)\n",
    "        self.register_buffer('weights', w)\n",
    "\n",
    "    def forward(self, pred, target, step_weights):\n",
    "        e = pred - target\n",
    "        ae = torch.abs(e)\n",
    "        hub = torch.where(ae <= self.delta, 0.5 * e * e, self.delta * (ae - 0.5 * self.delta))\n",
    "        return (hub * self.weights * step_weights).mean()\n",
    "\n",
    "\n",
    "def directional_penalty(mu: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Soft penalty when predicted return direction disagrees with actual.\n",
    "    Operates on close returns (column 3) across all horizon steps.\"\"\"\n",
    "    pred_close = mu[:, :, 3]    # (B, H)\n",
    "    actual_close = target[:, :, 3]  # (B, H)\n",
    "    sign_match = torch.sign(pred_close) * torch.sign(actual_close)\n",
    "    penalty = torch.clamp(-sign_match, min=0.0)  # 1 when disagree, 0 when agree\n",
    "    return penalty.mean()\n",
    "\n",
    "\n",
    "def tf_ratio_for_epoch(epoch: int) -> float:\n",
    "    \"\"\"Exponential decay: tf = TF_START * (TF_DECAY_RATE ** (epoch - 1)), clamped to TF_END.\"\"\"\n",
    "    ratio = TF_START * (TF_DECAY_RATE ** (epoch - 1))\n",
    "    return max(float(TF_END), float(ratio))\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    nll_loss_fn: nn.Module,\n",
    "    huber_loss_fn: nn.Module,\n",
    "    step_weights_t: torch.Tensor,\n",
    "    optimizer: torch.optim.Optimizer | None = None,\n",
    "    tf_ratio: float = 0.0,\n",
    ") -> dict:\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    total = 0.0\n",
    "    nll_tot = 0.0\n",
    "    hub_tot = 0.0\n",
    "    dir_tot = 0.0\n",
    "    n_items = 0\n",
    "\n",
    "    for xb, yb_s, yb_r in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb_s = yb_s.to(DEVICE)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            mu, log_sigma = model(\n",
    "                xb,\n",
    "                y_teacher=yb_s if is_train else None,\n",
    "                teacher_forcing_ratio=tf_ratio if is_train else 0.0,\n",
    "            )\n",
    "\n",
    "            nll = nll_loss_fn(mu, log_sigma, yb_s, step_weights_t)\n",
    "            hub = huber_loss_fn(mu, yb_s, step_weights_t)\n",
    "            dir_pen = directional_penalty(mu, yb_s)\n",
    "\n",
    "            loss = nll + MU_HUBER_WEIGHT * hub + DIR_PENALTY_WEIGHT * dir_pen\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total += loss.item() * bs\n",
    "        nll_tot += nll.item() * bs\n",
    "        hub_tot += hub.item() * bs\n",
    "        dir_tot += dir_pen.item() * bs\n",
    "        n_items += bs\n",
    "\n",
    "    return {\n",
    "        'total': total / max(n_items, 1),\n",
    "        'nll': nll_tot / max(n_items, 1),\n",
    "        'hub': hub_tot / max(n_items, 1),\n",
    "        'dir': dir_tot / max(n_items, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    max_epochs: int,\n",
    "    patience: int,\n",
    ") -> pd.DataFrame:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5)\n",
    "\n",
    "    nll_loss_fn = WeightedGaussianNLL(NLL_WEIGHTS).to(DEVICE)\n",
    "    huber_loss_fn = WeightedHuberMeanLoss(HUBER_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
    "\n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    wait = 0\n",
    "    rows = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tf = tf_ratio_for_epoch(epoch)\n",
    "\n",
    "        tr = run_epoch(model, train_loader, nll_loss_fn, huber_loss_fn, step_weights_t, optimizer=optimizer, tf_ratio=tf)\n",
    "        va = run_epoch(model, val_loader, nll_loss_fn, huber_loss_fn, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "\n",
    "        scheduler.step(va['total'])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        rows.append({\n",
    "            'epoch': epoch, 'tf_ratio': tf, 'lr': lr,\n",
    "            'train_total': tr['total'], 'val_total': va['total'],\n",
    "            'train_nll': tr['nll'], 'val_nll': va['nll'],\n",
    "            'train_hub': tr['hub'], 'val_hub': va['hub'],\n",
    "            'train_dir': tr['dir'], 'val_dir': va['dir'],\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
    "            f\"train={tr['total']:.6f} (nll={tr['nll']:.6f}, hub={tr['hub']:.6f}, dir={tr['dir']:.3f}) | \"\n",
    "            f\"val={va['total']:.6f} (nll={va['nll']:.6f}, hub={va['hub']:.6f}, dir={va['dir']:.3f}) | lr={lr:.6g}\"\n",
    "        )\n",
    "\n",
    "        if va['total'] < best_val:\n",
    "            best_val = va['total']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}.')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and fold execution (direct multi-step, no external recursion)\n",
    "def rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b)) ** 2)))\n",
    "\n",
    "\n",
    "def directional_accuracy_eps(actual_close: np.ndarray, pred_close: np.ndarray, prev_close: np.ndarray, eps: float) -> float:\n",
    "    am = np.asarray(actual_close) - np.asarray(prev_close)\n",
    "    pm = np.asarray(pred_close) - np.asarray(prev_close)\n",
    "    mask = np.abs(am) > eps\n",
    "    if not np.any(mask):\n",
    "        return float('nan')\n",
    "    return float(np.mean(np.sign(am[mask]) == np.sign(pm[mask])))\n",
    "\n",
    "\n",
    "def evaluate_metrics(actual_ohlc: np.ndarray, pred_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
    "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
    "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
    "    prev_close = np.asarray(prev_close, dtype=np.float32)\n",
    "\n",
    "    ac = actual_ohlc[:, 3]\n",
    "    pc = pred_ohlc[:, 3]\n",
    "\n",
    "    return {\n",
    "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
    "        'close_rmse': rmse(ac, pc),\n",
    "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
    "        'ohlc_rmse': rmse(actual_ohlc.reshape(-1), pred_ohlc.reshape(-1)),\n",
    "        'directional_accuracy_eps': directional_accuracy_eps(ac, pc, prev_close, DIRECTION_EPS),\n",
    "        'mean_signed_bias': float(np.mean(pc - ac)),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_baselines(actual_ohlc: np.ndarray, prev_ohlc: np.ndarray, prev_close: np.ndarray) -> dict:\n",
    "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
    "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
    "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
    "    return {'persistence': persistence, 'flat_close_rw': flat_rw}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_multistep_direct(model: nn.Module, X: np.ndarray, batch_size: int = 512) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Run model once to get all horizon steps directly (no recursion).\"\"\"\n",
    "    model.eval()\n",
    "    mus = []\n",
    "    logs = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = torch.from_numpy(X[i : i + batch_size]).float().to(DEVICE)\n",
    "        mu, log_sigma = model(xb, y_teacher=None, teacher_forcing_ratio=0.0)\n",
    "        mus.append(mu.cpu().numpy())\n",
    "        logs.append(log_sigma.cpu().numpy())\n",
    "    return np.concatenate(mus, axis=0).astype(np.float32), np.concatenate(logs, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "def evaluate_direct_by_horizon(\n",
    "    model: nn.Module,\n",
    "    X_test: np.ndarray,\n",
    "    price_values: np.ndarray,\n",
    "    prev_close_values: np.ndarray,\n",
    "    test_starts: np.ndarray,\n",
    "    target_mean: np.ndarray,\n",
    "    target_std: np.ndarray,\n",
    "    horizon: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Evaluate direct multi-step predictions per horizon step.\"\"\"\n",
    "    mu_s, _ = predict_multistep_direct(model, X_test)\n",
    "    # Un-standardize (if targets were standardized; otherwise tg_mean=0,tg_std=1)\n",
    "    mu_raw = (mu_s * target_std.reshape(1, 1, -1) + target_mean.reshape(1, 1, -1)).astype(np.float32)\n",
    "\n",
    "    m_err = [[] for _ in range(horizon)]\n",
    "    p_err = [[] for _ in range(horizon)]\n",
    "    f_err = [[] for _ in range(horizon)]\n",
    "\n",
    "    for idx in range(len(test_starts)):\n",
    "        anchor = int(test_starts[idx])\n",
    "        last_close = float(prev_close_values[anchor])\n",
    "        pred_rets = mu_raw[idx]  # (horizon, 4) direct return predictions\n",
    "        pred_price = returns_to_prices_seq(pred_rets, last_close=last_close)\n",
    "\n",
    "        actual = price_values[anchor + 1 : anchor + 1 + horizon]\n",
    "        if len(actual) < horizon:\n",
    "            continue\n",
    "\n",
    "        persist_seq = np.repeat(price_values[anchor : anchor + 1], horizon, axis=0)\n",
    "        c0 = prev_close_values[anchor]\n",
    "        flat_seq = np.repeat(np.array([[c0, c0, c0, c0]], dtype=np.float32), horizon, axis=0)\n",
    "\n",
    "        for h in range(horizon):\n",
    "            m_err[h].append(abs(float(pred_price[h, 3] - actual[h, 3])))\n",
    "            p_err[h].append(abs(float(persist_seq[h, 3] - actual[h, 3])))\n",
    "            f_err[h].append(abs(float(flat_seq[h, 3] - actual[h, 3])))\n",
    "\n",
    "    rows = []\n",
    "    for h in range(horizon):\n",
    "        rows.append({\n",
    "            'horizon': h + 1,\n",
    "            'model_close_mae': float(np.mean(m_err[h])) if m_err[h] else np.nan,\n",
    "            'persistence_close_mae': float(np.mean(p_err[h])) if p_err[h] else np.nan,\n",
    "            'flat_close_mae': float(np.mean(f_err[h])) if f_err[h] else np.nan,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def overfit_sanity_check(X_train, y_train_s, y_train_r, output_dim) -> dict:\n",
    "    n = min(1000, len(X_train))\n",
    "    if n < 300:\n",
    "        return {'ran': False, 'reason': 'too_few_samples'}\n",
    "\n",
    "    ds = MultiStepDataset(X_train[:n], y_train_s[:n], y_train_r[:n])\n",
    "    dl = DataLoader(ds, batch_size=128, shuffle=True, drop_last=False)\n",
    "\n",
    "    model = Seq2SeqAttnGRU(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=output_dim,\n",
    "        hidden_size=128,\n",
    "        num_layers=1,\n",
    "        dropout=0.0,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    nll_loss_fn = WeightedGaussianNLL(NLL_WEIGHTS).to(DEVICE)\n",
    "    huber_loss_fn = WeightedHuberMeanLoss(HUBER_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.0)\n",
    "\n",
    "    init_stats = run_epoch(model, dl, nll_loss_fn, huber_loss_fn, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "    for _ in range(16):\n",
    "        _ = run_epoch(model, dl, nll_loss_fn, huber_loss_fn, step_weights_t, optimizer=opt, tf_ratio=1.0)\n",
    "    fin_stats = run_epoch(model, dl, nll_loss_fn, huber_loss_fn, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "\n",
    "    passed = bool(fin_stats['total'] < init_stats['total'] * 0.70)\n",
    "    return {'ran': True, 'initial_total': float(init_stats['total']), 'final_total': float(fin_stats['total']), 'passed': passed}\n",
    "\n",
    "\n",
    "def run_fold(\n",
    "    fold_name: str,\n",
    "    price_fold: pd.DataFrame,\n",
    "    window: int,\n",
    "    max_epochs: int,\n",
    "    patience: int,\n",
    "    run_sanity: bool = False,\n",
    "    quick_mode: bool = False,\n",
    ") -> dict:\n",
    "    feat_fold = build_feature_frame(price_fold)\n",
    "    target_fold = build_target_frame(feat_fold)\n",
    "\n",
    "    input_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "    target_raw = target_fold[TARGET_COLS].to_numpy(np.float32)\n",
    "\n",
    "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "    row_open_skip = feat_fold['row_open_skip'].to_numpy(np.int8).astype(bool)\n",
    "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
    "\n",
    "    price_vals = price_fold.loc[feat_fold.index, OHLC_COLS].to_numpy(np.float32)\n",
    "\n",
    "    tr_end, va_end = split_points(len(input_raw))\n",
    "    if tr_end <= window or va_end <= tr_end:\n",
    "        raise RuntimeError(f'{fold_name}: invalid split points for rows={len(input_raw)}, window={window}')\n",
    "\n",
    "    in_mean, in_std = fit_standardizer(input_raw[:tr_end])\n",
    "\n",
    "    # V5: conditionally standardize targets\n",
    "    if STANDARDIZE_TARGETS:\n",
    "        tg_mean, tg_std = fit_standardizer(target_raw[:tr_end])\n",
    "    else:\n",
    "        tg_mean = np.zeros(target_raw.shape[1], dtype=np.float32)\n",
    "        tg_std = np.ones(target_raw.shape[1], dtype=np.float32)\n",
    "\n",
    "    input_scaled = apply_standardizer(input_raw, in_mean, in_std)\n",
    "    target_scaled = apply_standardizer(target_raw, tg_mean, tg_std)\n",
    "\n",
    "    ql, qh = CLIP_QUANTILES\n",
    "    clip_low = np.quantile(target_raw[:tr_end], ql, axis=0).astype(np.float32)\n",
    "    clip_high = np.quantile(target_raw[:tr_end], qh, axis=0).astype(np.float32)\n",
    "\n",
    "    X_all, y_all_s, y_all_r, starts, prev_close_starts, dropped_target_imputed, dropped_target_open_skip = make_multistep_windows(\n",
    "        input_scaled=input_scaled,\n",
    "        target_scaled=target_scaled,\n",
    "        target_raw=target_raw,\n",
    "        row_imputed=row_imputed,\n",
    "        row_open_skip=row_open_skip,\n",
    "        starts_prev_close=prev_close,\n",
    "        window=window,\n",
    "        horizon=HORIZON,\n",
    "    )\n",
    "\n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: no windows available after target filtering.')\n",
    "\n",
    "    end_idx = starts + HORIZON - 1\n",
    "    tr_m = end_idx < tr_end\n",
    "    va_m = (end_idx >= tr_end) & (end_idx < va_end)\n",
    "    te_m = end_idx >= va_end\n",
    "\n",
    "    X_train, y_train_s, y_train_r = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m]\n",
    "    X_val, y_val_s, y_val_r = X_all[va_m], y_all_s[va_m], y_all_r[va_m]\n",
    "    X_test, y_test_s, y_test_r = X_all[te_m], y_all_s[te_m], y_all_r[te_m]\n",
    "    test_starts = starts[te_m]\n",
    "    test_prev_close = prev_close_starts[te_m]\n",
    "\n",
    "    if min(len(X_train), len(X_val), len(X_test)) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: empty train/val/test split after windowing.')\n",
    "\n",
    "    print(f'  Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}')\n",
    "    print(f'  Target stats: mean={target_raw[:tr_end].mean(axis=0)}, std={target_raw[:tr_end].std(axis=0)}')\n",
    "    print(f'  Standardize targets: {STANDARDIZE_TARGETS}, tg_mean={tg_mean}, tg_std={tg_std}')\n",
    "\n",
    "    sanity = overfit_sanity_check(X_train, y_train_s, y_train_r, output_dim=len(TARGET_COLS)) if run_sanity else {'ran': False}\n",
    "\n",
    "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = Seq2SeqAttnGRU(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=len(TARGET_COLS),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    hist = train_model(model, train_loader, val_loader, max_epochs=max_epochs, patience=patience)\n",
    "\n",
    "    # Direct multi-step evaluation (no recursion!)\n",
    "    mu_test_s, _ = predict_multistep_direct(model, X_test)\n",
    "    mu_test_raw = (mu_test_s * tg_std.reshape(1, 1, -1) + tg_mean.reshape(1, 1, -1)).astype(np.float32)\n",
    "\n",
    "    # One-step metrics\n",
    "    pred_step1_ret = mu_test_raw[:, 0, :]\n",
    "    actual_step1_ret = y_test_r[:, 0, :]\n",
    "\n",
    "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
    "    pred_ohlc_1 = one_step_returns_to_prices_batch(pred_step1_ret, test_prev_close)\n",
    "    prev_ohlc = price_vals[test_starts]\n",
    "\n",
    "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
    "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
    "\n",
    "    pred_close_std = float(np.std(pred_step1_ret[:, 3]))\n",
    "    actual_close_std = float(np.std(actual_step1_ret[:, 3]))\n",
    "    std_ratio = float(pred_close_std / max(actual_close_std, 1e-12))\n",
    "\n",
    "    if quick_mode:\n",
    "        return {\n",
    "            'fold': fold_name, 'window': int(window), 'history_df': hist, 'sanity': sanity,\n",
    "            'model_metrics': model_metrics, 'baseline_metrics': baseline_metrics,\n",
    "            'pred_actual_std_ratio': std_ratio,\n",
    "            'samples': {\n",
    "                'train': int(len(X_train)), 'val': int(len(X_val)), 'test': int(len(X_test)),\n",
    "                'dropped_target_imputed': int(dropped_target_imputed),\n",
    "                'dropped_target_open_skip': int(dropped_target_open_skip),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # Per-horizon evaluation using DIRECT multi-step output\n",
    "    horizon_df = evaluate_direct_by_horizon(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        price_values=price_vals,\n",
    "        prev_close_values=prev_close,\n",
    "        test_starts=test_starts,\n",
    "        target_mean=tg_mean,\n",
    "        target_std=tg_std,\n",
    "        horizon=HORIZON,\n",
    "    )\n",
    "\n",
    "    # Build prediction chart from last test anchor (direct, single model call)\n",
    "    last_anchor_idx = len(test_starts) - 1\n",
    "    last_anchor = int(test_starts[last_anchor_idx])\n",
    "    last_close = float(prev_close[last_anchor])\n",
    "\n",
    "    pred_rets_last = mu_test_raw[last_anchor_idx]  # (horizon, 4)\n",
    "    pred_price_det = returns_to_prices_seq(pred_rets_last, last_close=last_close)\n",
    "\n",
    "    actual_price_path = price_vals[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
    "    future_idx = price_fold.loc[feat_fold.index].index[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
    "\n",
    "    pred_future_df_det = pd.DataFrame(pred_price_det[:len(future_idx)], index=future_idx[:len(pred_price_det)], columns=OHLC_COLS)\n",
    "    actual_future_df = pd.DataFrame(actual_price_path[:len(future_idx)], index=future_idx[:len(actual_price_path)], columns=OHLC_COLS)\n",
    "\n",
    "    known_pos = last_anchor\n",
    "    context_df = price_fold.loc[feat_fold.index].iloc[max(0, known_pos - 199) : known_pos + 1][OHLC_COLS].copy()\n",
    "\n",
    "    stepH = horizon_df[horizon_df['horizon'] == HORIZON].iloc[0]\n",
    "\n",
    "    return {\n",
    "        'fold': fold_name, 'window': int(window), 'history_df': hist, 'sanity': sanity,\n",
    "        'model_metrics': model_metrics, 'baseline_metrics': baseline_metrics,\n",
    "        'horizon_df': horizon_df, 'pred_actual_std_ratio': std_ratio,\n",
    "        'samples': {\n",
    "            'train': int(len(X_train)), 'val': int(len(X_val)), 'test': int(len(X_test)),\n",
    "            'dropped_target_imputed': int(dropped_target_imputed),\n",
    "            'dropped_target_open_skip': int(dropped_target_open_skip),\n",
    "            'anchors': int(len(test_starts)),\n",
    "        },\n",
    "        'stepH': {\n",
    "            'model_close_mae': float(stepH['model_close_mae']),\n",
    "            'persistence_close_mae': float(stepH['persistence_close_mae']),\n",
    "            'flat_close_mae': float(stepH['flat_close_mae']),\n",
    "        },\n",
    "        'context_df': context_df,\n",
    "        'actual_future_df': actual_future_df,\n",
    "        'pred_future_df_det': pred_future_df_det,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run lookback sweep then full walk-forward\n",
    "fold_results = []\n",
    "\n",
    "primary_slice = slices[0]\n",
    "sweep_rows = []\n",
    "selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "if ENABLE_LOOKBACK_SWEEP:\n",
    "    print('\\n=== Lookback sweep (quick mode on first fold) ===')\n",
    "    _, a0, b0 = primary_slice\n",
    "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
    "\n",
    "    for w in LOOKBACK_CANDIDATES:\n",
    "        print(f'\\n-- Sweep candidate lookback={w} --')\n",
    "        r = run_fold(\n",
    "            fold_name=f'sweep_w{w}',\n",
    "            price_fold=fold_price0,\n",
    "            window=w,\n",
    "            max_epochs=SWEEP_MAX_EPOCHS,\n",
    "            patience=SWEEP_PATIENCE,\n",
    "            run_sanity=False,\n",
    "            quick_mode=True,\n",
    "        )\n",
    "\n",
    "        m = r['model_metrics']\n",
    "        p = r['baseline_metrics']['persistence']\n",
    "        std_ratio = r['pred_actual_std_ratio']\n",
    "\n",
    "        ratio = m['close_mae'] / max(p['close_mae'], 1e-12)\n",
    "        score = -ratio\n",
    "        if ratio < 1.0:\n",
    "            score += 0.10\n",
    "        score -= 0.20 * abs(1.0 - std_ratio)\n",
    "        if std_ratio < 0.35:\n",
    "            score -= 0.25\n",
    "        if std_ratio > 1.40:\n",
    "            score -= 0.20 * (std_ratio - 1.40)\n",
    "\n",
    "        sweep_rows.append({\n",
    "            'lookback': w,\n",
    "            'model_close_mae': m['close_mae'],\n",
    "            'persistence_close_mae': p['close_mae'],\n",
    "            'model_vs_persistence_ratio': ratio,\n",
    "            'std_ratio': std_ratio,\n",
    "            'score': score,\n",
    "            'dropped_target_imputed': r['samples']['dropped_target_imputed'],\n",
    "            'dropped_target_open_skip': r['samples']['dropped_target_open_skip'],\n",
    "        })\n",
    "\n",
    "    sweep_df = pd.DataFrame(sweep_rows).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    display(sweep_df)\n",
    "\n",
    "    selected_window = int(sweep_df.iloc[0]['lookback'])\n",
    "    print(f'Selected lookback from sweep: {selected_window}')\n",
    "else:\n",
    "    sweep_df = pd.DataFrame()\n",
    "    selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "print('\\n=== Full walk-forward with selected lookback ===')\n",
    "for i, (name, a, b) in enumerate(slices, start=1):\n",
    "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
    "    fold_price = price_df.iloc[a:b].copy()\n",
    "\n",
    "    res = run_fold(\n",
    "        fold_name=name,\n",
    "        price_fold=fold_price,\n",
    "        window=selected_window,\n",
    "        max_epochs=FINAL_MAX_EPOCHS,\n",
    "        patience=FINAL_PATIENCE,\n",
    "        run_sanity=(i == 1),\n",
    "        quick_mode=False,\n",
    "    )\n",
    "    fold_results.append(res)\n",
    "\n",
    "    print('Samples:', res['samples'])\n",
    "    print('Sanity:', res['sanity'])\n",
    "    print('One-step model:', res['model_metrics'])\n",
    "    print('One-step persistence:', res['baseline_metrics']['persistence'])\n",
    "    print('One-step flat:', res['baseline_metrics']['flat_close_rw'])\n",
    "    print(f'Direct step-{HORIZON}:', res['stepH'])\n",
    "    print('Std ratio (pred/actual close return):', res['pred_actual_std_ratio'])\n",
    "\n",
    "if not fold_results:\n",
    "    raise RuntimeError('No fold results produced.')\n",
    "\n",
    "summary_rows = []\n",
    "for r in fold_results:\n",
    "    m = r['model_metrics']\n",
    "    p = r['baseline_metrics']['persistence']\n",
    "    f = r['baseline_metrics']['flat_close_rw']\n",
    "    sH = r['stepH']\n",
    "\n",
    "    summary_rows.append({\n",
    "        'fold': r['fold'],\n",
    "        'window': r['window'],\n",
    "        'model_close_mae': m['close_mae'],\n",
    "        'model_directional_acc_eps': m['directional_accuracy_eps'],\n",
    "        'model_bias': m['mean_signed_bias'],\n",
    "        'persist_close_mae': p['close_mae'],\n",
    "        'flat_close_mae': f['close_mae'],\n",
    "        f'step{HORIZON}_model_mae': sH['model_close_mae'],\n",
    "        f'step{HORIZON}_persist_mae': sH['persistence_close_mae'],\n",
    "        'pred_actual_std_ratio': r['pred_actual_std_ratio'],\n",
    "        'dropped_target_imputed': r['samples']['dropped_target_imputed'],\n",
    "        'dropped_target_open_skip': r['samples']['dropped_target_open_skip'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "display(summary_df)\n",
    "\n",
    "metric_cols = [c for c in summary_df.columns if c not in {'fold'}]\n",
    "agg_df = pd.DataFrame({'mean': summary_df[metric_cols].mean(), 'std': summary_df[metric_cols].std(ddof=0)})\n",
    "display(agg_df)\n",
    "\n",
    "horizon_all = pd.concat([r['horizon_df'].assign(fold=r['fold']) for r in fold_results], ignore_index=True)\n",
    "horizon_stats = horizon_all.groupby('horizon', as_index=False)[['model_close_mae', 'persistence_close_mae', 'flat_close_mae']].mean()\n",
    "display(horizon_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11_accept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptance checks + diagnostics\n",
    "latest = fold_results[-1]\n",
    "\n",
    "mean_model_mae = float(summary_df['model_close_mae'].mean())\n",
    "mean_persist_mae = float(summary_df['persist_close_mae'].mean())\n",
    "mean_dir = float(summary_df['model_directional_acc_eps'].mean())\n",
    "mean_bias = float(summary_df['model_bias'].mean())\n",
    "mean_stepH_model = float(summary_df[f'step{HORIZON}_model_mae'].mean())\n",
    "mean_stepH_persist = float(summary_df[f'step{HORIZON}_persist_mae'].mean())\n",
    "mean_std_ratio = float(summary_df['pred_actual_std_ratio'].mean())\n",
    "\n",
    "pred_close_path = latest['pred_future_df_det']['Close'].to_numpy()\n",
    "is_monotonic = bool(np.all(np.diff(pred_close_path) >= 0) or np.all(np.diff(pred_close_path) <= 0))\n",
    "\n",
    "acceptance = {\n",
    "    'criterion_1_model_mae_20pct_better_than_persistence': mean_model_mae <= 0.8 * mean_persist_mae,\n",
    "    'criterion_2_directional_accuracy_eps_at_least_0_52': mean_dir >= 0.52,\n",
    "    f'criterion_3_step{HORIZON}_better_than_persistence': mean_stepH_model < mean_stepH_persist,\n",
    "    'criterion_4_abs_bias_within_25pct_of_mae': abs(mean_bias) <= 0.25 * mean_model_mae,\n",
    "    'criterion_5_non_monotonic_latest_prediction_path': not is_monotonic,\n",
    "    'criterion_6_prediction_variance_not_collapsed': mean_std_ratio >= STD_RATIO_TARGET_MIN,\n",
    "}\n",
    "\n",
    "print('Selected lookback:', int(summary_df['window'].iloc[0]))\n",
    "print('Acceptance checks:')\n",
    "for k, v in acceptance.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4.8), facecolor='white')\n",
    "\n",
    "hist = latest['history_df']\n",
    "axes[0].plot(hist['epoch'], hist['train_total'], label='Train total', color='black')\n",
    "axes[0].plot(hist['epoch'], hist['val_total'], label='Val total', color='gray')\n",
    "axes[0].plot(hist['epoch'], hist['val_nll'], label='Val NLL', color='#1f77b4', alpha=0.8)\n",
    "if 'val_hub' in hist.columns:\n",
    "    axes[0].plot(hist['epoch'], hist['val_hub'], label='Val Huber(mu)', color='#ff7f0e', alpha=0.8)\n",
    "axes[0].set_title(f\"Loss Curves ({latest['fold']})\")\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(alpha=0.25)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(horizon_stats['horizon'], horizon_stats['model_close_mae'], label='Model', color='black', linewidth=2.0)\n",
    "axes[1].plot(horizon_stats['horizon'], horizon_stats['persistence_close_mae'], label='Persistence', color='#E74C3C')\n",
    "axes[1].plot(horizon_stats['horizon'], horizon_stats['flat_close_mae'], label='Flat RW', color='#3498DB')\n",
    "axes[1].set_title('Direct Close MAE by Horizon')\n",
    "axes[1].set_xlabel('Horizon')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].grid(alpha=0.25)\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].bar(summary_df['fold'], summary_df['pred_actual_std_ratio'], color='#555555')\n",
    "axes[2].axhline(1.0, color='#2ca02c', linestyle='--', linewidth=1.2, label='ideal=1.0')\n",
    "axes[2].axhline(STD_RATIO_TARGET_MIN, color='#ff7f0e', linestyle=':', linewidth=1.2, label=f'min target={STD_RATIO_TARGET_MIN}')\n",
    "axes[2].set_title('Predicted/Actual Close Return Std Ratio')\n",
    "axes[2].set_ylabel('Std ratio')\n",
    "axes[2].grid(alpha=0.25)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12_chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final candlestick chart (history green/red, predicted white/black)\n",
    "def draw_candles(\n",
    "    ax,\n",
    "    ohlc: pd.DataFrame,\n",
    "    start_x: int,\n",
    "    up_edge: str,\n",
    "    up_face: str,\n",
    "    down_edge: str,\n",
    "    down_face: str,\n",
    "    wick_color: str,\n",
    "    width: float = 0.62,\n",
    "    lw: float = 1.0,\n",
    "    alpha: float = 1.0,\n",
    "):\n",
    "    vals = ohlc[OHLC_COLS].to_numpy()\n",
    "    for i, (o, h, l, c) in enumerate(vals):\n",
    "        x = start_x + i\n",
    "        bull = c >= o\n",
    "\n",
    "        ax.vlines(x, l, h, color=wick_color, linewidth=lw, alpha=alpha, zorder=2)\n",
    "\n",
    "        lower = min(o, c)\n",
    "        height = abs(c - o)\n",
    "        if height < 1e-8:\n",
    "            height = 1e-6\n",
    "\n",
    "        rect = Rectangle(\n",
    "            (x - width / 2, lower),\n",
    "            width,\n",
    "            height,\n",
    "            facecolor=up_face if bull else down_face,\n",
    "            edgecolor=up_edge if bull else down_edge,\n",
    "            linewidth=lw,\n",
    "            alpha=alpha,\n",
    "            zorder=3,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "context_df = latest['context_df']\n",
    "actual_future_df = latest['actual_future_df']\n",
    "pred_future_df = latest['pred_future_df_det']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8), facecolor='black')\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "draw_candles(ax, context_df, 0, '#2ECC71', '#2ECC71', '#E74C3C', '#E74C3C', '#DADADA', width=0.58, lw=1.0, alpha=0.95)\n",
    "draw_candles(ax, actual_future_df, len(context_df), '#1D6F42', '#1D6F42', '#8E2F25', '#8E2F25', '#9A9A9A', width=0.58, lw=1.0, alpha=0.72)\n",
    "draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#000000', '#000000', '#F5F5F5', width=0.50, lw=1.35, alpha=1.0)\n",
    "\n",
    "ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=0.9, alpha=0.6)\n",
    "\n",
    "idx = context_df.index.append(actual_future_df.index)\n",
    "n = len(idx)\n",
    "step = max(1, n // 10)\n",
    "ticks = list(range(0, n, step))\n",
    "if ticks[-1] != n - 1:\n",
    "    ticks.append(n - 1)\n",
    "\n",
    "labels = [idx[i].strftime('%m-%d %H:%M') for i in ticks]\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(labels, rotation=26, ha='right', color='white', fontsize=9)\n",
    "\n",
    "ax.tick_params(axis='y', colors='white')\n",
    "for sp in ax.spines.values():\n",
    "    sp.set_color('#666666')\n",
    "\n",
    "ax.grid(color='#252525', linewidth=0.6, alpha=0.35)\n",
    "ax.set_title(f'MSFT 1m ({latest[\"fold\"]}) - History vs {HORIZON}-step Direct Forecast', color='white', pad=14)\n",
    "ax.set_ylabel('Price', color='white')\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='History bullish (green)'),\n",
    "    Patch(facecolor='#E74C3C', edgecolor='#E74C3C', label='History bearish (red)'),\n",
    "    Patch(facecolor='#1D6F42', edgecolor='#1D6F42', label='Actual future (dim green/red)'),\n",
    "    Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted bullish (white)'),\n",
    "    Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted bearish (black)'),\n",
    "]\n",
    "leg = ax.legend(handles=legend_elements, facecolor='black', edgecolor='#707070', framealpha=1.0, loc='upper left')\n",
    "for t in leg.get_texts():\n",
    "    t.set_color('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13_notes",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **V5 key fix**: Targets are NOT standardized. Raw log-returns (~1e-3 scale) used directly.\n",
    "- **Sigma floor**: NLL clamps sigma >= 1e-4 to prevent variance collapse.\n",
    "- **Directional penalty**: Soft loss penalizing wrong-sign predictions on close returns.\n",
    "- **Huber delta=0.005**: Matched to return scale so Huber is effective, not just a constant.\n",
    "- If `pred_actual_std_ratio` is near 1.0, variance collapse is fixed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}