{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: MSFT 1-Minute GRU Forecast (Trend-Aware v7.1)\n",
    "\n",
    "Key changes for trend-aware features:\n",
    "1. Multi-timeframe trend indicators:\n",
    "   - 5-minute EMA slope (resampled and broadcast to 1-min)\n",
    "   - 20-bar momentum: (Close[t] - Close[t-20]) / Close[t-20]\n",
    "   - RSI(14) on 1-minute closes\n",
    "2. DIR_PENALTY_WEIGHT = 2.0 (heavily punish wrong direction predictions)\n",
    "3. Probabilistic outputs (mu + sigma) with temperature control\n",
    "4. Autoregressive recursive generation with candle validity enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required = {\n",
    "    'alpaca': 'alpaca-py',\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'pandas_market_calendars': 'pandas-market-calendars',\n",
    "}\n",
    "missing = [pkg for mod, pkg in required.items() if importlib.util.find_spec(mod) is None]\n",
    "if missing:\n",
    "    print('Installing missing packages:', missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "else:\n",
    "    print('All required third-party packages are already installed.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed & Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Configuration\n",
    "SYMBOL = 'MSFT'\n",
    "LOOKBACK_DAYS = 120\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen', 'rHigh', 'rLow', 'rClose',\n",
    "    'logVolChange', 'logTradeCountChange',\n",
    "    'vwapDelta', 'rangeFrac', 'orderFlowProxy', 'tickPressure',\n",
    "    # NEW: Trend-aware features\n",
    "    'ema5Slope', 'momentum20', 'rsi14',\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "INPUT_EXTRA_COL = 'imputedFracWindow'\n",
    "\n",
    "HORIZON = 15\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "LOOKBACK_CANDIDATES = [64, 96, 160, 256]\n",
    "DEFAULT_LOOKBACK = 96\n",
    "ENABLE_LOOKBACK_SWEEP = True\n",
    "SKIP_OPEN_BARS_TARGET = 6"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model Configuration\n",
    "HIDDEN_SIZE = 256  # Increased for better generation capacity\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.20     # Slightly higher for stochasticity\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 256"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "SWEEP_MAX_EPOCHS = 15\n",
    "SWEEP_PATIENCE = 5\n",
    "FINAL_MAX_EPOCHS = 60  # More epochs for convergence\n",
    "FINAL_PATIENCE = 12\n",
    "TF_START = 1.0\n",
    "TF_END = 0.0\n",
    "TF_DECAY_RATE = 0.95"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Loss Configuration\n",
    "RANGE_LOSS_WEIGHT = 0.3\n",
    "VOLATILITY_WEIGHT = 0.5  # Encourage proper volatility\n",
    "DIR_PENALTY_WEIGHT = 2.0  # v7.1: INCREASED from 0.1 to heavily punish wrong direction\n",
    "STEP_LOSS_POWER = 1.5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inference Configuration\n",
    "SAMPLING_TEMPERATURE = 1.5  # >1.0 = more volatile/wild, <1.0 = conservative\n",
    "VOLATILITY_SCALING = True   # Match recent realized vol\n",
    "MIN_PREDICTED_VOL = 0.0001  # Minimum volatility to prevent flat lines"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Processing Configuration\n",
    "STANDARDIZE_TARGETS = False\n",
    "APPLY_CLIPPING = True\n",
    "CLIP_QUANTILES = (0.001, 0.999)\n",
    "DIRECTION_EPS = 0.0001\n",
    "STD_RATIO_TARGET_MIN = 0.3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Alpaca API Configuration\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.15"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print Configuration Summary\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'lookback_days': LOOKBACK_DAYS,\n",
    "    'horizon': HORIZON,\n",
    "    'sampling_temperature': SAMPLING_TEMPERATURE,\n",
    "    'loss_weights': {\n",
    "        'range': RANGE_LOSS_WEIGHT,\n",
    "        'volatility': VOLATILITY_WEIGHT,\n",
    "        'dir_penalty': DIR_PENALTY_WEIGHT,\n",
    "    },\n",
    "    'trend_features': ['ema5Slope', 'momentum20', 'rsi14'],\n",
    "    'device': str(DEVICE),\n",
    "})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be >0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "        \n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "    \n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "    \n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "    \n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(columns={\n",
    "                'timestamp': 'Datetime', 'open': 'Open', 'high': 'High',\n",
    "                'low': 'Low', 'close': 'Close', 'volume': 'Volume',\n",
    "                'trade_count': 'TradeCount', 'vwap': 'VWAP',\n",
    "            })\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "            \n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "        cursor = chunk_end\n",
    "    \n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "    \n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    \n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "    \n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "    \n",
    "    pieces = []\n",
    "    fill_ratios = []\n",
    "    \n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        \n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "            \n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "            \n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)]\n",
    "        day = day.reindex(exp_idx)\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "        \n",
    "        if fill_ratio >= 1.0 or fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            continue\n",
    "            \n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        if day['VWAP'].isna().all():\n",
    "            day['VWAP'] = day['Close']\n",
    "        else:\n",
    "            day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "            \n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "        \n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after per-session fill.')\n",
    "        pieces.append(day)\n",
    "        fill_ratios.append(fill_ratio)\n",
    "    \n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after calendar filtering.')\n",
    "        \n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "    out = out.copy()\n",
    "    \n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "    \n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(len(pieces)),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "    }\n",
    "    return out, meta"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
    "print(f'Sessionized rows kept: {len(price_df):,}')\n",
    "print('Session meta:', session_meta)\n",
    "\n",
    "min_needed = max(LOOKBACK_CANDIDATES) + HORIZON + 1000\n",
    "if len(price_df) < min_needed:\n",
    "    raise RuntimeError(f'Not enough rows after session filtering ({len(price_df)}). Need at least {min_needed}.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions (v7.1 with Trend Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    out = np.asarray(ohlc, dtype=np.float32)\n",
    "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    \"\"\"Calculate RSI for a price series.\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    \n",
    "    # Use exponential moving average for smoother RSI\n",
    "    avg_gain = gain.ewm(span=period, min_periods=period).mean()\n",
    "    avg_loss = loss.ewm(span=period, min_periods=period).mean()\n",
    "    \n",
    "    rs = avg_gain / (avg_loss + 1e-9)\n",
    "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calc_ema5_slope(df_1min: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate 5-minute EMA slope:\n",
    "    1. Resample to 5-min bars (using last Close)\n",
    "    2. Calculate EMA(12) on 5-min bars\n",
    "    3. Calculate slope (difference between consecutive EMA values)\n",
    "    4. Broadcast back to 1-min index via forward-fill\n",
    "    \"\"\"\n",
    "    # Resample to 5-minute bars using Close\n",
    "    close_5min = df_1min['Close'].resample('5min', label='right', closed='right').last()\n",
    "    \n",
    "    # Calculate EMA on 5-min data\n",
    "    ema_5min = close_5min.ewm(span=12, min_periods=12).mean()\n",
    "    \n",
    "    # Calculate slope (rate of change)\n",
    "    ema_slope_5min = ema_5min.diff() / (ema_5min.shift(1) + 1e-9)\n",
    "    \n",
    "    # Reindex back to 1-min by forward-filling\n",
    "    ema_slope_1min = ema_slope_5min.reindex(df_1min.index, method='ffill')\n",
    "    \n",
    "    return ema_slope_1min.fillna(0.0)\n",
    "\n",
    "def calc_momentum20(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate 20-bar momentum: (Close[t] - Close[t-20]) / Close[t-20]\n",
    "    \"\"\"\n",
    "    momentum = (series - series.shift(20)) / (series.shift(20) + 1e-9)\n",
    "    return momentum"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    eps = 1e-9\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "    \n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0) / (np.abs(out['rClose']) + eps)\n",
    "    \n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "    \n",
    "    # v7.1: NEW TREND-AWARE FEATURES\n",
    "    # 1. 5-minute EMA slope (broadcast from 5-min to 1-min)\n",
    "    out['ema5Slope'] = calc_ema5_slope(df).astype(np.float32)\n",
    "    \n",
    "    # 2. 20-bar momentum\n",
    "    out['momentum20'] = calc_momentum20(df['Close']).astype(np.float32)\n",
    "    \n",
    "    # 3. RSI(14) on 1-minute closes\n",
    "    rsi_raw = calc_rsi(df['Close'], period=14)\n",
    "    # Normalize RSI to [-1, 1] range for better neural network training\n",
    "    out['rsi14'] = ((rsi_raw - 50.0) / 50.0).fillna(0.0).astype(np.float32)\n",
    "    \n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "def build_target_frame(feat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return feat_df[TARGET_COLS].copy().astype(np.float32)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "feat_df = build_feature_frame(price_df)\n",
    "target_df = build_target_frame(feat_df)\n",
    "print('Feature rows:', len(feat_df))\n",
    "print('Feature columns:', list(feat_df.columns))\n",
    "print('Target columns:', list(target_df.columns))\n",
    "\n",
    "# Print trend feature stats\n",
    "print('\\nTrend Feature Statistics:')\n",
    "for col in ['ema5Slope', 'momentum20', 'rsi14']:\n",
    "    print(f'  {col}: mean={feat_df[col].mean():.6f}, std={feat_df[col].std():.6f}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing & Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def split_points(n_rows: int) -> tuple[int, int]:\n",
    "    tr = int(n_rows * TRAIN_RATIO)\n",
    "    va = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
    "    return tr, va\n",
    "\n",
    "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
    "    n = len(price_df_full)\n",
    "    span = int(round(n * 0.85))\n",
    "    shift = max(1, n - span)\n",
    "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for name, a, b in cands:\n",
    "        key = (a, b)\n",
    "        if key in seen or b - a < max(LOOKBACK_CANDIDATES) + HORIZON + 1400:\n",
    "            continue\n",
    "        out.append((name, a, b))\n",
    "        seen.add(key)\n",
    "    return out if out else [('full', 0, n)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def make_multistep_windows(input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, \n",
    "                           starts_prev_close, window, horizon):\n",
    "    X, y_s, y_r, starts, prev_close = [], [], [], [], []\n",
    "    dropped_target_imputed, dropped_target_open_skip = 0, 0\n",
    "    n = len(input_scaled)\n",
    "    \n",
    "    for i in range(window, n - horizon + 1):\n",
    "        if row_imputed[i:i+horizon].any():\n",
    "            dropped_target_imputed += 1\n",
    "            continue\n",
    "        if row_open_skip[i:i+horizon].any():\n",
    "            dropped_target_open_skip += 1\n",
    "            continue\n",
    "            \n",
    "        xb = input_scaled[i-window:i]\n",
    "        imp_frac = float(row_imputed[i-window:i].mean())\n",
    "        imp_col = np.full((window, 1), imp_frac, dtype=np.float32)\n",
    "        xb_aug = np.concatenate([xb, imp_col], axis=1)\n",
    "        \n",
    "        X.append(xb_aug)\n",
    "        y_s.append(target_scaled[i:i+horizon])\n",
    "        y_r.append(target_raw[i:i+horizon])\n",
    "        starts.append(i)\n",
    "        prev_close.append(starts_prev_close[i])\n",
    "    \n",
    "    return (np.asarray(X, dtype=np.float32), np.asarray(y_s, dtype=np.float32),\n",
    "            np.asarray(y_r, dtype=np.float32), np.asarray(starts, dtype=np.int64),\n",
    "            np.asarray(prev_close, dtype=np.float32), dropped_target_imputed, dropped_target_open_skip)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class MultiStepDataset(Dataset):\n",
    "    def __init__(self, X, y_s, y_r):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y_s = torch.from_numpy(y_s).float()\n",
    "        self.y_r = torch.from_numpy(y_r).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_s[idx], self.y_r[idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "slices = build_walkforward_slices(price_df)\n",
    "print('Walk-forward slices:', slices)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Seq2SeqAttnGRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout, horizon):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.decoder_cell = nn.GRUCell(output_size + hidden_size, hidden_size)\n",
    "        self.attn_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "        # Output mu and log_sigma for each OHLC\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, output_size),\n",
    "        )\n",
    "        \n",
    "        # Initialize sigma head to predict moderate volatility initially\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].weight)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].bias)\n",
    "        \n",
    "    def _attend(self, h_dec, enc_out):\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)\n",
    "        return context\n",
    "    \n",
    "    def forward(self, x, y_teacher=None, teacher_forcing_ratio=0.0, return_sigma=False):\n",
    "        enc_out, h = self.encoder(x)\n",
    "        h_dec = h[-1]\n",
    "        dec_input = x[:, -1, :self.output_size]  # Last timestep OHLC returns\n",
    "        \n",
    "        mu_seq, sigma_seq = [], []\n",
    "        for t in range(self.horizon):\n",
    "            context = self._attend(h_dec, enc_out)\n",
    "            cell_input = torch.cat([dec_input, context], dim=1)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "            out_features = torch.cat([h_dec, context], dim=1)\n",
    "            \n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = self.log_sigma_head(out_features)\n",
    "            \n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "            \n",
    "            # Teacher forcing or autoregressive\n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    # Sample for next input during training (scheduled sampling)\n",
    "                    noise = torch.randn_like(mu) * torch.exp(log_sigma).detach()\n",
    "                    dec_input = mu + noise\n",
    "            else:\n",
    "                dec_input = mu  # During pure inference, use mu\n",
    "        \n",
    "        mu_out = torch.cat(mu_seq, dim=1)\n",
    "        sigma_out = torch.cat(sigma_seq, dim=1)\n",
    "        \n",
    "        if return_sigma:\n",
    "            return mu_out, sigma_out\n",
    "        return mu_out\n",
    "    \n",
    "    def generate_realistic(self, x, temperature=1.0, historical_vol=None):\n",
    "        \"\"\"\n",
    "        Generate realistic price paths with controlled stochasticity.\n",
    "        temperature: controls volatility (1.0 = learned vol, >1.0 = more wild)\n",
    "        historical_vol: if provided, scale noise to match this volatilty\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_out, h = self.encoder(x)\n",
    "            h_dec = h[-1]\n",
    "            dec_input = x[:, -1, :self.output_size]\n",
    "            \n",
    "            generated = []\n",
    "            for t in range(self.horizon):\n",
    "                context = self._attend(h_dec, enc_out)\n",
    "                cell_input = torch.cat([dec_input, context], dim=1)\n",
    "                h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "                out_features = torch.cat([h_dec, context], dim=1)\n",
    "                \n",
    "                mu = self.mu_head(out_features)\n",
    "                log_sigma = self.log_sigma_head(out_features)\n",
    "                \n",
    "                # Scale sigma by temperature\n",
    "                sigma = torch.exp(log_sigma) * temperature\n",
    "                \n",
    "                # Optional: override with historical volatility for first few steps\n",
    "                if historical_vol is not None and t < 5:\n",
    "                    sigma = torch.ones_like(sigma) * historical_vol\n",
    "                \n",
    "                # Ensure minimum volatility to prevent flatness\n",
    "                sigma = torch.maximum(sigma, torch.tensor(MIN_PREDICTED_VOL))\n",
    "                \n",
    "                # Sample from distribution\n",
    "                noise = torch.randn_like(mu) * sigma\n",
    "                sample = mu + noise\n",
    "                \n",
    "                generated.append(sample.unsqueeze(1))\n",
    "                dec_input = sample  # Feed back the sample (autoregressive)\n",
    "            \n",
    "            return torch.cat(generated, dim=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def nll_loss(mu, log_sigma, target):\n",
    "    \"\"\"Negative log-likelihood for Gaussian\"\"\"\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    nll = 0.5 * ((target - mu) / sigma) ** 2 + log_sigma + 0.5 * np.log(2 * np.pi)\n",
    "    return nll.mean()\n",
    "\n",
    "def candle_range_loss(mu, target):\n",
    "    pred_range = mu[:, :, 1] - mu[:, :, 2]  # High - Low\n",
    "    actual_range = target[:, :, 1] - target[:, :, 2]\n",
    "    return ((pred_range - actual_range) ** 2).mean()\n",
    "\n",
    "def volatility_match_loss(log_sigma, target):\n",
    "    \"\"\"Encourage predicted uncertainty to match actual error magnitude\"\"\"\n",
    "    pred_vol = torch.exp(log_sigma).mean()\n",
    "    actual_vol = target.std()\n",
    "    return (pred_vol - actual_vol) ** 2\n",
    "\n",
    "def directional_penalty(mu, target):\n",
    "    \"\"\"\n",
    "    v7.1: Heavy penalty for wrong direction predictions.\n",
    "    DIR_PENALTY_WEIGHT increased to 2.0 to heavily punish wrong direction.\n",
    "    \"\"\"\n",
    "    pred_close = mu[:, :, 3]\n",
    "    actual_close = target[:, :, 3]\n",
    "    sign_match = torch.sign(pred_close) * torch.sign(actual_close)\n",
    "    penalty = torch.clamp(-sign_match, min=0.0)\n",
    "    return penalty.mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def tf_ratio_for_epoch(epoch):\n",
    "    ratio = TF_START * (TF_DECAY_RATE ** (epoch - 1))\n",
    "    return max(float(TF_END), float(ratio))\n",
    "\n",
    "def run_epoch(model, loader, step_weights_t, optimizer=None, tf_ratio=0.0):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    \n",
    "    total_loss, nll_total, range_total, vol_total, dir_total = 0, 0, 0, 0, 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for xb, yb_s, yb_r in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb_s = yb_s.to(DEVICE)\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            mu, log_sigma = model(xb, y_teacher=yb_s if is_train else None, \n",
    "                                  teacher_forcing_ratio=tf_ratio if is_train else 0.0, \n",
    "                                  return_sigma=True)\n",
    "            \n",
    "            # Weighted losses\n",
    "            nll = (nll_loss(mu, log_sigma, yb_s) * step_weights_t).mean()\n",
    "            rng = candle_range_loss(mu, yb_s)\n",
    "            vol = volatility_match_loss(log_sigma, yb_s)\n",
    "            dir_pen = directional_penalty(mu, yb_s)\n",
    "            \n",
    "            # v7.1: DIR_PENALTY_WEIGHT = 2.0 (heavily punish wrong direction)\n",
    "            loss = nll + RANGE_LOSS_WEIGHT * rng + VOLATILITY_WEIGHT * vol + DIR_PENALTY_WEIGHT * dir_pen\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        nll_total += nll.item() * bs\n",
    "        range_total += rng.item() * bs\n",
    "        vol_total += vol.item() * bs\n",
    "        dir_total += dir_pen.item() * bs\n",
    "        n_items += bs\n",
    "        \n",
    "    return {\n",
    "        'total': total_loss / max(n_items, 1),\n",
    "        'nll': nll_total / max(n_items, 1),\n",
    "        'range': range_total / max(n_items, 1),\n",
    "        'vol': vol_total / max(n_items, 1),\n",
    "        'dir': dir_total / max(n_items, 1),\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, val_loader, max_epochs, patience):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    step_idx = np.arange(HORIZON, dtype=np.float32)\n",
    "    step_w = 1.0 + (step_idx / max(HORIZON - 1, 1)) ** STEP_LOSS_POWER\n",
    "    step_weights_t = torch.as_tensor(step_w, dtype=torch.float32, device=DEVICE).view(1, HORIZON, 1)\n",
    "    \n",
    "    best_val = float('inf')\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    wait = 0\n",
    "    rows = []\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tf = tf_ratio_for_epoch(epoch)\n",
    "        tr = run_epoch(model, train_loader, step_weights_t, optimizer=optimizer, tf_ratio=tf)\n",
    "        va = run_epoch(model, val_loader, step_weights_t, optimizer=None, tf_ratio=0.0)\n",
    "        \n",
    "        scheduler.step(va['total'])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        rows.append({\n",
    "            'epoch': epoch, 'tf_ratio': tf, 'lr': lr,\n",
    "            'train_total': tr['total'], 'val_total': va['total'],\n",
    "            'train_nll': tr['nll'], 'val_nll': va['nll'],\n",
    "            'train_range': tr['range'], 'val_range': va['range'],\n",
    "            'train_dir': tr['dir'], 'val_dir': va['dir'],\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} | tf={tf:.3f} | \"\n",
    "              f\"train={tr['total']:.6f} (nll={tr['nll']:.6f}, dir={tr['dir']:.4f}) | \"\n",
    "              f\"val={va['total']:.6f} (nll={va['nll']:.6f}, dir={va['dir']:.4f}) | lr={lr:.6g}\")\n",
    "        \n",
    "        if va['total'] < best_val:\n",
    "            best_val = va['total']\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}.')\n",
    "                break\n",
    "                \n",
    "    model.load_state_dict(best_state)\n",
    "    return pd.DataFrame(rows)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_metrics(actual_ohlc, pred_ohlc, prev_close):\n",
    "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
    "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
    "    ac, pc = actual_ohlc[:, 3], pred_ohlc[:, 3]\n",
    "    \n",
    "    # Directional accuracy\n",
    "    actual_direction = np.sign(ac - prev_close)\n",
    "    pred_direction = np.sign(pc - prev_close)\n",
    "    dir_acc = np.mean(actual_direction == pred_direction)\n",
    "    \n",
    "    return {\n",
    "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
    "        'close_rmse': float(np.sqrt(np.mean((ac - pc) ** 2))),\n",
    "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
    "        'directional_accuracy': float(dir_acc),\n",
    "        'directional_accuracy_eps': float(np.mean(np.sign(ac - prev_close) == np.sign(pc - prev_close))),\n",
    "        'avg_pred_close': float(np.mean(pc)),\n",
    "        'avg_actual_close': float(np.mean(ac)),\n",
    "        'close_bias': float(np.mean(pc - ac)),\n",
    "    }\n",
    "\n",
    "def evaluate_baselines(actual_ohlc, prev_ohlc, prev_close):\n",
    "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close)\n",
    "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
    "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close)\n",
    "    return {'persistence': persistence, 'flat_close_rw': flat_rw}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def predict_realistic_recursive(model, X, context_prices, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate realistic predictions using autoregressive sampling.\n",
    "    context_prices: last LOOKBACK prices to calculate realized vol for scaling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate realized volatility from context for scaling\n",
    "    log_returns = np.log(context_prices[1:] / context_prices[:-1])\n",
    "    historical_vol = float(np.std(log_returns)) if len(log_returns) > 1 else 0.001\n",
    "    \n",
    "    print(f\"Historical realized vol: {historical_vol:.6f}, Temperature: {temperature}\")\n",
    "    \n",
    "    X_tensor = torch.from_numpy(X).float().to(DEVICE)\n",
    "    \n",
    "    # Generate multiple paths for diversity (optional)\n",
    "    all_paths = []\n",
    "    for _ in range(1):  # Can increase for ensemble\n",
    "        generated = model.generate_realistic(X_tensor, temperature=temperature, \n",
    "                                           historical_vol=historical_vol)\n",
    "        all_paths.append(generated.detach().cpu().numpy())\n",
    "    \n",
    "    # Take first path (or average if multiple)\n",
    "    return all_paths[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_fold(fold_name, price_fold, window, max_epochs, patience, run_sanity=False, quick_mode=False):\n",
    "    feat_fold = build_feature_frame(price_fold)\n",
    "    target_fold = build_target_frame(feat_fold)\n",
    "    \n",
    "    input_raw = feat_fold[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "    target_raw = target_fold[TARGET_COLS].to_numpy(np.float32)\n",
    "    row_imputed = feat_fold['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "    row_open_skip = feat_fold['row_open_skip'].to_numpy(np.int8).astype(bool)\n",
    "    prev_close = feat_fold['prev_close'].to_numpy(np.float32)\n",
    "    price_vals = price_fold.loc[feat_fold.index, OHLC_COLS].to_numpy(np.float32)\n",
    "    \n",
    "    tr_end, va_end = split_points(len(input_raw))\n",
    "    \n",
    "    # Standardize inputs only\n",
    "    in_mean, in_std = input_raw[:tr_end].mean(axis=0), input_raw[:tr_end].std(axis=0)\n",
    "    in_std = np.where(in_std < 1e-8, 1.0, in_std)\n",
    "    input_scaled = (input_raw - in_mean) / in_std\n",
    "    \n",
    "    # No target scaling (raw returns)\n",
    "    tg_mean, tg_std = np.zeros(4, dtype=np.float32), np.ones(4, dtype=np.float32)\n",
    "    target_scaled = target_raw.copy()\n",
    "    \n",
    "    X_all, y_all_s, y_all_r, starts, prev_close_starts, dropped_imputed, dropped_skip = make_multistep_windows(\n",
    "        input_scaled, target_scaled, target_raw, row_imputed, row_open_skip, prev_close, window, HORIZON\n",
    "    )\n",
    "    \n",
    "    if len(X_all) == 0:\n",
    "        raise RuntimeError(f'{fold_name}: no windows available.')\n",
    "    \n",
    "    # Splits\n",
    "    end_idx = starts + HORIZON - 1\n",
    "    tr_m, va_m, te_m = end_idx < tr_end, (end_idx >= tr_end) & (end_idx < va_end), end_idx >= va_end\n",
    "    \n",
    "    X_train, y_train_s, y_train_r = X_all[tr_m], y_all_s[tr_m], y_all_r[tr_m]\n",
    "    X_val, y_val_s, y_val_r = X_all[va_m], y_all_s[va_m], y_all_r[va_m]\n",
    "    X_test, y_test_s, y_test_r = X_all[te_m], y_all_s[te_m], y_all_r[te_m]\n",
    "    test_starts = starts[te_m]\n",
    "    test_prev_close = prev_close_starts[te_m]\n",
    "    \n",
    "    print(f'Samples: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}')\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), \n",
    "                             batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), \n",
    "                           batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = Seq2SeqAttnGRU(\n",
    "        input_size=X_train.shape[-1],\n",
    "        output_size=len(TARGET_COLS),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    hist = train_model(model, train_loader, val_loader, max_epochs, patience)\n",
    "    \n",
    "    # REALISTIC PREDICTION (autoregressive with noise)\n",
    "    # Pick last test sample for visualization\n",
    "    last_idx = len(X_test) - 1\n",
    "    X_last = X_test[last_idx:last_idx+1]\n",
    "    context_start = int(test_starts[last_idx]) - window\n",
    "    context_prices = price_vals[context_start:int(test_starts[last_idx]), 3]  # Close prices\n",
    "    \n",
    "    # Generate with temperature > 1 for more realistic wild market movement\n",
    "    pred_rets_realistic = predict_realistic_recursive(\n",
    "        model, X_last, context_prices, temperature=SAMPLING_TEMPERATURE\n",
    "    )[0]  # [horizon, 4]\n",
    "    \n",
    "    # Convert to prices\n",
    "    last_close = float(test_prev_close[last_idx])\n",
    "    pred_price_realistic = returns_to_prices_seq(pred_rets_realistic, last_close)\n",
    "    \n",
    "    # Actual future\n",
    "    actual_future = price_vals[int(test_starts[last_idx]):int(test_starts[last_idx])+HORIZON]\n",
    "    \n",
    "    # One-step metrics (deterministic for comparison)\n",
    "    mu_test = model(torch.from_numpy(X_test).float().to(DEVICE)).detach().cpu().numpy()\n",
    "    pred_step1_ret = mu_test[:, 0, :]\n",
    "    actual_step1_ret = y_test_r[:, 0, :]\n",
    "    \n",
    "    # Simple price reconstruction for metrics\n",
    "    pred_ohlc_1 = np.zeros((len(test_starts), 4))\n",
    "    for i in range(len(test_starts)):\n",
    "        pc = test_prev_close[i]\n",
    "        pred_ohlc_1[i] = [\n",
    "            pc * np.exp(pred_step1_ret[i, 0]),\n",
    "            pc * np.exp(pred_step1_ret[i, 1]),\n",
    "            pc * np.exp(pred_step1_ret[i, 2]),\n",
    "            pc * np.exp(pred_step1_ret[i, 3]),\n",
    "        ]\n",
    "        pred_ohlc_1[i] = enforce_candle_validity(pred_ohlc_1[i].reshape(1, -1))[0]\n",
    "    \n",
    "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
    "    prev_ohlc = price_vals[test_starts]\n",
    "    \n",
    "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, test_prev_close)\n",
    "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, test_prev_close)\n",
    "    \n",
    "    print(f\"\\nRealistic prediction stats:\")\n",
    "    print(f\"  Pred range: [{pred_price_realistic[:, 3].min():.2f}, {pred_price_realistic[:, 3].max():.2f}]\")\n",
    "    print(f\"  Actual range: [{actual_future[:, 3].min():.2f}, {actual_future[:, 3].max():.2f}]\")\n",
    "    print(f\"  Pred volatility: {np.std(pred_rets_realistic[:, 3]):.6f}\")\n",
    "    print(f\"  Actual volatility: {np.std(actual_step1_ret[:, 3]):.6f}\")\n",
    "    \n",
    "    # Build DataFrames for plotting\n",
    "    future_idx = price_fold.index[test_starts[last_idx]:test_starts[last_idx]+HORIZON]\n",
    "    pred_future_df = pd.DataFrame(pred_price_realistic, index=future_idx, columns=OHLC_COLS)\n",
    "    actual_future_df = pd.DataFrame(actual_future, index=future_idx, columns=OHLC_COLS)\n",
    "    context_df = price_fold.iloc[test_starts[last_idx]-window:test_starts[last_idx]+1][OHLC_COLS]\n",
    "    \n",
    "    return {\n",
    "        'fold': fold_name,\n",
    "        'window': window,\n",
    "        'history_df': hist,\n",
    "        'model_metrics': model_metrics,\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'context_df': context_df,\n",
    "        'actual_future_df': actual_future_df,\n",
    "        'pred_future_df': pred_future_df,\n",
    "        'samples': {'train': len(X_train), 'val': len(X_val), 'test': len(X_test)},\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run lookback sweep if enabled\n",
    "fold_results = []\n",
    "primary_slice = slices[0]\n",
    "selected_window = DEFAULT_LOOKBACK\n",
    "\n",
    "if ENABLE_LOOKBACK_SWEEP:\n",
    "    print('\\n=== Lookback sweep ===')\n",
    "    _, a0, b0 = primary_slice\n",
    "    fold_price0 = price_df.iloc[a0:b0].copy()\n",
    "    \n",
    "    best_score = -float('inf')\n",
    "    for w in LOOKBACK_CANDIDATES:\n",
    "        print(f'\\nSweep candidate lookback={w} --')\n",
    "        try:\n",
    "            r = run_fold(f'sweep_w{w}', fold_price0, w, SWEEP_MAX_EPOCHS, SWEEP_PATIENCE, quick_mode=True)\n",
    "            score = -r['model_metrics']['close_mae']  # Simple selection\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                selected_window = w\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for window {w}: {e}\")\n",
    "\n",
    "print(f'\\nSelected lookback: {selected_window}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run full walk-forward with realistic generation\n",
    "print('\\n=== Full walk-forward with realistic generation ===')\n",
    "for i, (name, a, b) in enumerate(slices, start=1):\n",
    "    print(f'\\n=== Running {name} [{a}:{b}] lookback={selected_window} ===')\n",
    "    fold_price = price_df.iloc[a:b].copy()\n",
    "    try:\n",
    "        res = run_fold(name, fold_price, selected_window, FINAL_MAX_EPOCHS, FINAL_PATIENCE)\n",
    "        fold_results.append(res)\n",
    "        \n",
    "        print(f\"\\nResults for {name}:\")\n",
    "        print(f\"  Model MAE: {res['model_metrics']['close_mae']:.4f}\")\n",
    "        print(f\"  Persistence MAE: {res['baseline_metrics']['persistence']['close_mae']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fold {name}: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8), facecolor='black')\n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    def draw_candles(ax, ohlc, start_x, up_edge, up_face, down_edge, down_face, wick_color, width=0.6, alpha=1.0):\n",
    "        vals = ohlc[OHLC_COLS].to_numpy()\n",
    "        for i, (o, h, l, c) in enumerate(vals):\n",
    "            x = start_x + i\n",
    "            bull = c >= o\n",
    "            ax.vlines(x, l, h, color=wick_color, linewidth=1.0, alpha=alpha, zorder=2)\n",
    "            lower = min(o, c)\n",
    "            height = max(abs(c - o), 1e-6)\n",
    "            rect = Rectangle((x - width/2, lower), width, height,\n",
    "                           facecolor=up_face if bull else down_face,\n",
    "                           edgecolor=up_edge if bull else down_edge,\n",
    "                           linewidth=1.0, alpha=alpha, zorder=3)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    context_df = latest['context_df']\n",
    "    actual_future_df = latest['actual_future_df']\n",
    "    pred_future_df = latest['pred_future_df']\n",
    "    \n",
    "    # Draw history (green/red)\n",
    "    draw_candles(ax, context_df, 0, '#00FF00', '#00FF00', '#FF0000', '#FF0000', '#FFFFFF', width=0.6, alpha=0.9)\n",
    "    \n",
    "    # Draw actual future (dimmed)\n",
    "    draw_candles(ax, actual_future_df, len(context_df), '#00AA00', '#00AA00', '#AA0000', '#AA0000', '#888888', \n",
    "                 width=0.6, alpha=0.6)\n",
    "    \n",
    "    # Draw realistic prediction (bright white/black with glow effect)\n",
    "    draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#888888', '#000000', '#FFFFFF',\n",
    "                 width=0.5, alpha=1.0)\n",
    "    \n",
    "    ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=1.0, alpha=0.8)\n",
    "    \n",
    "    # Labels\n",
    "    n = len(context_df) + len(actual_future_df)\n",
    "    step = max(1, n // 12)\n",
    "    ticks = list(range(0, n, step))\n",
    "    all_idx = context_df.index.append(actual_future_df.index)\n",
    "    labels = [all_idx[i].strftime('%m-%d %H:%M') for i in ticks if i < len(all_idx)]\n",
    "    \n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=30, ha='right', color='white', fontsize=9)\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_color('#666666')\n",
    "    ax.grid(color='#333333', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(f'MSFT 1m ({latest[\"fold\"]}) - Trend-Aware Forecast v7.1 (Temp={SAMPLING_TEMPERATURE})', \n",
    "                 color='white', fontsize=14, pad=15)\n",
    "    ax.set_ylabel('Price', color='white', fontsize=12)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#00FF00', edgecolor='#00FF00', label='History (bull)'),\n",
    "        Patch(facecolor='#FF0000', edgecolor='#FF0000', label='History (bear)'),\n",
    "        Patch(facecolor='#00AA00', edgecolor='#00AA00', label='Actual Future (dim)'),\n",
    "        Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted (bull)'),\n",
    "        Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted (bear)'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, facecolor='black', edgecolor='white', labelcolor='white', \n",
    "             loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal chart shows realistic candle generation with temperature {SAMPLING_TEMPERATURE}\")\n",
    "    print(\"Increase SAMPLING_TEMPERATURE for more volatile predictions, decrease for smoother trends.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell: Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test Cell: Validate Model Performance\n",
    "# This cell prints:\n",
    "# 1. Directional accuracy vs persistence baseline\n",
    "# 2. Average predicted close vs actual close (bias)\n",
    "# 3. Visual confirmation that candles have realistic wicks/bodies\n",
    "\n",
    "if fold_results:\n",
    "    latest = fold_results[-1]\n",
    "    model_metrics = latest['model_metrics']\n",
    "    baseline_metrics = latest['baseline_metrics']\n",
    "    \n",
    "    print('='*60)\n",
    "    print('v7.1 TREND-AWARE MODEL VALIDATION RESULTS')\n",
    "    print('='*60)\n",
    "    \n",
    "    # 1. Directional accuracy vs persistence baseline\n",
    "    print('\\n1. DIRECTIONAL ACCURACY:')\n",
    "    print(f\"   Model:           {model_metrics['directional_accuracy']:.2%}\")\n",
    "    print(f\"   Persistence:     {baseline_metrics['persistence']['directional_accuracy']:.2%}\")\n",
    "    improvement = model_metrics['directional_accuracy'] - baseline_metrics['persistence']['directional_accuracy']\n",
    "    print(f\"   Improvement:     {improvement:+.2%}\")\n",
    "    \n",
    "    # 2. Bias check\n",
    "    print('\\n2. BIAS (Average Predicted vs Actual Close):')\n",
    "    print(f\"   Avg Predicted:   {model_metrics['avg_pred_close']:.4f}\")\n",
    "    print(f\"   Avg Actual:      {model_metrics['avg_actual_close']:.4f}\")\n",
    "    print(f\"   Bias:            {model_metrics['close_bias']:+.4f}\")\n",
    "    \n",
    "    # 3. Candle validity check\n",
    "    print('\\n3. CANDLE VALIDITY CHECK:')\n",
    "    pred_df = latest['pred_future_df']\n",
    "    valid_candles = 0\n",
    "    total_candles = len(pred_df)\n",
    "    \n",
    "    wick_stats = {'body_avg': 0.0, 'upper_wick_avg': 0.0, 'lower_wick_avg': 0.0}\n",
    "    \n",
    "    for idx, row in pred_df.iterrows():\n",
    "        o, h, l, c = row['Open'], row['High'], row['Low'], row['Close']\n",
    "        \n",
    "        # Check validity: High >= max(Open, Close) and Low <= min(Open, Close)\n",
    "        if h >= max(o, c) and l <= min(o, c):\n",
    "            valid_candles += 1\n",
    "        \n",
    "        # Calculate wick statistics\n",
    "        body = abs(c - o)\n",
    "        upper_wick = h - max(o, c)\n",
    "        lower_wick = min(o, c) - l\n",
    "        wick_stats['body_avg'] += body\n",
    "        wick_stats['upper_wick_avg'] += upper_wick\n",
    "        wick_stats['lower_wick_avg'] += lower_wick\n",
    "    \n",
    "    wick_stats['body_avg'] /= total_candles\n",
    "    wick_stats['upper_wick_avg'] /= total_candles\n",
    "    wick_stats['lower_wick_avg'] /= total_candles\n",
    "    \n",
    "    print(f\"   Valid Candles:   {valid_candles}/{total_candles} ({valid_candles/total_candles:.1%})\")\n",
    "    print(f\"   Avg Body Size:   ${wick_stats['body_avg']:.4f}\")\n",
    "    print(f\"   Avg Upper Wick:  ${wick_stats['upper_wick_avg']:.4f}\")\n",
    "    print(f\"   Avg Lower Wick:  ${wick_stats['lower_wick_avg']:.4f}\")\n",
    "    \n",
    "    # Visual confirmation\n",
    "    print('\\n4. VISUAL CONFIRMATION:')\n",
    "    if wick_stats['body_avg'] > 0 and (wick_stats['upper_wick_avg'] > 0 or wick_stats['lower_wick_avg'] > 0):\n",
    "        print(\"   [PASS] Candles have realistic wicks and bodies\")\n",
    "    else:\n",
    "        print(\"   [WARN] Candles may lack realistic wick structure\")\n",
    "    \n",
    "    if model_metrics['directional_accuracy'] > baseline_metrics['persistence']['directional_accuracy']:\n",
    "        print(\"   [PASS] Model beats persistence baseline on direction\")\n",
    "    else:\n",
    "        print(\"   [WARN] Model does not beat persistence on direction\")\n",
    "    \n",
    "    if abs(model_metrics['close_bias']) < 0.5:\n",
    "        print(\"   [PASS] Low bias in predictions\")\n",
    "    else:\n",
    "        print(f\"   [WARN] Significant bias detected: {model_metrics['close_bias']:+.4f}\")\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "else:\n",
    "    print(\"No fold results available. Please run the training cells first.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}