{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: MSFT 1-Minute GRU Forecast (Anti-Collapse v2)\n",
        "\n",
        "This notebook implements all requested fixes:\n",
        "1. Exclude windows touching imputed bars (no synthetic-bar contamination)\n",
        "2. Multi-step training objective with scheduled sampling (h=15)\n",
        "3. Relaxed clipping bounds for recursive rollout\n",
        "4. Larger model + lower regularization\n",
        "5. Auxiliary direction loss (BCE)\n",
        "6. Directional diagnostics with epsilon threshold\n",
        "\n",
        "It also adds volume/order-flow features to the model input state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional dependency bootstrap\n",
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "required = {\n",
        "    'alpaca': 'alpaca-py',\n",
        "    'numpy': 'numpy',\n",
        "    'pandas': 'pandas',\n",
        "    'matplotlib': 'matplotlib',\n",
        "}\n",
        "\n",
        "missing = [pkg for module_name, pkg in required.items() if importlib.util.find_spec(module_name) is None]\n",
        "if missing:\n",
        "    print('Installing missing packages:', missing)\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', *missing])\n",
        "else:\n",
        "    print('All required third-party packages are already installed.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from alpaca.data.enums import DataFeed\n",
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests import StockBarsRequest\n",
        "from alpaca.data.timeframe import TimeFrame\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Patch, Rectangle\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f'Using device: {DEVICE}')\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('CUDA:', torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config\n",
        "\n",
        "- Feed default: `iex` (free-plan friendly)\n",
        "- Sessionization: weekday RTH 09:30-15:59 ET\n",
        "- Forecast mode: recursive 15-step\n",
        "- Target: transformed candle-price features + volume/order-flow features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "SYMBOL = 'MSFT'\n",
        "LOOKBACK_DAYS = 60\n",
        "\n",
        "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
        "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
        "\n",
        "FEATURE_COLS = [\n",
        "    'rOpen',\n",
        "    'rHigh',\n",
        "    'rLow',\n",
        "    'rClose',\n",
        "    'logVolChange',\n",
        "    'logTradeCountChange',\n",
        "    'vwapDelta',\n",
        "    'rangeFrac',\n",
        "    'orderFlowProxy',\n",
        "    'tickPressure',\n",
        "]\n",
        "\n",
        "WINDOW = 500\n",
        "HORIZON = 15\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO = 0.15\n",
        "\n",
        "# Model/training (item 4)\n",
        "BATCH_SIZE = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.0\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "MAX_EPOCHS = 40\n",
        "EARLY_STOPPING_PATIENCE = 8\n",
        "\n",
        "# Losses (item 5)\n",
        "LOSS_WEIGHTS = np.array([1.0, 1.0, 1.0, 2.0, 0.4, 0.4, 0.3, 0.5, 0.6, 0.6], dtype=np.float32)\n",
        "HUBER_DELTA = 1.0\n",
        "DIR_LOSS_WEIGHT = 0.25\n",
        "\n",
        "# Scheduled sampling (item 2)\n",
        "TF_START = 1.0\n",
        "TF_END = 0.2\n",
        "\n",
        "# Clipping (item 3): wider bounds than previous run\n",
        "CLIP_TARGET_QUANTILES = (0.001, 0.999)\n",
        "\n",
        "# Direction diagnostics (item 6)\n",
        "DIRECTION_EPS = 0.02\n",
        "\n",
        "# Alpaca / session\n",
        "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
        "USE_RTH_ONLY = True\n",
        "SESSION_TZ = 'America/New_York'\n",
        "RTH_START = '09:30'\n",
        "RTH_END = '16:00'\n",
        "REQUEST_CHUNK_DAYS = 5\n",
        "MAX_REQUESTS_PER_MINUTE = 120\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "print({\n",
        "    'symbol': SYMBOL,\n",
        "    'lookback_days': LOOKBACK_DAYS,\n",
        "    'features': len(FEATURE_COLS),\n",
        "    'window': WINDOW,\n",
        "    'horizon': HORIZON,\n",
        "    'alpaca_feed': ALPACA_FEED,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'layers': NUM_LAYERS,\n",
        "    'dropout': DROPOUT,\n",
        "    'tf_schedule': (TF_START, TF_END),\n",
        "    'clip_quantiles': CLIP_TARGET_QUANTILES,\n",
        "    'direction_eps': DIRECTION_EPS,\n",
        "    'device': str(DEVICE),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data pull + sessionization with imputation flags\n",
        "class RequestPacer:\n",
        "    def __init__(self, max_calls_per_minute: int):\n",
        "        if max_calls_per_minute <= 0:\n",
        "            raise ValueError('max_calls_per_minute must be > 0')\n",
        "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
        "        self.last_call_ts = 0.0\n",
        "\n",
        "    def wait(self) -> None:\n",
        "        now = time.monotonic()\n",
        "        elapsed = now - self.last_call_ts\n",
        "        if elapsed < self.min_interval:\n",
        "            time.sleep(self.min_interval - elapsed)\n",
        "        self.last_call_ts = time.monotonic()\n",
        "\n",
        "\n",
        "def _require_alpaca_credentials() -> tuple[str, str]:\n",
        "    api_key = os.getenv('ALPACA_API_KEY')\n",
        "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
        "    if not api_key or not secret_key:\n",
        "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY environment variables.')\n",
        "    return api_key, secret_key\n",
        "\n",
        "\n",
        "def _resolve_feed(feed_name: str) -> DataFeed:\n",
        "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
        "    k = feed_name.strip().lower()\n",
        "    if k not in mapping:\n",
        "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
        "    return mapping[k]\n",
        "\n",
        "\n",
        "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
        "    api_key, secret_key = _require_alpaca_credentials()\n",
        "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
        "\n",
        "    feed = _resolve_feed(ALPACA_FEED)\n",
        "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
        "\n",
        "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
        "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
        "        end_ts = end_ts - timedelta(minutes=20)\n",
        "    start_ts = end_ts - timedelta(days=lookback_days)\n",
        "\n",
        "    frames = []\n",
        "    cursor = start_ts\n",
        "    calls = 0\n",
        "\n",
        "    while cursor < end_ts:\n",
        "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
        "        chunk = None\n",
        "\n",
        "        for attempt in range(1, MAX_RETRIES + 1):\n",
        "            pacer.wait()\n",
        "            calls += 1\n",
        "            try:\n",
        "                req = StockBarsRequest(\n",
        "                    symbol_or_symbols=[symbol],\n",
        "                    timeframe=TimeFrame.Minute,\n",
        "                    start=cursor,\n",
        "                    end=chunk_end,\n",
        "                    feed=feed,\n",
        "                    limit=10_000,\n",
        "                )\n",
        "                chunk = client.get_stock_bars(req).df\n",
        "                break\n",
        "            except Exception as exc:\n",
        "                msg = str(exc).lower()\n",
        "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
        "                    backoff = min(2 ** attempt, 30)\n",
        "                    print(f'Rate-limited; backoff {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
        "                    time.sleep(backoff)\n",
        "                    continue\n",
        "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
        "                    raise RuntimeError('Requested feed unavailable. Use ALPACA_FEED=iex for free-plan compatibility.') from exc\n",
        "                raise\n",
        "\n",
        "        if chunk is not None and not chunk.empty:\n",
        "            d = chunk.reset_index().rename(\n",
        "                columns={\n",
        "                    'timestamp': 'Datetime',\n",
        "                    'open': 'Open',\n",
        "                    'high': 'High',\n",
        "                    'low': 'Low',\n",
        "                    'close': 'Close',\n",
        "                    'volume': 'Volume',\n",
        "                    'trade_count': 'TradeCount',\n",
        "                    'vwap': 'VWAP',\n",
        "                }\n",
        "            )\n",
        "\n",
        "            if 'VWAP' not in d.columns:\n",
        "                d['VWAP'] = d.get('Close')\n",
        "            if 'TradeCount' not in d.columns:\n",
        "                d['TradeCount'] = 0\n",
        "            if 'Volume' not in d.columns:\n",
        "                d['Volume'] = 0\n",
        "\n",
        "            keep = ['Datetime'] + RAW_COLS\n",
        "            missing = [c for c in keep if c not in d.columns]\n",
        "            if missing:\n",
        "                raise RuntimeError(f'Alpaca response missing columns: {missing}')\n",
        "\n",
        "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
        "            d = d[keep].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
        "            frames.append(d)\n",
        "\n",
        "        cursor = chunk_end\n",
        "\n",
        "    if not frames:\n",
        "        raise RuntimeError('No bars returned from Alpaca.')\n",
        "\n",
        "    df = pd.concat(frames, axis=0).sort_index()\n",
        "    df = df[~df.index.duplicated(keep='last')]\n",
        "    return df.astype(np.float32), calls\n",
        "\n",
        "\n",
        "def sessionize(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
        "    if df_utc.empty:\n",
        "        raise RuntimeError('Input bars are empty.')\n",
        "\n",
        "    local = df_utc.copy()\n",
        "    idx = pd.DatetimeIndex(local.index)\n",
        "    if idx.tz is None:\n",
        "        idx = idx.tz_localize('UTC')\n",
        "    idx = idx.tz_convert(SESSION_TZ)\n",
        "    local.index = idx\n",
        "\n",
        "    local = local[local.index.dayofweek < 5]\n",
        "    if USE_RTH_ONLY:\n",
        "        local = local.between_time(RTH_START, RTH_END, inclusive='left')\n",
        "\n",
        "    if local.empty:\n",
        "        raise RuntimeError('No rows after weekday/RTH filtering.')\n",
        "\n",
        "    session_dates = pd.Index(local.index.date).unique().tolist()\n",
        "    raw_counts = {d: int((local.index.date == d).sum()) for d in session_dates}\n",
        "\n",
        "    latest_date = max(raw_counts.keys())\n",
        "    drop_latest = raw_counts[latest_date] < 390\n",
        "\n",
        "    pieces = []\n",
        "    fill_counts = []\n",
        "\n",
        "    for d in session_dates:\n",
        "        if drop_latest and d == latest_date:\n",
        "            continue\n",
        "\n",
        "        day = local[local.index.date == d][RAW_COLS].copy()\n",
        "        day_start = pd.Timestamp(f'{d} {RTH_START}', tz=SESSION_TZ)\n",
        "        exp_idx = pd.date_range(day_start, periods=390, freq='1min')\n",
        "\n",
        "        day = day.reindex(exp_idx)\n",
        "        imputed = day['Open'].isna() | day['High'].isna() | day['Low'].isna() | day['Close'].isna()\n",
        "        fill_counts.append(int(imputed.sum()))\n",
        "\n",
        "        # Fill OHLC/VWAP for continuity, mark imputed so they can be excluded from windows.\n",
        "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
        "        day['Volume'] = day['Volume'].fillna(0.0)\n",
        "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
        "        day['is_imputed'] = imputed.astype(np.int8)\n",
        "\n",
        "        if day[RAW_COLS].isna().any().any():\n",
        "            raise RuntimeError(f'Session {d} still has NaNs after fill.')\n",
        "\n",
        "        pieces.append(day)\n",
        "\n",
        "    if not pieces:\n",
        "        raise RuntimeError('No complete sessions after filtering.')\n",
        "\n",
        "    out = pd.concat(pieces, axis=0).sort_index()\n",
        "\n",
        "    sizes = out.groupby(out.index.date).size()\n",
        "    assert bool((sizes == 390).all()), 'Each kept session must have 390 bars.'\n",
        "\n",
        "    out.index = out.index.tz_localize(None)\n",
        "\n",
        "    meta = {\n",
        "        'raw_sessions': len(session_dates),\n",
        "        'kept_sessions': int(len(sizes)),\n",
        "        'dropped_latest_session': bool(drop_latest),\n",
        "        'latest_session_raw_count': int(raw_counts[latest_date]),\n",
        "        'avg_filled_bars_per_session': float(np.mean(fill_counts)),\n",
        "        'max_filled_bars_in_session': int(np.max(fill_counts)),\n",
        "    }\n",
        "    return out.astype(np.float32), meta\n",
        "\n",
        "\n",
        "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
        "price_df, session_meta = sessionize(raw_df_utc)\n",
        "\n",
        "print(f'Raw rows from Alpaca: {len(raw_df_utc):,}')\n",
        "print(f'Sessionized rows: {len(price_df):,}')\n",
        "print(f'Alpaca API calls: {api_calls} (<= {MAX_REQUESTS_PER_MINUTE}/min target)')\n",
        "print('Session meta:', session_meta)\n",
        "\n",
        "if len(price_df) < WINDOW + HORIZON + 1000:\n",
        "    raise RuntimeError('Not enough rows for robust multi-step training.')\n",
        "\n",
        "display(price_df.head(3))\n",
        "display(price_df.tail(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering (includes volume/order-flow) + transform checks\n",
        "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
        "    out = np.asarray(ohlc, dtype=np.float32).copy()\n",
        "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
        "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
        "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
        "    return out\n",
        "\n",
        "\n",
        "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
        "    seq = []\n",
        "    prev_close = float(last_close)\n",
        "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
        "        o = prev_close * np.exp(float(rO))\n",
        "        h = prev_close * np.exp(float(rH))\n",
        "        l = prev_close * np.exp(float(rL))\n",
        "        c = prev_close * np.exp(float(rC))\n",
        "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
        "        seq.append(cand)\n",
        "        prev_close = float(cand[3])\n",
        "    return np.asarray(seq, dtype=np.float32)\n",
        "\n",
        "\n",
        "def one_step_returns_to_prices_batch(return_ohlc: np.ndarray, prev_close: np.ndarray) -> np.ndarray:\n",
        "    r = np.asarray(return_ohlc, dtype=np.float32)\n",
        "    p = np.asarray(prev_close, dtype=np.float32)\n",
        "\n",
        "    out = np.stack([\n",
        "        p * np.exp(r[:, 0]),\n",
        "        p * np.exp(r[:, 1]),\n",
        "        p * np.exp(r[:, 2]),\n",
        "        p * np.exp(r[:, 3]),\n",
        "    ], axis=1).astype(np.float32)\n",
        "    return enforce_candle_validity(out)\n",
        "\n",
        "\n",
        "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    eps = 1e-9\n",
        "\n",
        "    prev_close = df['Close'].shift(1)\n",
        "    prev_vol = df['Volume'].shift(1)\n",
        "    prev_tc = df['TradeCount'].shift(1)\n",
        "\n",
        "    valid = prev_close.notna() & prev_vol.notna() & prev_tc.notna()\n",
        "\n",
        "    c = df.loc[valid]\n",
        "    base_close = prev_close.loc[valid]\n",
        "\n",
        "    out = pd.DataFrame(index=c.index, dtype=np.float32)\n",
        "\n",
        "    # Candle-price returns (core target components)\n",
        "    out['rOpen'] = np.log(c['Open'] / (base_close + eps))\n",
        "    out['rHigh'] = np.log(c['High'] / (base_close + eps))\n",
        "    out['rLow'] = np.log(c['Low'] / (base_close + eps))\n",
        "    out['rClose'] = np.log(c['Close'] / (base_close + eps))\n",
        "\n",
        "    # Volume/order-flow style features\n",
        "    out['logVolChange'] = np.log((c['Volume'] + 1.0) / (prev_vol.loc[valid] + 1.0))\n",
        "    out['logTradeCountChange'] = np.log((c['TradeCount'] + 1.0) / (prev_tc.loc[valid] + 1.0))\n",
        "    out['vwapDelta'] = np.log((c['VWAP'] + eps) / (c['Close'] + eps))\n",
        "    out['rangeFrac'] = (c['High'] - c['Low']) / (base_close + eps)\n",
        "\n",
        "    signed_body = (c['Close'] - c['Open']) / ((c['High'] - c['Low']) + eps)\n",
        "    out['orderFlowProxy'] = signed_body * np.log1p(c['Volume'])\n",
        "    out['tickPressure'] = np.sign(c['Close'] - c['Open']) * np.log1p(c['TradeCount'])\n",
        "\n",
        "    # Mark feature rows influenced by imputed bars in either t-1 or t (item 1).\n",
        "    imp = df['is_imputed'].astype(bool).to_numpy()\n",
        "    imp_feat = imp[1:] | imp[:-1]\n",
        "    out['invalid_feature_row'] = imp_feat.astype(np.int8)\n",
        "\n",
        "    # Prev close aligned for reconstructing price from predicted returns.\n",
        "    out['prev_close'] = base_close.astype(np.float32)\n",
        "\n",
        "    return out.astype(np.float32)\n",
        "\n",
        "\n",
        "feat_df = build_feature_frame(price_df)\n",
        "print('Feature rows:', len(feat_df))\n",
        "print('Invalid feature rows (imputation-affected):', int(feat_df['invalid_feature_row'].sum()))\n",
        "\n",
        "# Round-trip check for OHLC return transform component\n",
        "round_n = min(2000, len(price_df))\n",
        "sub = price_df.iloc[:round_n].copy()\n",
        "sub_feat = build_feature_frame(sub)\n",
        "ret = sub_feat[['rOpen', 'rHigh', 'rLow', 'rClose']].to_numpy(np.float32)\n",
        "recon = returns_to_prices_seq(ret, float(sub['Close'].iloc[0]))\n",
        "actual = sub[OHLC_COLS].iloc[1:].to_numpy(np.float32)\n",
        "\n",
        "rt_max = float(np.max(np.abs(recon - actual)))\n",
        "rt_mean = float(np.mean(np.abs(recon - actual)))\n",
        "print({'roundtrip_max_abs_error': rt_max, 'roundtrip_mean_abs_error': rt_mean})\n",
        "assert rt_max < 2e-3, 'Round-trip transform error too large.'\n",
        "\n",
        "display(feat_df[FEATURE_COLS + ['invalid_feature_row', 'prev_close']].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowing, splits, and dataset\n",
        "def fit_standardizer(train_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
        "    mean = train_values.mean(axis=0)\n",
        "    std = train_values.std(axis=0)\n",
        "    std = np.where(std < 1e-8, 1.0, std)\n",
        "    return mean.astype(np.float32), std.astype(np.float32)\n",
        "\n",
        "\n",
        "def apply_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return ((values - mean) / std).astype(np.float32)\n",
        "\n",
        "\n",
        "def undo_standardizer(values: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return (values * std + mean).astype(np.float32)\n",
        "\n",
        "\n",
        "def build_walkforward_slices(price_df_full: pd.DataFrame) -> list[tuple[str, int, int]]:\n",
        "    n = len(price_df_full)\n",
        "    span = int(round(n * 0.85))\n",
        "    shift = max(1, n - span)\n",
        "\n",
        "    cands = [('slice_1', 0, min(span, n)), ('slice_2', shift, min(shift + span, n))]\n",
        "\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for name, a, b in cands:\n",
        "        key = (a, b)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        if b - a < WINDOW + HORIZON + 1500:\n",
        "            continue\n",
        "        out.append((name, a, b))\n",
        "        seen.add(key)\n",
        "\n",
        "    if not out:\n",
        "        raise RuntimeError('Unable to create walk-forward slices with enough rows.')\n",
        "    return out\n",
        "\n",
        "\n",
        "def make_multistep_windows(\n",
        "    values_scaled: np.ndarray,\n",
        "    values_raw: np.ndarray,\n",
        "    invalid_rows: np.ndarray,\n",
        "    window: int,\n",
        "    horizon: int,\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    X = []\n",
        "    y_scaled = []\n",
        "    y_raw = []\n",
        "    starts = []\n",
        "    dropped_invalid = 0\n",
        "\n",
        "    n = len(values_scaled)\n",
        "    for i in range(window, n - horizon + 1):\n",
        "        # Exclude any sample whose input or target touches imputation-affected rows (item 1).\n",
        "        if invalid_rows[i - window : i + horizon].any():\n",
        "            dropped_invalid += 1\n",
        "            continue\n",
        "\n",
        "        X.append(values_scaled[i - window : i])\n",
        "        y_scaled.append(values_scaled[i : i + horizon])\n",
        "        y_raw.append(values_raw[i : i + horizon])\n",
        "        starts.append(i)\n",
        "\n",
        "    return (\n",
        "        np.asarray(X, dtype=np.float32),\n",
        "        np.asarray(y_scaled, dtype=np.float32),\n",
        "        np.asarray(y_raw, dtype=np.float32),\n",
        "        np.asarray(starts, dtype=np.int64),\n",
        "        dropped_invalid,\n",
        "    )\n",
        "\n",
        "\n",
        "class MultiStepDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y_scaled: np.ndarray, y_raw: np.ndarray):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y_scaled = torch.from_numpy(y_scaled).float()\n",
        "        self.y_raw = torch.from_numpy(y_raw).float()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.X[idx], self.y_scaled[idx], self.y_raw[idx]\n",
        "\n",
        "\n",
        "slices = build_walkforward_slices(price_df)\n",
        "print('Walk-forward slices:', slices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and optimization (multi-step + scheduled sampling + direction head)\n",
        "class Seq2SeqGRU(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float, horizon: int):\n",
        "        super().__init__()\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.encoder = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        self.decoder_cell = nn.GRUCell(input_size, hidden_size)\n",
        "        self.feature_head = nn.Linear(hidden_size, input_size)\n",
        "        self.direction_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        y_teacher: torch.Tensor | None = None,\n",
        "        teacher_forcing_ratio: float = 0.0,\n",
        "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        _, h = self.encoder(x)\n",
        "        h_dec = h[-1]\n",
        "\n",
        "        dec_input = x[:, -1, :]\n",
        "\n",
        "        feats = []\n",
        "        dirs = []\n",
        "\n",
        "        for t in range(self.horizon):\n",
        "            h_dec = self.decoder_cell(dec_input, h_dec)\n",
        "            feat_pred = self.feature_head(h_dec)\n",
        "            dir_logit = self.direction_head(h_dec).squeeze(-1)\n",
        "\n",
        "            feats.append(feat_pred.unsqueeze(1))\n",
        "            dirs.append(dir_logit.unsqueeze(1))\n",
        "\n",
        "            if y_teacher is not None:\n",
        "                if teacher_forcing_ratio >= 1.0:\n",
        "                    dec_input = y_teacher[:, t, :]\n",
        "                elif teacher_forcing_ratio <= 0.0:\n",
        "                    dec_input = feat_pred\n",
        "                else:\n",
        "                    m = (torch.rand(x.size(0), device=x.device) < teacher_forcing_ratio).unsqueeze(1)\n",
        "                    dec_input = torch.where(m, y_teacher[:, t, :], feat_pred)\n",
        "            else:\n",
        "                dec_input = feat_pred\n",
        "\n",
        "        feat_seq = torch.cat(feats, dim=1)\n",
        "        dir_seq = torch.cat(dirs, dim=1)\n",
        "        return feat_seq, dir_seq\n",
        "\n",
        "\n",
        "class WeightedHuberSeqLoss(nn.Module):\n",
        "    def __init__(self, weights: np.ndarray, delta: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.delta = float(delta)\n",
        "        self.register_buffer('weights', torch.as_tensor(weights, dtype=torch.float32).view(1, 1, -1))\n",
        "\n",
        "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        e = pred - target\n",
        "        ae = torch.abs(e)\n",
        "        huber = torch.where(ae <= self.delta, 0.5 * e * e, self.delta * (ae - 0.5 * self.delta))\n",
        "        w = huber * self.weights\n",
        "        return w.mean()\n",
        "\n",
        "\n",
        "def tf_ratio_for_epoch(epoch: int, total_epochs: int, start: float, end: float) -> float:\n",
        "    if total_epochs <= 1:\n",
        "        return float(end)\n",
        "    p = (epoch - 1) / float(total_epochs - 1)\n",
        "    return float(start + (end - start) * p)\n",
        "\n",
        "\n",
        "def run_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    reg_loss_fn: nn.Module,\n",
        "    dir_loss_fn: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer | None = None,\n",
        "    tf_ratio: float = 0.0,\n",
        ") -> dict:\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    total = 0.0\n",
        "    reg_total = 0.0\n",
        "    dir_total = 0.0\n",
        "    n_items = 0\n",
        "\n",
        "    for xb, yb_scaled, yb_raw in loader:\n",
        "        xb = xb.to(DEVICE)\n",
        "        yb_scaled = yb_scaled.to(DEVICE)\n",
        "        yb_raw = yb_raw.to(DEVICE)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            pred_seq, dir_logits = model(\n",
        "                xb,\n",
        "                y_teacher=yb_scaled if is_train else None,\n",
        "                teacher_forcing_ratio=tf_ratio if is_train else 0.0,\n",
        "            )\n",
        "\n",
        "            reg_loss = reg_loss_fn(pred_seq, yb_scaled)\n",
        "\n",
        "            dir_target = (yb_raw[:, :, 3] > 0.0).float()\n",
        "            dir_loss = dir_loss_fn(dir_logits, dir_target)\n",
        "\n",
        "            loss = reg_loss + DIR_LOSS_WEIGHT * dir_loss\n",
        "\n",
        "        if is_train:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total += loss.item() * bs\n",
        "        reg_total += reg_loss.item() * bs\n",
        "        dir_total += dir_loss.item() * bs\n",
        "        n_items += bs\n",
        "\n",
        "    return {\n",
        "        'total': total / max(n_items, 1),\n",
        "        'reg': reg_total / max(n_items, 1),\n",
        "        'dir': dir_total / max(n_items, 1),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader) -> pd.DataFrame:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-5\n",
        "    )\n",
        "\n",
        "    reg_loss_fn = WeightedHuberSeqLoss(LOSS_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
        "    dir_loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    wait = 0\n",
        "    rows = []\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        tf_ratio = tf_ratio_for_epoch(epoch, MAX_EPOCHS, TF_START, TF_END)\n",
        "\n",
        "        tr = run_epoch(model, train_loader, reg_loss_fn, dir_loss_fn, optimizer=optimizer, tf_ratio=tf_ratio)\n",
        "        va = run_epoch(model, val_loader, reg_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "\n",
        "        scheduler.step(va['total'])\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        rows.append({\n",
        "            'epoch': epoch,\n",
        "            'tf_ratio': tf_ratio,\n",
        "            'lr': lr,\n",
        "            'train_total': tr['total'],\n",
        "            'val_total': va['total'],\n",
        "            'train_reg': tr['reg'],\n",
        "            'val_reg': va['reg'],\n",
        "            'train_dir': tr['dir'],\n",
        "            'val_dir': va['dir'],\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | tf={tf_ratio:.3f} | \"\n",
        "            f\"train={tr['total']:.5f} (reg={tr['reg']:.5f}, dir={tr['dir']:.5f}) | \"\n",
        "            f\"val={va['total']:.5f} (reg={va['reg']:.5f}, dir={va['dir']:.5f}) | lr={lr:.6g}\"\n",
        "        )\n",
        "\n",
        "        if va['total'] < best_val:\n",
        "            best_val = va['total']\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f'Early stopping at epoch {epoch}.')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics, baselines, recursive rollout, and fold pipeline\n",
        "def rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b)) ** 2)))\n",
        "\n",
        "\n",
        "def directional_accuracy_eps(actual_close: np.ndarray, pred_close: np.ndarray, prev_close: np.ndarray, eps: float) -> float:\n",
        "    actual_move = np.asarray(actual_close) - np.asarray(prev_close)\n",
        "    pred_move = np.asarray(pred_close) - np.asarray(prev_close)\n",
        "\n",
        "    mask = np.abs(actual_move) > eps\n",
        "    if not np.any(mask):\n",
        "        return float('nan')\n",
        "\n",
        "    a = np.sign(actual_move[mask])\n",
        "    p = np.sign(pred_move[mask])\n",
        "    return float(np.mean(a == p))\n",
        "\n",
        "\n",
        "def evaluate_metrics(actual_ohlc: np.ndarray, pred_ohlc: np.ndarray, prev_close: np.ndarray, eps: float) -> dict:\n",
        "    actual_ohlc = np.asarray(actual_ohlc, dtype=np.float32)\n",
        "    pred_ohlc = np.asarray(pred_ohlc, dtype=np.float32)\n",
        "    prev_close = np.asarray(prev_close, dtype=np.float32)\n",
        "\n",
        "    ac = actual_ohlc[:, 3]\n",
        "    pc = pred_ohlc[:, 3]\n",
        "\n",
        "    return {\n",
        "        'close_mae': float(np.mean(np.abs(ac - pc))),\n",
        "        'close_rmse': rmse(ac, pc),\n",
        "        'ohlc_mae': float(np.mean(np.abs(actual_ohlc - pred_ohlc))),\n",
        "        'ohlc_rmse': rmse(actual_ohlc.reshape(-1), pred_ohlc.reshape(-1)),\n",
        "        'directional_accuracy_eps': directional_accuracy_eps(ac, pc, prev_close, eps),\n",
        "        'mean_signed_bias': float(np.mean(pc - ac)),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_baselines(actual_ohlc: np.ndarray, prev_ohlc: np.ndarray, prev_close: np.ndarray, eps: float) -> dict:\n",
        "    persistence = evaluate_metrics(actual_ohlc, prev_ohlc, prev_close, eps)\n",
        "\n",
        "    flat = np.repeat(prev_close.reshape(-1, 1), 4, axis=1).astype(np.float32)\n",
        "    flat_rw = evaluate_metrics(actual_ohlc, flat, prev_close, eps)\n",
        "\n",
        "    return {'persistence': persistence, 'flat_close_rw': flat_rw}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_multistep_scaled(model: nn.Module, X: np.ndarray, batch_size: int = 512) -> np.ndarray:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = torch.from_numpy(X[i : i + batch_size]).float().to(DEVICE)\n",
        "        yp, _ = model(xb, y_teacher=None, teacher_forcing_ratio=0.0)\n",
        "        preds.append(yp.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(preds, axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def recursive_forecast_features(\n",
        "    model: nn.Module,\n",
        "    seed_window_scaled: np.ndarray,\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "    model.eval()\n",
        "    w = seed_window_scaled.copy().astype(np.float32)\n",
        "    out = []\n",
        "\n",
        "    for _ in range(horizon):\n",
        "        xb = torch.from_numpy(w).float().unsqueeze(0).to(DEVICE)\n",
        "        pred_scaled, _ = model(xb, y_teacher=None, teacher_forcing_ratio=0.0)\n",
        "        step_scaled = pred_scaled[:, 0, :].cpu().numpy()[0]\n",
        "\n",
        "        step_raw = undo_standardizer(step_scaled.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "        step_raw = np.clip(step_raw, clip_low, clip_high).astype(np.float32)\n",
        "\n",
        "        step_scaled_clipped = apply_standardizer(step_raw.reshape(1, -1), scale_mean, scale_std)[0]\n",
        "\n",
        "        out.append(step_raw)\n",
        "        w = np.vstack([w[1:], step_scaled_clipped]).astype(np.float32)\n",
        "\n",
        "    return np.asarray(out, dtype=np.float32)\n",
        "\n",
        "\n",
        "def evaluate_recursive_by_horizon(\n",
        "    model: nn.Module,\n",
        "    feat_scaled: np.ndarray,\n",
        "    price_values: np.ndarray,\n",
        "    prev_close_values: np.ndarray,\n",
        "    anchors: list[int],\n",
        "    horizon: int,\n",
        "    scale_mean: np.ndarray,\n",
        "    scale_std: np.ndarray,\n",
        "    clip_low: np.ndarray,\n",
        "    clip_high: np.ndarray,\n",
        ") -> pd.DataFrame:\n",
        "    m_err = [[] for _ in range(horizon)]\n",
        "    p_err = [[] for _ in range(horizon)]\n",
        "    f_err = [[] for _ in range(horizon)]\n",
        "\n",
        "    for i in anchors:\n",
        "        seed = feat_scaled[i - WINDOW : i]\n",
        "        last_close = float(prev_close_values[i])\n",
        "\n",
        "        pred_feat = recursive_forecast_features(\n",
        "            model=model,\n",
        "            seed_window_scaled=seed,\n",
        "            horizon=horizon,\n",
        "            scale_mean=scale_mean,\n",
        "            scale_std=scale_std,\n",
        "            clip_low=clip_low,\n",
        "            clip_high=clip_high,\n",
        "        )\n",
        "\n",
        "        pred_price = returns_to_prices_seq(pred_feat[:, :4], last_close=last_close)\n",
        "        actual = price_values[i + 1 : i + 1 + horizon]\n",
        "\n",
        "        persist_seq = np.repeat(price_values[i : i + 1], horizon, axis=0)\n",
        "        c0 = prev_close_values[i]\n",
        "        flat_seq = np.repeat(np.array([[c0, c0, c0, c0]], dtype=np.float32), horizon, axis=0)\n",
        "\n",
        "        for h in range(horizon):\n",
        "            m_err[h].append(abs(float(pred_price[h, 3] - actual[h, 3])))\n",
        "            p_err[h].append(abs(float(persist_seq[h, 3] - actual[h, 3])))\n",
        "            f_err[h].append(abs(float(flat_seq[h, 3] - actual[h, 3])))\n",
        "\n",
        "    rows = []\n",
        "    for h in range(horizon):\n",
        "        rows.append({\n",
        "            'horizon': h + 1,\n",
        "            'model_close_mae': float(np.mean(m_err[h])) if m_err[h] else np.nan,\n",
        "            'persistence_close_mae': float(np.mean(p_err[h])) if p_err[h] else np.nan,\n",
        "            'flat_close_mae': float(np.mean(f_err[h])) if f_err[h] else np.nan,\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def split_points(n_rows: int) -> tuple[int, int]:\n",
        "    t_end = int(n_rows * TRAIN_RATIO)\n",
        "    v_end = int(n_rows * (TRAIN_RATIO + VAL_RATIO))\n",
        "    return t_end, v_end\n",
        "\n",
        "\n",
        "def overfit_sanity_check(X_train: np.ndarray, y_train_s: np.ndarray, y_train_r: np.ndarray) -> dict:\n",
        "    n = min(1000, len(X_train))\n",
        "    if n < 300:\n",
        "        return {'ran': False, 'reason': 'too_few_samples'}\n",
        "\n",
        "    ds = MultiStepDataset(X_train[:n], y_train_s[:n], y_train_r[:n])\n",
        "    dl = DataLoader(ds, batch_size=128, shuffle=True, drop_last=False)\n",
        "\n",
        "    model = Seq2SeqGRU(input_size=len(FEATURE_COLS), hidden_size=128, num_layers=1, dropout=0.0, horizon=HORIZON).to(DEVICE)\n",
        "    reg_loss_fn = WeightedHuberSeqLoss(LOSS_WEIGHTS, delta=HUBER_DELTA).to(DEVICE)\n",
        "    dir_loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.0)\n",
        "\n",
        "    init_stats = run_epoch(model, dl, reg_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "    for _ in range(15):\n",
        "        _ = run_epoch(model, dl, reg_loss_fn, dir_loss_fn, optimizer=opt, tf_ratio=1.0)\n",
        "    fin_stats = run_epoch(model, dl, reg_loss_fn, dir_loss_fn, optimizer=None, tf_ratio=0.0)\n",
        "\n",
        "    passed = bool(fin_stats['total'] < init_stats['total'] * 0.65)\n",
        "    return {\n",
        "        'ran': True,\n",
        "        'initial_total': float(init_stats['total']),\n",
        "        'final_total': float(fin_stats['total']),\n",
        "        'passed': passed,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_fold(fold_name: str, price_fold: pd.DataFrame, run_sanity: bool = False) -> dict:\n",
        "    feat_fold = build_feature_frame(price_fold)\n",
        "\n",
        "    raw_feat = feat_fold[FEATURE_COLS].to_numpy(np.float32)\n",
        "    invalid_rows = feat_fold['invalid_feature_row'].to_numpy(np.int8).astype(bool)\n",
        "    prev_close_vals = feat_fold['prev_close'].to_numpy(np.float32)\n",
        "\n",
        "    price_vals = price_fold[OHLC_COLS].to_numpy(np.float32)\n",
        "\n",
        "    train_end, val_end = split_points(len(raw_feat))\n",
        "    if train_end <= WINDOW or val_end <= train_end:\n",
        "        raise RuntimeError(f'{fold_name}: invalid split points for feature rows={len(raw_feat)}')\n",
        "\n",
        "    scale_mean, scale_std = fit_standardizer(raw_feat[:train_end])\n",
        "    feat_scaled = apply_standardizer(raw_feat, scale_mean, scale_std)\n",
        "\n",
        "    ql, qh = CLIP_TARGET_QUANTILES\n",
        "    clip_low = np.quantile(raw_feat[:train_end], ql, axis=0).astype(np.float32)\n",
        "    clip_high = np.quantile(raw_feat[:train_end], qh, axis=0).astype(np.float32)\n",
        "\n",
        "    X_all, y_all_s, y_all_r, starts, dropped_invalid = make_multistep_windows(\n",
        "        values_scaled=feat_scaled,\n",
        "        values_raw=raw_feat,\n",
        "        invalid_rows=invalid_rows,\n",
        "        window=WINDOW,\n",
        "        horizon=HORIZON,\n",
        "    )\n",
        "\n",
        "    if len(X_all) == 0:\n",
        "        raise RuntimeError(f'{fold_name}: no valid windows after filtering imputation-affected rows.')\n",
        "\n",
        "    end_idx = starts + HORIZON - 1\n",
        "    train_m = end_idx < train_end\n",
        "    val_m = (end_idx >= train_end) & (end_idx < val_end)\n",
        "    test_m = end_idx >= val_end\n",
        "\n",
        "    X_train, y_train_s, y_train_r = X_all[train_m], y_all_s[train_m], y_all_r[train_m]\n",
        "    X_val, y_val_s, y_val_r = X_all[val_m], y_all_s[val_m], y_all_r[val_m]\n",
        "    X_test, y_test_s, y_test_r = X_all[test_m], y_all_s[test_m], y_all_r[test_m]\n",
        "    test_starts = starts[test_m]\n",
        "\n",
        "    if min(len(X_train), len(X_val), len(X_test)) == 0:\n",
        "        raise RuntimeError(f'{fold_name}: empty split after filtering.')\n",
        "\n",
        "    sanity = overfit_sanity_check(X_train, y_train_s, y_train_r) if run_sanity else {'ran': False}\n",
        "\n",
        "    train_loader = DataLoader(MultiStepDataset(X_train, y_train_s, y_train_r), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(MultiStepDataset(X_val, y_val_s, y_val_r), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    model = Seq2SeqGRU(\n",
        "        input_size=len(FEATURE_COLS),\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "        horizon=HORIZON,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    hist = train_model(model, train_loader, val_loader)\n",
        "\n",
        "    pred_test_seq_scaled = predict_multistep_scaled(model, X_test, batch_size=512)\n",
        "    pred_test_seq_raw = undo_standardizer(pred_test_seq_scaled.reshape(-1, len(FEATURE_COLS)), scale_mean, scale_std).reshape(pred_test_seq_scaled.shape)\n",
        "\n",
        "    pred_step1_ret = pred_test_seq_raw[:, 0, :4]\n",
        "    actual_step1_ret = y_test_r[:, 0, :4]\n",
        "\n",
        "    prev_close_t = prev_close_vals[test_starts]\n",
        "    actual_ohlc_1 = price_vals[test_starts + 1]\n",
        "    pred_ohlc_1 = one_step_returns_to_prices_batch(pred_step1_ret, prev_close_t)\n",
        "\n",
        "    prev_ohlc = price_vals[test_starts]\n",
        "\n",
        "    model_metrics = evaluate_metrics(actual_ohlc_1, pred_ohlc_1, prev_close_t, DIRECTION_EPS)\n",
        "    baseline_metrics = evaluate_baselines(actual_ohlc_1, prev_ohlc, prev_close_t, DIRECTION_EPS)\n",
        "\n",
        "    # Prediction variance diagnostic for collapse detection.\n",
        "    pred_close_ret_std = float(np.std(pred_step1_ret[:, 3]))\n",
        "    actual_close_ret_std = float(np.std(actual_step1_ret[:, 3]))\n",
        "    std_ratio = pred_close_ret_std / max(actual_close_ret_std, 1e-12)\n",
        "\n",
        "    anchors = [int(i) for i in test_starts if i >= WINDOW and (i + HORIZON) < len(price_vals)]\n",
        "    if not anchors:\n",
        "        raise RuntimeError(f'{fold_name}: no valid anchors for recursive evaluation.')\n",
        "\n",
        "    horizon_df = evaluate_recursive_by_horizon(\n",
        "        model=model,\n",
        "        feat_scaled=feat_scaled,\n",
        "        price_values=price_vals,\n",
        "        prev_close_values=prev_close_vals,\n",
        "        anchors=anchors,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=scale_mean,\n",
        "        scale_std=scale_std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "    )\n",
        "\n",
        "    last_anchor = anchors[-1]\n",
        "    seed = feat_scaled[last_anchor - WINDOW : last_anchor]\n",
        "    last_close = float(prev_close_vals[last_anchor])\n",
        "\n",
        "    pred_path_feat = recursive_forecast_features(\n",
        "        model=model,\n",
        "        seed_window_scaled=seed,\n",
        "        horizon=HORIZON,\n",
        "        scale_mean=scale_mean,\n",
        "        scale_std=scale_std,\n",
        "        clip_low=clip_low,\n",
        "        clip_high=clip_high,\n",
        "    )\n",
        "\n",
        "    pred_path_price = returns_to_prices_seq(pred_path_feat[:, :4], last_close=last_close)\n",
        "    actual_path_price = price_vals[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
        "\n",
        "    future_idx = price_fold.index[last_anchor + 1 : last_anchor + 1 + HORIZON]\n",
        "    pred_future_df = pd.DataFrame(pred_path_price, index=future_idx, columns=OHLC_COLS)\n",
        "    actual_future_df = pd.DataFrame(actual_path_price, index=future_idx, columns=OHLC_COLS)\n",
        "\n",
        "    known_pos = last_anchor\n",
        "    context = price_fold.iloc[max(0, known_pos - 169) : known_pos + 1][OHLC_COLS].copy()\n",
        "\n",
        "    step15 = horizon_df[horizon_df['horizon'] == HORIZON].iloc[0]\n",
        "\n",
        "    return {\n",
        "        'fold': fold_name,\n",
        "        'history_df': hist,\n",
        "        'sanity': sanity,\n",
        "        'model_metrics': model_metrics,\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'horizon_df': horizon_df,\n",
        "        'pred_close_ret_std': pred_close_ret_std,\n",
        "        'actual_close_ret_std': actual_close_ret_std,\n",
        "        'pred_actual_std_ratio': float(std_ratio),\n",
        "        'samples': {\n",
        "            'train': int(len(X_train)),\n",
        "            'val': int(len(X_val)),\n",
        "            'test': int(len(X_test)),\n",
        "            'dropped_invalid_windows': int(dropped_invalid),\n",
        "            'anchors': int(len(anchors)),\n",
        "        },\n",
        "        'step15': {\n",
        "            'model_close_mae': float(step15['model_close_mae']),\n",
        "            'persistence_close_mae': float(step15['persistence_close_mae']),\n",
        "            'flat_close_mae': float(step15['flat_close_mae']),\n",
        "        },\n",
        "        'context_df': context,\n",
        "        'actual_future_df': actual_future_df,\n",
        "        'pred_future_df': pred_future_df,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run walk-forward evaluation\n",
        "fold_results = []\n",
        "\n",
        "for i, (name, a, b) in enumerate(slices, start=1):\n",
        "    print(f'\\n=== Running {name} [{a}:{b}] ===')\n",
        "    fold_price = price_df.iloc[a:b].copy()\n",
        "\n",
        "    res = run_fold(name, fold_price, run_sanity=(i == 1))\n",
        "    fold_results.append(res)\n",
        "\n",
        "    print('Samples:', res['samples'])\n",
        "    print('Sanity:', res['sanity'])\n",
        "    print('One-step model:', res['model_metrics'])\n",
        "    print('One-step persistence:', res['baseline_metrics']['persistence'])\n",
        "    print('One-step flat:', res['baseline_metrics']['flat_close_rw'])\n",
        "    print('Recursive step-15:', res['step15'])\n",
        "    print('Std ratio (pred/actual close return):', res['pred_actual_std_ratio'])\n",
        "\n",
        "if not fold_results:\n",
        "    raise RuntimeError('No fold results produced.')\n",
        "\n",
        "summary_rows = []\n",
        "for r in fold_results:\n",
        "    m = r['model_metrics']\n",
        "    p = r['baseline_metrics']['persistence']\n",
        "    f = r['baseline_metrics']['flat_close_rw']\n",
        "    s = r['step15']\n",
        "\n",
        "    summary_rows.append({\n",
        "        'fold': r['fold'],\n",
        "        'model_close_mae': m['close_mae'],\n",
        "        'model_directional_acc_eps': m['directional_accuracy_eps'],\n",
        "        'model_bias': m['mean_signed_bias'],\n",
        "        'persist_close_mae': p['close_mae'],\n",
        "        'flat_close_mae': f['close_mae'],\n",
        "        'step15_model_mae': s['model_close_mae'],\n",
        "        'step15_persist_mae': s['persistence_close_mae'],\n",
        "        'pred_actual_std_ratio': r['pred_actual_std_ratio'],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "display(summary_df)\n",
        "\n",
        "metric_cols = [c for c in summary_df.columns if c != 'fold']\n",
        "agg_df = pd.DataFrame({'mean': summary_df[metric_cols].mean(), 'std': summary_df[metric_cols].std(ddof=0)})\n",
        "display(agg_df)\n",
        "\n",
        "horizon_all = pd.concat([r['horizon_df'].assign(fold=r['fold']) for r in fold_results], ignore_index=True)\n",
        "horizon_stats = horizon_all.groupby('horizon', as_index=False)[['model_close_mae', 'persistence_close_mae', 'flat_close_mae']].mean()\n",
        "display(horizon_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Acceptance checks + diagnostics\n",
        "latest = fold_results[-1]\n",
        "\n",
        "mean_model_mae = float(summary_df['model_close_mae'].mean())\n",
        "mean_persist_mae = float(summary_df['persist_close_mae'].mean())\n",
        "mean_dir = float(summary_df['model_directional_acc_eps'].mean())\n",
        "mean_bias = float(summary_df['model_bias'].mean())\n",
        "mean_step15_model = float(summary_df['step15_model_mae'].mean())\n",
        "mean_step15_persist = float(summary_df['step15_persist_mae'].mean())\n",
        "mean_std_ratio = float(summary_df['pred_actual_std_ratio'].mean())\n",
        "\n",
        "pred_close_path = latest['pred_future_df']['Close'].to_numpy()\n",
        "is_monotonic = bool(np.all(np.diff(pred_close_path) >= 0) or np.all(np.diff(pred_close_path) <= 0))\n",
        "\n",
        "acceptance = {\n",
        "    'criterion_1_model_mae_20pct_better_than_persistence': mean_model_mae <= 0.8 * mean_persist_mae,\n",
        "    'criterion_2_directional_accuracy_eps_at_least_0_52': mean_dir >= 0.52,\n",
        "    'criterion_3_step15_better_than_persistence': mean_step15_model < mean_step15_persist,\n",
        "    'criterion_4_abs_bias_within_25pct_of_mae': abs(mean_bias) <= 0.25 * mean_model_mae,\n",
        "    'criterion_5_non_monotonic_latest_prediction_path': not is_monotonic,\n",
        "    'criterion_6_prediction_variance_not_collapsed': mean_std_ratio >= 0.60,\n",
        "}\n",
        "\n",
        "print('Acceptance checks:')\n",
        "for k, v in acceptance.items():\n",
        "    print(f'  {k}: {v}')\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 4.8), facecolor='white')\n",
        "\n",
        "hist = latest['history_df']\n",
        "axes[0].plot(hist['epoch'], hist['train_total'], label='Train total', color='black')\n",
        "axes[0].plot(hist['epoch'], hist['val_total'], label='Val total', color='gray')\n",
        "axes[0].plot(hist['epoch'], hist['val_reg'], label='Val reg', color='#1f77b4', alpha=0.8)\n",
        "axes[0].plot(hist['epoch'], hist['val_dir'], label='Val dir', color='#d62728', alpha=0.8)\n",
        "axes[0].set_title(f\"Loss Curves ({latest['fold']})\")\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(alpha=0.25)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['model_close_mae'], label='Model', color='black', linewidth=2.0)\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['persistence_close_mae'], label='Persistence', color='#E74C3C')\n",
        "axes[1].plot(horizon_stats['horizon'], horizon_stats['flat_close_mae'], label='Flat RW', color='#3498DB')\n",
        "axes[1].set_title('Recursive Close MAE by Horizon')\n",
        "axes[1].set_xlabel('Horizon')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].grid(alpha=0.25)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].bar(summary_df['fold'], summary_df['pred_actual_std_ratio'], color='#555555')\n",
        "axes[2].axhline(1.0, color='#2ca02c', linestyle='--', linewidth=1.2, label='ideal=1.0')\n",
        "axes[2].axhline(0.6, color='#ff7f0e', linestyle=':', linewidth=1.2, label='min target=0.6')\n",
        "axes[2].set_title('Predicted/Actual Close Return Std Ratio')\n",
        "axes[2].set_ylabel('Std ratio')\n",
        "axes[2].grid(alpha=0.25)\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final candlestick chart (history green/red, predicted white/black)\n",
        "def draw_candles(\n",
        "    ax,\n",
        "    ohlc: pd.DataFrame,\n",
        "    start_x: int,\n",
        "    up_edge: str,\n",
        "    up_face: str,\n",
        "    down_edge: str,\n",
        "    down_face: str,\n",
        "    wick_color: str,\n",
        "    width: float = 0.62,\n",
        "    lw: float = 1.0,\n",
        "    alpha: float = 1.0,\n",
        "):\n",
        "    vals = ohlc[OHLC_COLS].to_numpy()\n",
        "    for i, (o, h, l, c) in enumerate(vals):\n",
        "        x = start_x + i\n",
        "        bull = c >= o\n",
        "\n",
        "        ax.vlines(x, l, h, color=wick_color, linewidth=lw, alpha=alpha, zorder=2)\n",
        "\n",
        "        lower = min(o, c)\n",
        "        height = abs(c - o)\n",
        "        if height < 1e-8:\n",
        "            height = 1e-6\n",
        "\n",
        "        rect = Rectangle(\n",
        "            (x - width / 2, lower),\n",
        "            width,\n",
        "            height,\n",
        "            facecolor=up_face if bull else down_face,\n",
        "            edgecolor=up_edge if bull else down_edge,\n",
        "            linewidth=lw,\n",
        "            alpha=alpha,\n",
        "            zorder=3,\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "\n",
        "context_df = latest['context_df']\n",
        "actual_future_df = latest['actual_future_df']\n",
        "pred_future_df = latest['pred_future_df']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(17, 8), facecolor='black')\n",
        "ax.set_facecolor('black')\n",
        "\n",
        "draw_candles(ax, context_df, 0, '#2ECC71', '#2ECC71', '#E74C3C', '#E74C3C', '#DADADA', width=0.58, lw=1.0, alpha=0.95)\n",
        "draw_candles(ax, actual_future_df, len(context_df), '#1D6F42', '#1D6F42', '#8E2F25', '#8E2F25', '#9A9A9A', width=0.58, lw=1.0, alpha=0.70)\n",
        "draw_candles(ax, pred_future_df, len(context_df), '#FFFFFF', '#FFFFFF', '#000000', '#000000', '#F5F5F5', width=0.50, lw=1.35, alpha=1.0)\n",
        "\n",
        "ax.axvline(len(context_df) - 0.5, color='white', linestyle='--', linewidth=0.9, alpha=0.6)\n",
        "\n",
        "idx = context_df.index.append(actual_future_df.index)\n",
        "n = len(idx)\n",
        "step = max(1, n // 10)\n",
        "ticks = list(range(0, n, step))\n",
        "if ticks[-1] != n - 1:\n",
        "    ticks.append(n - 1)\n",
        "\n",
        "labels = [idx[i].strftime('%m-%d %H:%M') for i in ticks]\n",
        "ax.set_xticks(ticks)\n",
        "ax.set_xticklabels(labels, rotation=26, ha='right', color='white', fontsize=9)\n",
        "\n",
        "ax.tick_params(axis='y', colors='white')\n",
        "for sp in ax.spines.values():\n",
        "    sp.set_color('#666666')\n",
        "\n",
        "ax.grid(color='#252525', linewidth=0.6, alpha=0.35)\n",
        "ax.set_title(f'MSFT 1m ({latest[\"fold\"]}) - History vs 15-step Forecast', color='white', pad=14)\n",
        "ax.set_ylabel('Price', color='white')\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='History bullish (green)'),\n",
        "    Patch(facecolor='#E74C3C', edgecolor='#E74C3C', label='History bearish (red)'),\n",
        "    Patch(facecolor='#FFFFFF', edgecolor='#FFFFFF', label='Predicted bullish (white)'),\n",
        "    Patch(facecolor='#000000', edgecolor='#FFFFFF', label='Predicted bearish (black)'),\n",
        "]\n",
        "leg = ax.legend(handles=legend_elements, facecolor='black', edgecolor='#707070', framealpha=1.0, loc='upper left')\n",
        "for t in leg.get_texts():\n",
        "    t.set_color('white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- If `pred_actual_std_ratio` stays very low (<0.6), predictions are still volatility-collapsed.\n",
        "- The model now learns a richer latent state via volume/order-flow proxies, but with IEX feed quality constraints this remains challenging.\n",
        "- If results are still weak, next step is richer market microstructure data (quotes/trades) and/or horizon-specific objective tuning.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
