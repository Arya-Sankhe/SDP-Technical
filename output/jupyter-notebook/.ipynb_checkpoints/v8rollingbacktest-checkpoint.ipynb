{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V8 Rolling Walk-Forward Backtest (MSFT 1m, Strictly Causal)\n",
    "This notebook wraps the existing v7 `Seq2SeqAttnGRU` + `generate_realistic()` model in a bar-by-bar rolling backtest engine.\n",
    "\n",
    "What it does:\n",
    "1. Runs **strictly causal** rolling inference every minute.\n",
    "2. Produces a 15-step autoregressive forecast at each anchor minute.\n",
    "3. Stores prediction logs for every anchor.\n",
    "4. Plots a fading **prediction fan/cone** over real candles.\n",
    "5. Computes rolling metrics (hit rate, path divergence, trend correlation)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 1: Imports, Configuration, and v7 Model/Helpers\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from alpaca.data.enums import DataFeed\n",
    "from alpaca.data.historical import StockHistoricalDataClient\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.timeframe import TimeFrame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch, Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- Repro ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# ---------- Core config ----------\n",
    "SYMBOL = 'MSFT'\n",
    "OHLC_COLS = ['Open', 'High', 'Low', 'Close']\n",
    "RAW_COLS = OHLC_COLS + ['Volume', 'TradeCount', 'VWAP']\n",
    "BASE_FEATURE_COLS = [\n",
    "    'rOpen', 'rHigh', 'rLow', 'rClose',\n",
    "    'logVolChange', 'logTradeCountChange',\n",
    "    'vwapDelta', 'rangeFrac', 'orderFlowProxy', 'tickPressure',\n",
    "]\n",
    "TARGET_COLS = ['rOpen', 'rHigh', 'rLow', 'rClose']\n",
    "\n",
    "# Keep v7 architecture/horizon unchanged\n",
    "WINDOW_SIZE = 160\n",
    "HORIZON = 15\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.20\n",
    "MIN_PREDICTED_VOL = 0.0001\n",
    "\n",
    "# Rolling backtest controls\n",
    "ROLLING_START_TIME = '09:30'\n",
    "ROLLING_END_TIME = '16:00'\n",
    "MAX_PREDICTION_AGE = 50\n",
    "FAN_OPACITY_DECAY = 0.02\n",
    "ROLLING_EVERY_N_BARS = 1  # Option A default: every bar\n",
    "DEFAULT_TEMPERATURE = 1.5\n",
    "USE_TEMPERATURE_SCHEDULE = True\n",
    "TEMPERATURE_SCHEDULE = [\n",
    "    ('09:30', '10:15', 1.25),\n",
    "    ('10:15', '14:00', 1.45),\n",
    "    ('14:00', '16:00', 1.60),\n",
    "]\n",
    "\n",
    "# Data / API\n",
    "LOOKBACK_DAYS = 120\n",
    "ALPACA_FEED = os.getenv('ALPACA_FEED', 'iex').strip().lower()\n",
    "SESSION_TZ = 'America/New_York'\n",
    "REQUEST_CHUNK_DAYS = 5\n",
    "MAX_REQUESTS_PER_MINUTE = 120\n",
    "MAX_RETRIES = 5\n",
    "MAX_SESSION_FILL_RATIO = 0.15\n",
    "SKIP_OPEN_BARS_TARGET = 6\n",
    "\n",
    "# Backtest selection\n",
    "BACKTEST_DATE = os.getenv('BACKTEST_DATE', '').strip() or None  # example: '2025-02-13'\n",
    "\n",
    "# Model loading\n",
    "MODEL_CHECKPOINT_PATH = os.getenv('V7_MODEL_CHECKPOINT', 'output/models/msft_v7_model.pt')\n",
    "USE_RANDOM_MODEL_FOR_DEBUG = False\n",
    "\n",
    "print({\n",
    "    'symbol': SYMBOL,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'horizon': HORIZON,\n",
    "    'rolling_hours': (ROLLING_START_TIME, ROLLING_END_TIME),\n",
    "    'max_prediction_age': MAX_PREDICTION_AGE,\n",
    "    'fan_opacity_decay': FAN_OPACITY_DECAY,\n",
    "    'default_temperature': DEFAULT_TEMPERATURE,\n",
    "    'use_temperature_schedule': USE_TEMPERATURE_SCHEDULE,\n",
    "    'feed': ALPACA_FEED,\n",
    "    'backtest_date': BACKTEST_DATE,\n",
    "})\n",
    "\n",
    "\n",
    "# ---------- v7 model architecture: UNCHANGED ----------\n",
    "class Seq2SeqAttnGRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, dropout, horizon):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.decoder_cell = nn.GRUCell(output_size + hidden_size, hidden_size)\n",
    "        self.attn_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        self.mu_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "        self.log_sigma_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size // 2, output_size),\n",
    "        )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.mu_head[-1].weight, gain=0.1)\n",
    "        nn.init.zeros_(self.mu_head[-1].bias)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].weight)\n",
    "        nn.init.zeros_(self.log_sigma_head[-1].bias)\n",
    "\n",
    "    def _attend(self, h_dec, enc_out):\n",
    "        query = self.attn_proj(h_dec).unsqueeze(2)\n",
    "        scores = torch.bmm(enc_out, query).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(weights.unsqueeze(1), enc_out).squeeze(1)\n",
    "        return context\n",
    "\n",
    "    def forward(self, x, y_teacher=None, teacher_forcing_ratio=0.0, return_sigma=False):\n",
    "        enc_out, h = self.encoder(x)\n",
    "        h_dec = h[-1]\n",
    "        dec_input = x[:, -1, : self.output_size]\n",
    "\n",
    "        mu_seq, sigma_seq = [], []\n",
    "        for t in range(self.horizon):\n",
    "            context = self._attend(h_dec, enc_out)\n",
    "            cell_input = torch.cat([dec_input, context], dim=1)\n",
    "            h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "            out_features = torch.cat([h_dec, context], dim=1)\n",
    "\n",
    "            mu = self.mu_head(out_features)\n",
    "            log_sigma = self.log_sigma_head(out_features)\n",
    "\n",
    "            mu_seq.append(mu.unsqueeze(1))\n",
    "            sigma_seq.append(log_sigma.unsqueeze(1))\n",
    "\n",
    "            if y_teacher is not None and teacher_forcing_ratio > 0.0:\n",
    "                if teacher_forcing_ratio >= 1.0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    dec_input = y_teacher[:, t, :]\n",
    "                else:\n",
    "                    noise = torch.randn_like(mu) * torch.exp(log_sigma).detach()\n",
    "                    dec_input = mu + noise\n",
    "            else:\n",
    "                dec_input = mu\n",
    "\n",
    "        mu_out = torch.cat(mu_seq, dim=1)\n",
    "        sigma_out = torch.cat(sigma_seq, dim=1)\n",
    "        if return_sigma:\n",
    "            return mu_out, sigma_out\n",
    "        return mu_out\n",
    "\n",
    "    def generate_realistic(self, x, temperature=1.0, historical_vol=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            enc_out, h = self.encoder(x)\n",
    "            h_dec = h[-1]\n",
    "            dec_input = x[:, -1, : self.output_size]\n",
    "\n",
    "            generated = []\n",
    "            for t in range(self.horizon):\n",
    "                context = self._attend(h_dec, enc_out)\n",
    "                cell_input = torch.cat([dec_input, context], dim=1)\n",
    "                h_dec = self.decoder_cell(cell_input, h_dec)\n",
    "                out_features = torch.cat([h_dec, context], dim=1)\n",
    "\n",
    "                mu = self.mu_head(out_features)\n",
    "                log_sigma = self.log_sigma_head(out_features)\n",
    "\n",
    "                sigma = torch.exp(log_sigma) * temperature\n",
    "                if historical_vol is not None and t < 5:\n",
    "                    sigma = torch.ones_like(sigma) * historical_vol\n",
    "\n",
    "                sigma = torch.maximum(sigma, torch.tensor(MIN_PREDICTED_VOL, device=x.device))\n",
    "                noise = torch.randn_like(mu) * sigma\n",
    "                sample = mu + noise\n",
    "\n",
    "                generated.append(sample.unsqueeze(1))\n",
    "                dec_input = sample\n",
    "\n",
    "            return torch.cat(generated, dim=1)\n",
    "\n",
    "\n",
    "# ---------- Data helpers ----------\n",
    "class RequestPacer:\n",
    "    def __init__(self, max_calls_per_minute: int):\n",
    "        if max_calls_per_minute <= 0:\n",
    "            raise ValueError('max_calls_per_minute must be > 0')\n",
    "        self.min_interval = 60.0 / float(max_calls_per_minute)\n",
    "        self.last_call_ts = 0.0\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self.last_call_ts\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        self.last_call_ts = time.monotonic()\n",
    "\n",
    "\n",
    "def _require_alpaca_credentials() -> tuple[str, str]:\n",
    "    api_key = os.getenv('ALPACA_API_KEY')\n",
    "    secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "    if not api_key or not secret_key:\n",
    "        raise RuntimeError('Missing ALPACA_API_KEY / ALPACA_SECRET_KEY.')\n",
    "    return api_key, secret_key\n",
    "\n",
    "\n",
    "def _resolve_feed(feed_name: str) -> DataFeed:\n",
    "    mapping = {'iex': DataFeed.IEX, 'sip': DataFeed.SIP, 'delayed_sip': DataFeed.DELAYED_SIP}\n",
    "    k = feed_name.strip().lower()\n",
    "    if k not in mapping:\n",
    "        raise ValueError(f'Unsupported ALPACA_FEED={feed_name!r}. Use one of: {list(mapping)}')\n",
    "    return mapping[k]\n",
    "\n",
    "\n",
    "def fetch_bars_alpaca(symbol: str, lookback_days: int) -> tuple[pd.DataFrame, int]:\n",
    "    api_key, secret_key = _require_alpaca_credentials()\n",
    "    client = StockHistoricalDataClient(api_key=api_key, secret_key=secret_key)\n",
    "\n",
    "    feed = _resolve_feed(ALPACA_FEED)\n",
    "    pacer = RequestPacer(MAX_REQUESTS_PER_MINUTE)\n",
    "\n",
    "    end_ts = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    if ALPACA_FEED in {'sip', 'delayed_sip'}:\n",
    "        end_ts = end_ts - timedelta(minutes=20)\n",
    "    start_ts = end_ts - timedelta(days=lookback_days)\n",
    "\n",
    "    parts = []\n",
    "    cursor = start_ts\n",
    "    calls = 0\n",
    "\n",
    "    while cursor < end_ts:\n",
    "        chunk_end = min(cursor + timedelta(days=REQUEST_CHUNK_DAYS), end_ts)\n",
    "        chunk = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            pacer.wait()\n",
    "            calls += 1\n",
    "            try:\n",
    "                req = StockBarsRequest(\n",
    "                    symbol_or_symbols=[symbol],\n",
    "                    timeframe=TimeFrame.Minute,\n",
    "                    start=cursor,\n",
    "                    end=chunk_end,\n",
    "                    feed=feed,\n",
    "                    limit=10000,\n",
    "                )\n",
    "                chunk = client.get_stock_bars(req).df\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                msg = str(exc).lower()\n",
    "                if ('429' in msg or 'rate limit' in msg) and attempt < MAX_RETRIES:\n",
    "                    backoff = min(2 ** attempt, 30)\n",
    "                    print(f'Rate-limited; sleeping {backoff}s (attempt {attempt}/{MAX_RETRIES}).')\n",
    "                    time.sleep(backoff)\n",
    "                    continue\n",
    "                if ('subscription' in msg or 'forbidden' in msg) and ALPACA_FEED != 'iex':\n",
    "                    raise RuntimeError('Feed unavailable for account. Use ALPACA_FEED=iex or upgrade subscription.') from exc\n",
    "                raise\n",
    "\n",
    "        if chunk is not None and not chunk.empty:\n",
    "            d = chunk.reset_index().rename(\n",
    "                columns={\n",
    "                    'timestamp': 'Datetime',\n",
    "                    'open': 'Open',\n",
    "                    'high': 'High',\n",
    "                    'low': 'Low',\n",
    "                    'close': 'Close',\n",
    "                    'volume': 'Volume',\n",
    "                    'trade_count': 'TradeCount',\n",
    "                    'vwap': 'VWAP',\n",
    "                }\n",
    "            )\n",
    "            if 'Volume' not in d.columns:\n",
    "                d['Volume'] = 0.0\n",
    "            if 'TradeCount' not in d.columns:\n",
    "                d['TradeCount'] = 0.0\n",
    "            if 'VWAP' not in d.columns:\n",
    "                d['VWAP'] = d['Close']\n",
    "\n",
    "            need = ['Datetime'] + RAW_COLS\n",
    "            missing = [c for c in need if c not in d.columns]\n",
    "            if missing:\n",
    "                raise RuntimeError(f'Alpaca response missing columns: {missing}')\n",
    "\n",
    "            d['Datetime'] = pd.to_datetime(d['Datetime'], utc=True)\n",
    "            d = d[need].dropna(subset=OHLC_COLS).set_index('Datetime').sort_index()\n",
    "            parts.append(d)\n",
    "\n",
    "        cursor = chunk_end\n",
    "\n",
    "    if not parts:\n",
    "        raise RuntimeError('No bars returned from Alpaca.')\n",
    "\n",
    "    out = pd.concat(parts, axis=0).sort_index()\n",
    "    out = out[~out.index.duplicated(keep='last')]\n",
    "    return out.astype(np.float32), calls\n",
    "\n",
    "\n",
    "def sessionize_with_calendar(df_utc: pd.DataFrame) -> tuple[pd.DataFrame, dict]:\n",
    "    if df_utc.empty:\n",
    "        raise RuntimeError('Input bars are empty.')\n",
    "\n",
    "    idx = pd.DatetimeIndex(df_utc.index)\n",
    "    if idx.tz is None:\n",
    "        idx = idx.tz_localize('UTC')\n",
    "    else:\n",
    "        idx = idx.tz_convert('UTC')\n",
    "    df_utc = df_utc.copy()\n",
    "    df_utc.index = idx\n",
    "\n",
    "    cal = mcal.get_calendar('XNYS')\n",
    "    sched = cal.schedule(\n",
    "        start_date=(idx.min() - pd.Timedelta(days=2)).date(),\n",
    "        end_date=(idx.max() + pd.Timedelta(days=2)).date(),\n",
    "    )\n",
    "\n",
    "    pieces = []\n",
    "    kept = 0\n",
    "    dropped_high_fill = 0\n",
    "    dropped_empty = 0\n",
    "    fill_ratios = []\n",
    "\n",
    "    for sid, (_, row) in enumerate(sched.iterrows()):\n",
    "        open_ts = pd.Timestamp(row['market_open'])\n",
    "        close_ts = pd.Timestamp(row['market_close'])\n",
    "        if open_ts.tzinfo is None:\n",
    "            open_ts = open_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            open_ts = open_ts.tz_convert('UTC')\n",
    "        if close_ts.tzinfo is None:\n",
    "            close_ts = close_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            close_ts = close_ts.tz_convert('UTC')\n",
    "\n",
    "        exp_idx = pd.date_range(open_ts, close_ts, freq='1min', inclusive='left')\n",
    "        if len(exp_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        day = df_utc[(df_utc.index >= open_ts) & (df_utc.index < close_ts)][RAW_COLS].copy()\n",
    "        day = day.reindex(exp_idx)\n",
    "\n",
    "        imputed = day[OHLC_COLS].isna().any(axis=1).to_numpy()\n",
    "        fill_ratio = float(imputed.mean())\n",
    "\n",
    "        if fill_ratio >= 1.0:\n",
    "            dropped_empty += 1\n",
    "            continue\n",
    "        if fill_ratio > MAX_SESSION_FILL_RATIO:\n",
    "            dropped_high_fill += 1\n",
    "            continue\n",
    "\n",
    "        day[OHLC_COLS + ['VWAP']] = day[OHLC_COLS + ['VWAP']].ffill().bfill()\n",
    "        day['VWAP'] = day['VWAP'].fillna(day['Close'])\n",
    "        day['Volume'] = day['Volume'].fillna(0.0)\n",
    "        day['TradeCount'] = day['TradeCount'].fillna(0.0)\n",
    "        day['is_imputed'] = imputed.astype(np.int8)\n",
    "        day['session_id'] = int(sid)\n",
    "        day['bar_in_session'] = np.arange(len(day), dtype=np.int32)\n",
    "        day['session_len'] = int(len(day))\n",
    "\n",
    "        if day[RAW_COLS].isna().any().any():\n",
    "            raise RuntimeError('NaNs remain after session fill.')\n",
    "\n",
    "        pieces.append(day)\n",
    "        kept += 1\n",
    "        fill_ratios.append(fill_ratio)\n",
    "\n",
    "    if not pieces:\n",
    "        raise RuntimeError('No sessions kept after filtering.')\n",
    "\n",
    "    out = pd.concat(pieces, axis=0).sort_index()\n",
    "    out.index = out.index.tz_convert(SESSION_TZ).tz_localize(None)\n",
    "\n",
    "    for c in RAW_COLS:\n",
    "        out[c] = out[c].astype(np.float32)\n",
    "    out['is_imputed'] = out['is_imputed'].astype(np.int8)\n",
    "    out['session_id'] = out['session_id'].astype(np.int32)\n",
    "    out['bar_in_session'] = out['bar_in_session'].astype(np.int32)\n",
    "    out['session_len'] = out['session_len'].astype(np.int32)\n",
    "\n",
    "    meta = {\n",
    "        'calendar_sessions_total': int(len(sched)),\n",
    "        'kept_sessions': int(kept),\n",
    "        'dropped_empty_sessions': int(dropped_empty),\n",
    "        'dropped_high_fill_sessions': int(dropped_high_fill),\n",
    "        'avg_fill_ratio_kept': float(np.mean(fill_ratios)) if fill_ratios else float('nan'),\n",
    "        'max_fill_ratio_kept': float(np.max(fill_ratios)) if fill_ratios else float('nan'),\n",
    "    }\n",
    "    return out, meta\n",
    "\n",
    "\n",
    "def enforce_candle_validity(ohlc: np.ndarray) -> np.ndarray:\n",
    "    out = np.asarray(ohlc, dtype=np.float32)\n",
    "    o, h, l, c = out[:, 0], out[:, 1], out[:, 2], out[:, 3]\n",
    "    out[:, 1] = np.maximum.reduce([h, o, c])\n",
    "    out[:, 2] = np.minimum.reduce([l, o, c])\n",
    "    return out\n",
    "\n",
    "\n",
    "def returns_to_prices_seq(return_ohlc: np.ndarray, last_close: float) -> np.ndarray:\n",
    "    seq = []\n",
    "    prev_close = float(last_close)\n",
    "    for rO, rH, rL, rC in np.asarray(return_ohlc, dtype=np.float32):\n",
    "        o = prev_close * np.exp(float(rO))\n",
    "        h = prev_close * np.exp(float(rH))\n",
    "        l = prev_close * np.exp(float(rL))\n",
    "        c = prev_close * np.exp(float(rC))\n",
    "        cand = enforce_candle_validity(np.array([[o, h, l, c]], dtype=np.float32))[0]\n",
    "        seq.append(cand)\n",
    "        prev_close = float(cand[3])\n",
    "    return np.asarray(seq, dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_feature_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    eps = 1e-9\n",
    "    g = df.groupby('session_id', sort=False)\n",
    "\n",
    "    prev_close = g['Close'].shift(1)\n",
    "    prev_close = prev_close.fillna(df['Open'])\n",
    "    prev_vol = g['Volume'].shift(1).fillna(df['Volume'])\n",
    "    prev_tc = g['TradeCount'].shift(1).fillna(df['TradeCount'])\n",
    "    prev_imp = g['is_imputed'].shift(1).fillna(0).astype(bool)\n",
    "\n",
    "    row_imputed = (df['is_imputed'].astype(bool) | prev_imp)\n",
    "    row_open_skip = (df['bar_in_session'].astype(int) < SKIP_OPEN_BARS_TARGET)\n",
    "\n",
    "    out = pd.DataFrame(index=df.index, dtype=np.float32)\n",
    "    out['rOpen'] = np.log(df['Open'] / (prev_close + eps))\n",
    "    out['rHigh'] = np.log(df['High'] / (prev_close + eps))\n",
    "    out['rLow'] = np.log(df['Low'] / (prev_close + eps))\n",
    "    out['rClose'] = np.log(df['Close'] / (prev_close + eps))\n",
    "\n",
    "    out['logVolChange'] = np.log((df['Volume'] + 1.0) / (prev_vol + 1.0))\n",
    "    out['logTradeCountChange'] = np.log((df['TradeCount'] + 1.0) / (prev_tc + 1.0))\n",
    "    out['vwapDelta'] = np.log((df['VWAP'] + eps) / (df['Close'] + eps))\n",
    "    out['rangeFrac'] = np.maximum(out['rHigh'] - out['rLow'], 0.0) / (np.abs(out['rClose']) + eps)\n",
    "\n",
    "    signed_body = (df['Close'] - df['Open']) / ((df['High'] - df['Low']) + eps)\n",
    "    out['orderFlowProxy'] = signed_body * np.log1p(df['Volume'])\n",
    "    out['tickPressure'] = np.sign(df['Close'] - df['Open']) * np.log1p(df['TradeCount'])\n",
    "\n",
    "    out['row_imputed'] = row_imputed.astype(np.int8).to_numpy()\n",
    "    out['row_open_skip'] = row_open_skip.astype(np.int8).to_numpy()\n",
    "    out['prev_close'] = prev_close.astype(np.float32).to_numpy()\n",
    "    return out.astype(np.float32)\n",
    "\n",
    "\n",
    "def temperature_for_anchor(anchor_time: pd.Timestamp, default_temp: float = DEFAULT_TEMPERATURE) -> float:\n",
    "    if not USE_TEMPERATURE_SCHEDULE:\n",
    "        return float(default_temp)\n",
    "\n",
    "    tt = anchor_time.time()\n",
    "    for start_s, end_s, temp in TEMPERATURE_SCHEDULE:\n",
    "        start_t = pd.Timestamp(start_s).time()\n",
    "        end_t = pd.Timestamp(end_s).time()\n",
    "        if start_t <= tt < end_t:\n",
    "            return float(temp)\n",
    "    return float(default_temp)\n",
    "\n",
    "\n",
    "def load_trained_v7_model(input_size: int, checkpoint_path: str = MODEL_CHECKPOINT_PATH) -> Seq2SeqAttnGRU:\n",
    "    model = Seq2SeqAttnGRU(\n",
    "        input_size=input_size,\n",
    "        output_size=len(TARGET_COLS),\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        horizon=HORIZON,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        payload = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        if isinstance(payload, dict) and 'state_dict' in payload:\n",
    "            state_dict = payload['state_dict']\n",
    "        else:\n",
    "            state_dict = payload\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        print(f'Loaded checkpoint: {checkpoint_path}')\n",
    "    elif USE_RANDOM_MODEL_FOR_DEBUG:\n",
    "        print('WARNING: checkpoint missing, using random weights (debug only).')\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f'Checkpoint not found: {checkpoint_path}. Set V7_MODEL_CHECKPOINT or enable USE_RANDOM_MODEL_FOR_DEBUG.'\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 2: Rolling backtest engine (strict causality)\n",
    "@dataclass\n",
    "class RollingPredictionLog:\n",
    "    anchor_time: pd.Timestamp\n",
    "    context_end_price: float\n",
    "    predicted_path: pd.DataFrame\n",
    "    actual_path: pd.DataFrame\n",
    "    prediction_horizon: int\n",
    "    temperature: float\n",
    "    context_start_time: pd.Timestamp\n",
    "    context_last_time: pd.Timestamp\n",
    "\n",
    "    step_mae: Optional[np.ndarray] = None\n",
    "    directional_hit: Optional[bool] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert len(self.predicted_path) == self.prediction_horizon, (\n",
    "            f'predicted_path length mismatch: {len(self.predicted_path)} != {self.prediction_horizon}'\n",
    "        )\n",
    "        assert len(self.actual_path) == self.prediction_horizon, (\n",
    "            f'actual_path length mismatch: {len(self.actual_path)} != {self.prediction_horizon}'\n",
    "        )\n",
    "        assert self.predicted_path.index[0] == self.anchor_time, (\n",
    "            f'Off-by-one: first predicted ts {self.predicted_path.index[0]} != anchor {self.anchor_time}'\n",
    "        )\n",
    "        assert self.actual_path.index[0] == self.anchor_time, (\n",
    "            f'Off-by-one: first actual ts {self.actual_path.index[0]} != anchor {self.anchor_time}'\n",
    "        )\n",
    "        assert self.context_last_time < self.anchor_time, (\n",
    "            f'Lookahead violation: context_last_time {self.context_last_time} must be < anchor {self.anchor_time}'\n",
    "        )\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        pred_close = self.predicted_path['Close'].to_numpy(np.float32)\n",
    "        actual_close = self.actual_path['Close'].to_numpy(np.float32)\n",
    "        self.step_mae = np.abs(pred_close - actual_close)\n",
    "\n",
    "        pred_t1_ret = float(pred_close[0] - self.context_end_price)\n",
    "        actual_t1_ret = float(actual_close[0] - self.context_end_price)\n",
    "        self.directional_hit = np.sign(pred_t1_ret) == np.sign(actual_t1_ret)\n",
    "        return self\n",
    "\n",
    "\n",
    "class RollingBacktester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        price_df: pd.DataFrame,\n",
    "        feature_df: pd.DataFrame,\n",
    "        window_size: int,\n",
    "        horizon: int,\n",
    "        rolling_start_time: str,\n",
    "        rolling_end_time: str,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.price_df = price_df.copy()\n",
    "        self.feature_df = feature_df.copy()\n",
    "        self.window_size = int(window_size)\n",
    "        self.horizon = int(horizon)\n",
    "        self.rolling_start_time = rolling_start_time\n",
    "        self.rolling_end_time = rolling_end_time\n",
    "        self.device = device\n",
    "\n",
    "        self.input_raw = self.feature_df[BASE_FEATURE_COLS].to_numpy(np.float32)\n",
    "        self.row_imputed = self.feature_df['row_imputed'].to_numpy(np.int8).astype(bool)\n",
    "\n",
    "        self.input_mean = None\n",
    "        self.input_std = None\n",
    "        self.input_scaled = None\n",
    "\n",
    "        self.backtest_date = None\n",
    "        self.backtest_positions = None\n",
    "        self.backtest_day_df = None\n",
    "\n",
    "    def _fit_scaler_on_past_only(self, cutoff_pos: int):\n",
    "        if cutoff_pos <= self.window_size:\n",
    "            raise RuntimeError(\n",
    "                f'Need > window_size history before backtest day. cutoff={cutoff_pos}, window={self.window_size}'\n",
    "            )\n",
    "        train_values = self.input_raw[:cutoff_pos]\n",
    "        self.input_mean = train_values.mean(axis=0)\n",
    "        self.input_std = train_values.std(axis=0)\n",
    "        self.input_std = np.where(self.input_std < 1e-8, 1.0, self.input_std)\n",
    "        self.input_scaled = (self.input_raw - self.input_mean) / self.input_std\n",
    "\n",
    "    def _resolve_day_positions(self, date: Optional[str]) -> np.ndarray:\n",
    "        idx = self.price_df.index\n",
    "        start_t = pd.Timestamp(self.rolling_start_time).time()\n",
    "        end_t = pd.Timestamp(self.rolling_end_time).time()\n",
    "\n",
    "        times = idx.time\n",
    "        intraday_mask = np.array([(t >= start_t) and (t < end_t) for t in times], dtype=bool)\n",
    "\n",
    "        if date:\n",
    "            day_mask = (idx.strftime('%Y-%m-%d') == date)\n",
    "            pos = np.where(day_mask & intraday_mask)[0]\n",
    "            if len(pos) == 0:\n",
    "                raise ValueError(f'No intraday bars found for date={date}')\n",
    "            return pos\n",
    "\n",
    "        # Auto-pick latest day with enough past and future bars\n",
    "        unique_dates = sorted(pd.Index(idx.strftime('%Y-%m-%d')).unique())\n",
    "        for d in reversed(unique_dates):\n",
    "            day_mask = (idx.strftime('%Y-%m-%d') == d)\n",
    "            pos = np.where(day_mask & intraday_mask)[0]\n",
    "            if len(pos) == 0:\n",
    "                continue\n",
    "            first_pos = int(pos[0])\n",
    "            last_pos = int(pos[-1])\n",
    "            enough_past = first_pos >= self.window_size\n",
    "            enough_future = (last_pos + self.horizon) < len(idx)\n",
    "            if enough_past and enough_future:\n",
    "                return pos\n",
    "\n",
    "        raise RuntimeError('Could not find a valid backtest date with sufficient past/future context.')\n",
    "\n",
    "    def _build_context_tensor(self, context_start: int, context_end: int) -> torch.Tensor:\n",
    "        # Context is [context_start, context_end), so last seen bar is context_end-1.\n",
    "        assert context_end - context_start == self.window_size\n",
    "        x_raw = self.input_scaled[context_start:context_end]\n",
    "        imp_frac = float(self.row_imputed[context_start:context_end].mean())\n",
    "        imp_col = np.full((self.window_size, 1), imp_frac, dtype=np.float32)\n",
    "        x_aug = np.concatenate([x_raw, imp_col], axis=1)\n",
    "        x_tensor = torch.from_numpy(x_aug).unsqueeze(0).float().to(self.device)\n",
    "        return x_tensor\n",
    "\n",
    "    def _historical_vol(self, context_start: int, context_end: int) -> float:\n",
    "        closes = self.price_df['Close'].iloc[context_start:context_end].to_numpy(np.float32)\n",
    "        if len(closes) < 2:\n",
    "            return 0.001\n",
    "        log_returns = np.log(closes[1:] / np.maximum(closes[:-1], 1e-8))\n",
    "        v = float(np.std(log_returns))\n",
    "        return max(v, MIN_PREDICTED_VOL)\n",
    "\n",
    "    def run_rolling_backtest(\n",
    "        self,\n",
    "        date: Optional[str] = None,\n",
    "        default_temperature: float = DEFAULT_TEMPERATURE,\n",
    "        every_n_bars: int = ROLLING_EVERY_N_BARS,\n",
    "    ) -> List[RollingPredictionLog]:\n",
    "        if every_n_bars <= 0:\n",
    "            raise ValueError('every_n_bars must be >= 1')\n",
    "\n",
    "        positions = self._resolve_day_positions(date)\n",
    "        self.backtest_positions = positions\n",
    "        self.backtest_date = self.price_df.index[positions[0]].strftime('%Y-%m-%d')\n",
    "\n",
    "        day_start_pos = int(positions[0])\n",
    "        self._fit_scaler_on_past_only(day_start_pos)\n",
    "\n",
    "        sampled_positions = positions[::every_n_bars]\n",
    "        expected_count = len(sampled_positions)\n",
    "        logs: List[Optional[RollingPredictionLog]] = [None] * expected_count\n",
    "\n",
    "        self.backtest_day_df = self.price_df.iloc[positions][OHLC_COLS].copy()\n",
    "\n",
    "        pbar = tqdm(total=expected_count, desc='Processing minute 0/0')\n",
    "        for k, anchor_pos in enumerate(sampled_positions):\n",
    "            anchor_pos = int(anchor_pos)\n",
    "            anchor_time = self.price_df.index[anchor_pos]\n",
    "\n",
    "            context_start = anchor_pos - self.window_size\n",
    "            context_end = anchor_pos\n",
    "            assert context_start >= 0, f'Not enough history for anchor_pos={anchor_pos}'\n",
    "\n",
    "            context_idx = self.price_df.index[context_start:context_end]\n",
    "            context_last_time = context_idx[-1]\n",
    "            assert context_last_time < anchor_time, (\n",
    "                f'Lookahead detected: context_last_time={context_last_time}, anchor_time={anchor_time}'\n",
    "            )\n",
    "\n",
    "            future_slice = self.price_df.iloc[anchor_pos: anchor_pos + self.horizon][OHLC_COLS].copy()\n",
    "            if len(future_slice) != self.horizon:\n",
    "                raise RuntimeError(\n",
    "                    f'Future slice too short at anchor={anchor_time}: got {len(future_slice)}, expected {self.horizon}'\n",
    "                )\n",
    "\n",
    "            x_tensor = self._build_context_tensor(context_start, context_end)\n",
    "            hist_vol = self._historical_vol(context_start, context_end)\n",
    "            temp = temperature_for_anchor(anchor_time, default_temperature)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_returns = self.model.generate_realistic(\n",
    "                    x_tensor,\n",
    "                    temperature=temp,\n",
    "                    historical_vol=hist_vol,\n",
    "                )[0].detach().cpu().numpy()\n",
    "\n",
    "            context_end_price = float(self.price_df['Close'].iloc[context_end - 1])\n",
    "            pred_prices = returns_to_prices_seq(pred_returns, context_end_price)\n",
    "\n",
    "            pred_df = pd.DataFrame(pred_prices, index=future_slice.index, columns=OHLC_COLS)\n",
    "\n",
    "            log = RollingPredictionLog(\n",
    "                anchor_time=anchor_time,\n",
    "                context_end_price=context_end_price,\n",
    "                predicted_path=pred_df,\n",
    "                actual_path=future_slice,\n",
    "                prediction_horizon=self.horizon,\n",
    "                temperature=float(temp),\n",
    "                context_start_time=context_idx[0],\n",
    "                context_last_time=context_last_time,\n",
    "            ).compute_metrics()\n",
    "\n",
    "            logs[k] = log\n",
    "\n",
    "            pbar.set_description(f'Processing minute {k + 1}/{expected_count}')\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        final_logs: List[RollingPredictionLog] = [x for x in logs if x is not None]\n",
    "\n",
    "        # Required validations\n",
    "        assert len(final_logs) == expected_count, (\n",
    "            f'Prediction count mismatch: {len(final_logs)} != {expected_count}'\n",
    "        )\n",
    "        assert final_logs[0].predicted_path.index[0] == final_logs[0].anchor_time, (\n",
    "            'First prediction timestamp does not match anchor time.'\n",
    "        )\n",
    "\n",
    "        # Full-day example check (390 bars on normal days)\n",
    "        day_minutes = len(positions)\n",
    "        if every_n_bars == 1 and day_minutes == 390:\n",
    "            assert len(final_logs) == 390, (\n",
    "                f'Expected 390 predictions for full day, got {len(final_logs)}'\n",
    "            )\n",
    "\n",
    "        print({\n",
    "            'backtest_date': self.backtest_date,\n",
    "            'expected_predictions': expected_count,\n",
    "            'actual_predictions': len(final_logs),\n",
    "            'day_minutes': day_minutes,\n",
    "            'window_size': self.window_size,\n",
    "            'horizon': self.horizon,\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"First anchor={final_logs[0].anchor_time}, \"\n",
    "            f\"first predicted ts={final_logs[0].predicted_path.index[0]}, \"\n",
    "            f\"context_last_ts={final_logs[0].context_last_time}\"\n",
    "        )\n",
    "\n",
    "        return final_logs\n",
    "\n",
    "\n",
    "def runrollingbacktest(\n",
    "    model: nn.Module,\n",
    "    pricedf: pd.DataFrame,\n",
    "    featuredf: pd.DataFrame,\n",
    "    windowsize: int,\n",
    "    starttime: str,\n",
    "    endtime: str,\n",
    "    date: Optional[str] = None,\n",
    ") -> tuple[List[RollingPredictionLog], RollingBacktester]:\n",
    "    backtester = RollingBacktester(\n",
    "        model=model,\n",
    "        price_df=pricedf,\n",
    "        feature_df=featuredf,\n",
    "        window_size=windowsize,\n",
    "        horizon=HORIZON,\n",
    "        rolling_start_time=starttime,\n",
    "        rolling_end_time=endtime,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    logs = backtester.run_rolling_backtest(\n",
    "        date=date,\n",
    "        default_temperature=DEFAULT_TEMPERATURE,\n",
    "        every_n_bars=ROLLING_EVERY_N_BARS,\n",
    "    )\n",
    "    return logs, backtester"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 3: Visualization (fan / cone)\n",
    "def _draw_candles(\n",
    "    ax,\n",
    "    ohlc: pd.DataFrame,\n",
    "    ts_to_x: Dict[pd.Timestamp, float],\n",
    "    up_color: str,\n",
    "    down_color: str,\n",
    "    wick_color: str,\n",
    "    width: float = 0.58,\n",
    "    alpha: float = 1.0,\n",
    "):\n",
    "    vals = ohlc[OHLC_COLS].to_numpy(np.float32)\n",
    "    idx = ohlc.index\n",
    "    for i, (o, h, l, c) in enumerate(vals):\n",
    "        x = ts_to_x[idx[i]]\n",
    "        bull = c >= o\n",
    "        ax.vlines(x, l, h, color=wick_color, linewidth=0.8, alpha=alpha, zorder=1)\n",
    "\n",
    "        lower = min(o, c)\n",
    "        height = max(abs(c - o), 1e-6)\n",
    "        rect = Rectangle(\n",
    "            (x - width / 2, lower),\n",
    "            width,\n",
    "            height,\n",
    "            facecolor=up_color if bull else down_color,\n",
    "            edgecolor=up_color if bull else down_color,\n",
    "            linewidth=0.8,\n",
    "            alpha=alpha,\n",
    "            zorder=2,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "def plot_prediction_fan(\n",
    "    prediction_logs: List[RollingPredictionLog],\n",
    "    actual_day_df: pd.DataFrame,\n",
    "    max_prediction_age: int = MAX_PREDICTION_AGE,\n",
    "    fan_opacity_decay: float = FAN_OPACITY_DECAY,\n",
    "    title_prefix: str = 'Rolling Backtest',\n",
    "):\n",
    "    if not prediction_logs:\n",
    "        raise ValueError('prediction_logs is empty.')\n",
    "\n",
    "    logs = prediction_logs[-max_prediction_age:]\n",
    "\n",
    "    all_ts = list(actual_day_df.index)\n",
    "    for log in logs:\n",
    "        all_ts.extend(list(log.predicted_path.index))\n",
    "    all_ts = pd.Index(sorted(set(all_ts)))\n",
    "\n",
    "    ts_to_x = {ts: i for i, ts in enumerate(all_ts)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), facecolor='black')\n",
    "    ax.set_facecolor('black')\n",
    "\n",
    "    # Actual candles (faint background)\n",
    "    _draw_candles(\n",
    "        ax,\n",
    "        actual_day_df,\n",
    "        ts_to_x=ts_to_x,\n",
    "        up_color='#2ECC71',\n",
    "        down_color='#E74C3C',\n",
    "        wick_color='#BBBBBB',\n",
    "        width=0.60,\n",
    "        alpha=0.30,\n",
    "    )\n",
    "\n",
    "    # Prediction fan: older -> more transparent\n",
    "    for i, log in enumerate(logs):\n",
    "        age_from_latest = len(logs) - 1 - i\n",
    "        alpha = max(0.10, 0.90 - fan_opacity_decay * age_from_latest)\n",
    "\n",
    "        p = log.predicted_path['Close']\n",
    "        x = [ts_to_x[t] for t in p.index]\n",
    "        y = p.to_numpy(np.float32)\n",
    "\n",
    "        up_move = float(y[-1] - log.context_end_price) >= 0.0\n",
    "        color = '#7FDBFF' if up_move else '#FF851B'\n",
    "        ax.plot(x, y, color=color, alpha=alpha, linewidth=1.1, zorder=3)\n",
    "\n",
    "    # Current time marker = latest prediction anchor\n",
    "    latest_anchor = logs[-1].anchor_time\n",
    "    ax.axvline(ts_to_x[latest_anchor], color='white', linestyle='--', linewidth=1.2, alpha=0.85, zorder=4)\n",
    "\n",
    "    # X ticks\n",
    "    n = len(all_ts)\n",
    "    step = max(1, n // 12)\n",
    "    ticks = list(range(0, n, step))\n",
    "    if ticks[-1] != n - 1:\n",
    "        ticks.append(n - 1)\n",
    "\n",
    "    labels = [all_ts[t].strftime('%H:%M') for t in ticks]\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(labels, rotation=25, ha='right', color='white')\n",
    "\n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    for sp in ax.spines.values():\n",
    "        sp.set_color('#666666')\n",
    "    ax.grid(color='#2a2a2a', alpha=0.35)\n",
    "\n",
    "    directional_hits = [bool(log.directional_hit) for log in prediction_logs if log.directional_hit is not None]\n",
    "    directional_acc = float(np.mean(directional_hits)) if directional_hits else float('nan')\n",
    "    step1_mae = float(np.mean([log.step_mae[0] for log in prediction_logs if log.step_mae is not None]))\n",
    "\n",
    "    date_txt = prediction_logs[0].anchor_time.strftime('%Y-%m-%d')\n",
    "    ax.set_title(\n",
    "        f'{title_prefix}: {SYMBOL} {date_txt} | {len(prediction_logs)} predictions | '\n",
    "        f'Directional Accuracy={directional_acc:.2%} | Mean Step1 MAE=${step1_mae:.2f} | Temp~{DEFAULT_TEMPERATURE}',\n",
    "        color='white',\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_ylabel('Price', color='white')\n",
    "\n",
    "    legend_items = [\n",
    "        Patch(facecolor='#2ECC71', edgecolor='#2ECC71', label='Actual candles (faint)'),\n",
    "        Line2D([0], [0], color='#7FDBFF', lw=1.5, label='Prediction cone (fading up)'),\n",
    "        Line2D([0], [0], color='#FF851B', lw=1.5, label='Prediction cone (fading down)'),\n",
    "        Line2D([0], [0], color='white', lw=1.2, linestyle='--', label='Current time'),\n",
    "    ]\n",
    "    leg = ax.legend(handles=legend_items, loc='upper left', facecolor='black', edgecolor='#666666', framealpha=1.0)\n",
    "    for txt in leg.get_texts():\n",
    "        txt.set_color('white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plotpredictionfan(*args, **kwargs):\n",
    "    \"\"\"Alias requested in prompt.\"\"\"\n",
    "    return plot_prediction_fan(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 4: Execute rolling backtest (one full day) and show fan chart\n",
    "print('Fetching data from Alpaca...')\n",
    "raw_df_utc, api_calls = fetch_bars_alpaca(SYMBOL, LOOKBACK_DAYS)\n",
    "price_df, session_meta = sessionize_with_calendar(raw_df_utc)\n",
    "print({\n",
    "    'raw_rows': len(raw_df_utc),\n",
    "    'sessionized_rows': len(price_df),\n",
    "    'api_calls': api_calls,\n",
    "    'session_meta': session_meta,\n",
    "})\n",
    "\n",
    "feature_df = build_feature_frame(price_df)\n",
    "print({'feature_rows': len(feature_df)})\n",
    "\n",
    "# IMPORTANT: model architecture stays exactly v7; only wrapper changes\n",
    "model = load_trained_v7_model(input_size=len(BASE_FEATURE_COLS) + 1)\n",
    "\n",
    "prediction_logs, backtester = runrollingbacktest(\n",
    "    model=model,\n",
    "    pricedf=price_df,\n",
    "    featuredf=feature_df,\n",
    "    windowsize=WINDOW_SIZE,\n",
    "    starttime=ROLLING_START_TIME,\n",
    "    endtime=ROLLING_END_TIME,\n",
    "    date=BACKTEST_DATE,\n",
    ")\n",
    "\n",
    "# Required validation asserts\n",
    "first_log = prediction_logs[0]\n",
    "assert first_log.predicted_path.index[0] == first_log.anchor_time, (\n",
    "    'Validation failed: first prediction candle timestamp must equal anchor time.'\n",
    ")\n",
    "\n",
    "for log in prediction_logs:\n",
    "    assert log.context_last_time < log.anchor_time, (\n",
    "        f'Validation failed: context includes future data at anchor {log.anchor_time}'\n",
    "    )\n",
    "\n",
    "# Prediction count check for full day\n",
    "expected_count = len(backtester.backtest_positions[::ROLLING_EVERY_N_BARS])\n",
    "assert len(prediction_logs) == expected_count, (\n",
    "    f'Validation failed: predictions={len(prediction_logs)} expected={expected_count}'\n",
    ")\n",
    "if len(backtester.backtest_positions) == 390 and ROLLING_EVERY_N_BARS == 1:\n",
    "    assert len(prediction_logs) == 390, (\n",
    "        f'Validation failed: expected 390 predictions for full day, got {len(prediction_logs)}'\n",
    "    )\n",
    "\n",
    "print('All strict-causality validation checks passed.')\n",
    "\n",
    "fig, ax = plotpredictionfan(\n",
    "    prediction_logs=prediction_logs,\n",
    "    actual_day_df=backtester.backtest_day_df,\n",
    "    max_prediction_age=MAX_PREDICTION_AGE,\n",
    "    fan_opacity_decay=FAN_OPACITY_DECAY,\n",
    "    title_prefix='Rolling Backtest',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cell 5: Rolling metrics + error analysis\n",
    "def calculate_rolling_metrics(logs: List[RollingPredictionLog]) -> Dict[str, float]:\n",
    "    if not logs:\n",
    "        raise ValueError('No prediction logs.')\n",
    "\n",
    "    # 1) Hit rate (t+1 direction)\n",
    "    hits = np.array([bool(l.directional_hit) for l in logs if l.directional_hit is not None], dtype=bool)\n",
    "    hit_rate = float(hits.mean()) if len(hits) else float('nan')\n",
    "\n",
    "    # 2) Path divergence MAE at steps 1/5/10/15\n",
    "    step_points = [1, 5, 10, 15]\n",
    "    step_mae = {}\n",
    "    for s in step_points:\n",
    "        vals = [float(l.step_mae[s - 1]) for l in logs if (l.step_mae is not None and len(l.step_mae) >= s)]\n",
    "        step_mae[s] = float(np.mean(vals)) if vals else float('nan')\n",
    "\n",
    "    # 3) Trend correlation (slope over 15 bars)\n",
    "    pred_slopes = []\n",
    "    actual_slopes = []\n",
    "    x = np.arange(HORIZON, dtype=np.float32)\n",
    "    for l in logs:\n",
    "        p = l.predicted_path['Close'].to_numpy(np.float32)\n",
    "        a = l.actual_path['Close'].to_numpy(np.float32)\n",
    "        if len(p) == HORIZON and len(a) == HORIZON:\n",
    "            pred_slopes.append(np.polyfit(x, p, 1)[0])\n",
    "            actual_slopes.append(np.polyfit(x, a, 1)[0])\n",
    "\n",
    "    if len(pred_slopes) > 1:\n",
    "        trend_corr = float(np.corrcoef(pred_slopes, actual_slopes)[0, 1])\n",
    "    else:\n",
    "        trend_corr = float('nan')\n",
    "\n",
    "    return {\n",
    "        'prediction_count': len(logs),\n",
    "        'directional_hit_rate_t1': hit_rate,\n",
    "        'mae_step_1': step_mae[1],\n",
    "        'mae_step_5': step_mae[5],\n",
    "        'mae_step_10': step_mae[10],\n",
    "        'mae_step_15': step_mae[15],\n",
    "        'trend_correlation_15bar_slope': trend_corr,\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = calculate_rolling_metrics(prediction_logs)\n",
    "summary_df = pd.DataFrame([\n",
    "    ('Prediction count', metrics['prediction_count']),\n",
    "    ('Directional hit rate (t+1)', f\"{metrics['directional_hit_rate_t1']:.2%}\"),\n",
    "    ('Path MAE @ step 1', f\"${metrics['mae_step_1']:.4f}\"),\n",
    "    ('Path MAE @ step 5', f\"${metrics['mae_step_5']:.4f}\"),\n",
    "    ('Path MAE @ step 10', f\"${metrics['mae_step_10']:.4f}\"),\n",
    "    ('Path MAE @ step 15', f\"${metrics['mae_step_15']:.4f}\"),\n",
    "    ('Trend correlation (15-bar slope)', f\"{metrics['trend_correlation_15bar_slope']:.4f}\"),\n",
    "], columns=['Metric', 'Value'])\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "# Simple error analysis by anchor time bucket\n",
    "records = []\n",
    "for l in prediction_logs:\n",
    "    records.append({\n",
    "        'anchor_time': l.anchor_time,\n",
    "        'hour': l.anchor_time.hour,\n",
    "        'step1_error': float(l.step_mae[0]) if l.step_mae is not None else np.nan,\n",
    "        'direction_hit': bool(l.directional_hit) if l.directional_hit is not None else np.nan,\n",
    "    })\n",
    "err_df = pd.DataFrame(records)\n",
    "err_by_hour = err_df.groupby('hour', as_index=False).agg(\n",
    "    step1_error_mean=('step1_error', 'mean'),\n",
    "    step1_error_std=('step1_error', 'std'),\n",
    "    direction_hit_rate=('direction_hit', 'mean'),\n",
    "    count=('anchor_time', 'count'),\n",
    ")\n",
    "\n",
    "display(err_by_hour)\n",
    "\n",
    "print(\n",
    "    f\"Rolling Backtest: {metrics['prediction_count']} predictions | \"\n",
    "    f\"Directional Accuracy: {metrics['directional_hit_rate_t1']:.2%} | \"\n",
    "    f\"Mean Path MAE (step1): ${metrics['mae_step_1']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- This notebook keeps the v7 model architecture unchanged and only adds rolling evaluation/visualization wrappers.\n",
    "- For strict causality, scaling statistics are fit only on data **before** the selected backtest day.\n",
    "- If you need animation, you can add an animation cell on top of the same `prediction_logs` structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}